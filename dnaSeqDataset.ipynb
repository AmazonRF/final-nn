{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import read_text_file, read_fasta_file,sample_window, one_hot_encode_seqs, sample_seqs\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from nn import NeuralNetwork\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_raw_dataset = read_text_file(\"data/rap1-lieb-positives.txt\")\n",
    "negative_raw_dataset = read_fasta_file(\"data/yeast-upstream-1k-negative.fa\")\n",
    "negative_raw_dataset = sample_window(negative_raw_dataset, window_size=17)\n",
    "\n",
    "pos_dataset = one_hot_encode_seqs(positive_raw_dataset)\n",
    "neg_dataset = one_hot_encode_seqs(negative_raw_dataset)\n",
    "\n",
    "pos_labels = [1] * len(pos_dataset)\n",
    "neg_labels = [0] * len(neg_dataset)\n",
    "\n",
    "# sequences = np.vstack((pos_dataset, neg_dataset))[:, 2:-2]\n",
    "sequences = np.vstack((pos_dataset, neg_dataset))\n",
    "\n",
    "labels = pos_labels + neg_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(sequences)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, labels, test_size=0.2, random_state=36)\n",
    "X_train, y_train = sample_seqs(list(X_train), list(y_train))\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000, Training Loss: 0.6937369807105577, Validation Loss: 0.7243568588779548\n",
      "Epoch 2/3000, Training Loss: 0.6934745797221548, Validation Loss: 0.718952995337338\n",
      "Epoch 3/3000, Training Loss: 0.693252760476566, Validation Loss: 0.714150774766489\n",
      "Epoch 4/3000, Training Loss: 0.6930764723884166, Validation Loss: 0.7104913038744013\n",
      "Epoch 5/3000, Training Loss: 0.6929878586085515, Validation Loss: 0.7074505108678596\n",
      "Epoch 6/3000, Training Loss: 0.6929244438823138, Validation Loss: 0.7049225108244419\n",
      "Epoch 7/3000, Training Loss: 0.6928784173684291, Validation Loss: 0.7028094354992881\n",
      "Epoch 8/3000, Training Loss: 0.6928444484065125, Validation Loss: 0.7010444974100732\n",
      "Epoch 9/3000, Training Loss: 0.6928189239096231, Validation Loss: 0.6995774964971173\n",
      "Epoch 10/3000, Training Loss: 0.6927993573872572, Validation Loss: 0.6983476005925839\n",
      "Epoch 11/3000, Training Loss: 0.6927838616174457, Validation Loss: 0.6973248384146651\n",
      "Epoch 12/3000, Training Loss: 0.6927712617846853, Validation Loss: 0.6964665693617382\n",
      "Epoch 13/3000, Training Loss: 0.6927606623178009, Validation Loss: 0.695756367074981\n",
      "Epoch 14/3000, Training Loss: 0.6927514802344729, Validation Loss: 0.6951558720443238\n",
      "Epoch 15/3000, Training Loss: 0.6927431955891715, Validation Loss: 0.6946539028829687\n",
      "Epoch 16/3000, Training Loss: 0.6927356020655294, Validation Loss: 0.6942367691097517\n",
      "Epoch 17/3000, Training Loss: 0.6927284911665467, Validation Loss: 0.6938850355787276\n",
      "Epoch 18/3000, Training Loss: 0.6927217023573611, Validation Loss: 0.6935959848105282\n",
      "Epoch 19/3000, Training Loss: 0.6927151453621645, Validation Loss: 0.6933469915334931\n",
      "Epoch 20/3000, Training Loss: 0.6927086820049959, Validation Loss: 0.693141019308606\n",
      "Epoch 21/3000, Training Loss: 0.6927023471724404, Validation Loss: 0.692969551548954\n",
      "Epoch 22/3000, Training Loss: 0.6926960771933169, Validation Loss: 0.6928215416076529\n",
      "Epoch 23/3000, Training Loss: 0.6926898954229571, Validation Loss: 0.6927066189485318\n",
      "Epoch 24/3000, Training Loss: 0.6926837077162797, Validation Loss: 0.6926015252965209\n",
      "Epoch 25/3000, Training Loss: 0.6926774802670038, Validation Loss: 0.6925102428954175\n",
      "Epoch 26/3000, Training Loss: 0.692671242606906, Validation Loss: 0.6924413435402555\n",
      "Epoch 27/3000, Training Loss: 0.6926650786003482, Validation Loss: 0.6923747692741423\n",
      "Epoch 28/3000, Training Loss: 0.6926588346634543, Validation Loss: 0.6923264824141934\n",
      "Epoch 29/3000, Training Loss: 0.692652647342363, Validation Loss: 0.6922771268410449\n",
      "Epoch 30/3000, Training Loss: 0.6926463826247823, Validation Loss: 0.6922446199537731\n",
      "Epoch 31/3000, Training Loss: 0.6926402044847764, Validation Loss: 0.6922069249016158\n",
      "Epoch 32/3000, Training Loss: 0.6926338559731039, Validation Loss: 0.6921787312496408\n",
      "Epoch 33/3000, Training Loss: 0.6926275738870448, Validation Loss: 0.6921543008406338\n",
      "Epoch 34/3000, Training Loss: 0.6926212740790119, Validation Loss: 0.6921330089816013\n",
      "Epoch 35/3000, Training Loss: 0.6926149559877736, Validation Loss: 0.6921143339702307\n",
      "Epoch 36/3000, Training Loss: 0.6926086191918803, Validation Loss: 0.6920978402056346\n",
      "Epoch 37/3000, Training Loss: 0.6926022633365523, Validation Loss: 0.6920831639731578\n",
      "Epoch 38/3000, Training Loss: 0.6925958879629419, Validation Loss: 0.6920700016011413\n",
      "Epoch 39/3000, Training Loss: 0.6925894939834533, Validation Loss: 0.6920580998819321\n",
      "Epoch 40/3000, Training Loss: 0.6925830820528631, Validation Loss: 0.692047247405017\n",
      "Epoch 41/3000, Training Loss: 0.6925766498452559, Validation Loss: 0.6920372672527783\n",
      "Epoch 42/3000, Training Loss: 0.6925701973337544, Validation Loss: 0.6920280115732217\n",
      "Epoch 43/3000, Training Loss: 0.6925637236294858, Validation Loss: 0.692019356868286\n",
      "Epoch 44/3000, Training Loss: 0.6925572286243238, Validation Loss: 0.6920112000811453\n",
      "Epoch 45/3000, Training Loss: 0.6925507230029977, Validation Loss: 0.6920008972655743\n",
      "Epoch 46/3000, Training Loss: 0.6925441975193748, Validation Loss: 0.691991337264927\n",
      "Epoch 47/3000, Training Loss: 0.6925376500666774, Validation Loss: 0.6919823933390548\n",
      "Epoch 48/3000, Training Loss: 0.6925310803595897, Validation Loss: 0.6919739595984314\n",
      "Epoch 49/3000, Training Loss: 0.692524488114711, Validation Loss: 0.6919659476562668\n",
      "Epoch 50/3000, Training Loss: 0.6925178730493612, Validation Loss: 0.6919582833080782\n",
      "Epoch 51/3000, Training Loss: 0.6925112348807377, Validation Loss: 0.6919509053100805\n",
      "Epoch 52/3000, Training Loss: 0.6925045733253201, Validation Loss: 0.6919437621751544\n",
      "Epoch 53/3000, Training Loss: 0.6924978880984463, Validation Loss: 0.6919368108228231\n",
      "Epoch 54/3000, Training Loss: 0.6924911782757371, Validation Loss: 0.6919286893900447\n",
      "Epoch 55/3000, Training Loss: 0.6924844531196311, Validation Loss: 0.6919263021916982\n",
      "Epoch 56/3000, Training Loss: 0.6924777502984846, Validation Loss: 0.6919138787311044\n",
      "Epoch 57/3000, Training Loss: 0.6924709472747271, Validation Loss: 0.691911836167941\n",
      "Epoch 58/3000, Training Loss: 0.6924641110229314, Validation Loss: 0.6919024751437794\n",
      "Epoch 59/3000, Training Loss: 0.6924572621886728, Validation Loss: 0.6918989973156123\n",
      "Epoch 60/3000, Training Loss: 0.6924504181282308, Validation Loss: 0.6918897229809412\n",
      "Epoch 61/3000, Training Loss: 0.6924435140671664, Validation Loss: 0.6918863093395232\n",
      "Epoch 62/3000, Training Loss: 0.6924366254154377, Validation Loss: 0.6918770803547137\n",
      "Epoch 63/3000, Training Loss: 0.6924296716514627, Validation Loss: 0.6918736964903196\n",
      "Epoch 64/3000, Training Loss: 0.6924227384075977, Validation Loss: 0.6918644831429382\n",
      "Epoch 65/3000, Training Loss: 0.692415726646875, Validation Loss: 0.6918611029160843\n",
      "Epoch 66/3000, Training Loss: 0.6924087431998072, Validation Loss: 0.6918505778983552\n",
      "Epoch 67/3000, Training Loss: 0.6924016923177096, Validation Loss: 0.6918446999322087\n",
      "Epoch 68/3000, Training Loss: 0.6923946158785942, Validation Loss: 0.691837478675575\n",
      "Epoch 69/3000, Training Loss: 0.6923874951585043, Validation Loss: 0.6918292619397154\n",
      "Epoch 70/3000, Training Loss: 0.6923802900353043, Validation Loss: 0.6918203577026079\n",
      "Epoch 71/3000, Training Loss: 0.692373013059292, Validation Loss: 0.691813045513882\n",
      "Epoch 72/3000, Training Loss: 0.6923657576166404, Validation Loss: 0.6918049765146485\n",
      "Epoch 73/3000, Training Loss: 0.6923583408951249, Validation Loss: 0.6917950523637856\n",
      "Epoch 74/3000, Training Loss: 0.6923505325548841, Validation Loss: 0.6917861495209202\n",
      "Epoch 75/3000, Training Loss: 0.6923427506222243, Validation Loss: 0.6917799745360722\n",
      "Epoch 76/3000, Training Loss: 0.6923351835689818, Validation Loss: 0.6917740668063146\n",
      "Epoch 77/3000, Training Loss: 0.6923274908786978, Validation Loss: 0.6917686086048676\n",
      "Epoch 78/3000, Training Loss: 0.692319800511564, Validation Loss: 0.6917620685021151\n",
      "Epoch 79/3000, Training Loss: 0.6923116982417575, Validation Loss: 0.6917559344864849\n",
      "Epoch 80/3000, Training Loss: 0.692303410328363, Validation Loss: 0.6917503158704777\n",
      "Epoch 81/3000, Training Loss: 0.6922953903768615, Validation Loss: 0.6917448717765863\n",
      "Epoch 82/3000, Training Loss: 0.6922875467806455, Validation Loss: 0.6917392889129568\n",
      "Epoch 83/3000, Training Loss: 0.6922797800593202, Validation Loss: 0.691733540434455\n",
      "Epoch 84/3000, Training Loss: 0.692272084048683, Validation Loss: 0.6917275770096778\n",
      "Epoch 85/3000, Training Loss: 0.6922643539228149, Validation Loss: 0.6917214285559907\n",
      "Epoch 86/3000, Training Loss: 0.6922565891757413, Validation Loss: 0.6917151197202652\n",
      "Epoch 87/3000, Training Loss: 0.6922487893809203, Validation Loss: 0.6917086712755202\n",
      "Epoch 88/3000, Training Loss: 0.6922409543362934, Validation Loss: 0.6917021005400278\n",
      "Epoch 89/3000, Training Loss: 0.6922330836573617, Validation Loss: 0.6916954221200448\n",
      "Epoch 90/3000, Training Loss: 0.6922251769562029, Validation Loss: 0.6916886477250499\n",
      "Epoch 91/3000, Training Loss: 0.6922172338414003, Validation Loss: 0.6916817870417449\n",
      "Epoch 92/3000, Training Loss: 0.692209253917974, Validation Loss: 0.6916748481903082\n",
      "Epoch 93/3000, Training Loss: 0.6922012367873203, Validation Loss: 0.6916678378474561\n",
      "Epoch 94/3000, Training Loss: 0.6921931820471504, Validation Loss: 0.6916607612704877\n",
      "Epoch 95/3000, Training Loss: 0.6921850892914319, Validation Loss: 0.6916536233671164\n",
      "Epoch 96/3000, Training Loss: 0.6921769581251209, Validation Loss: 0.6916464278137274\n",
      "Epoch 97/3000, Training Loss: 0.6921687882173083, Validation Loss: 0.6916391775407645\n",
      "Epoch 98/3000, Training Loss: 0.6921605790532158, Validation Loss: 0.6916318750105284\n",
      "Epoch 99/3000, Training Loss: 0.6921523302112784, Validation Loss: 0.6916245223593271\n",
      "Epoch 100/3000, Training Loss: 0.6921440412390838, Validation Loss: 0.6916171209693266\n",
      "Epoch 101/3000, Training Loss: 0.6921357115577622, Validation Loss: 0.691609672487477\n",
      "Epoch 102/3000, Training Loss: 0.6921273429167887, Validation Loss: 0.6916021780898542\n",
      "Epoch 103/3000, Training Loss: 0.6921189328940965, Validation Loss: 0.6915946386605051\n",
      "Epoch 104/3000, Training Loss: 0.6921104810135417, Validation Loss: 0.6915870545662043\n",
      "Epoch 105/3000, Training Loss: 0.6921019868284757, Validation Loss: 0.6915794261068469\n",
      "Epoch 106/3000, Training Loss: 0.6920934498878575, Validation Loss: 0.6915717534016282\n",
      "Epoch 107/3000, Training Loss: 0.6920848697361903, Validation Loss: 0.691564036582108\n",
      "Epoch 108/3000, Training Loss: 0.6920762462454787, Validation Loss: 0.6915562752499672\n",
      "Epoch 109/3000, Training Loss: 0.6920675789990884, Validation Loss: 0.6915484697913621\n",
      "Epoch 110/3000, Training Loss: 0.6920588662334489, Validation Loss: 0.6915406201198648\n",
      "Epoch 111/3000, Training Loss: 0.6920501077484991, Validation Loss: 0.6915327258917536\n",
      "Epoch 112/3000, Training Loss: 0.6920413029216672, Validation Loss: 0.691524786855444\n",
      "Epoch 113/3000, Training Loss: 0.6920324519792619, Validation Loss: 0.691516802720447\n",
      "Epoch 114/3000, Training Loss: 0.6920235544318708, Validation Loss: 0.6915087731657625\n",
      "Epoch 115/3000, Training Loss: 0.6920146097850723, Validation Loss: 0.6915006978440064\n",
      "Epoch 116/3000, Training Loss: 0.6920056175393644, Validation Loss: 0.691492576170438\n",
      "Epoch 117/3000, Training Loss: 0.6919965790658499, Validation Loss: 0.6914844084282231\n",
      "Epoch 118/3000, Training Loss: 0.6919874951444682, Validation Loss: 0.6914761942289672\n",
      "Epoch 119/3000, Training Loss: 0.6919783621844063, Validation Loss: 0.6914679330826345\n",
      "Epoch 120/3000, Training Loss: 0.6919691796660494, Validation Loss: 0.6914596244959483\n",
      "Epoch 121/3000, Training Loss: 0.6919599470643354, Validation Loss: 0.6914512681303774\n",
      "Epoch 122/3000, Training Loss: 0.6919506638306704, Validation Loss: 0.6914428635851863\n",
      "Epoch 123/3000, Training Loss: 0.6919413290904753, Validation Loss: 0.6914344100658455\n",
      "Epoch 124/3000, Training Loss: 0.6919319425388699, Validation Loss: 0.6914259069759157\n",
      "Epoch 125/3000, Training Loss: 0.6919225027288898, Validation Loss: 0.6914173536767237\n",
      "Epoch 126/3000, Training Loss: 0.6919130079318495, Validation Loss: 0.6914087497879403\n",
      "Epoch 127/3000, Training Loss: 0.6919034597152558, Validation Loss: 0.6914000947981283\n",
      "Epoch 128/3000, Training Loss: 0.6918938575126599, Validation Loss: 0.6913913882261079\n",
      "Epoch 129/3000, Training Loss: 0.6918842007514828, Validation Loss: 0.6913826295591634\n",
      "Epoch 130/3000, Training Loss: 0.6918744888529252, Validation Loss: 0.6913738182267539\n",
      "Epoch 131/3000, Training Loss: 0.6918647212318749, Validation Loss: 0.6913649536050877\n",
      "Epoch 132/3000, Training Loss: 0.6918548972968157, Validation Loss: 0.6913560352419765\n",
      "Epoch 133/3000, Training Loss: 0.6918450164497313, Validation Loss: 0.6913470626615238\n",
      "Epoch 134/3000, Training Loss: 0.6918350780183246, Validation Loss: 0.6913380352348837\n",
      "Epoch 135/3000, Training Loss: 0.6918250810214843, Validation Loss: 0.6913289524789688\n",
      "Epoch 136/3000, Training Loss: 0.6918150252700574, Validation Loss: 0.6913198138116209\n",
      "Epoch 137/3000, Training Loss: 0.6918049115187251, Validation Loss: 0.6913106191839646\n",
      "Epoch 138/3000, Training Loss: 0.6917947414475208, Validation Loss: 0.6913013680112724\n",
      "Epoch 139/3000, Training Loss: 0.691784510814504, Validation Loss: 0.6912920593314504\n",
      "Epoch 140/3000, Training Loss: 0.691774218974285, Validation Loss: 0.6912826927738381\n",
      "Epoch 141/3000, Training Loss: 0.6917638652741745, Validation Loss: 0.6912732678596742\n",
      "Epoch 142/3000, Training Loss: 0.6917534490540734, Validation Loss: 0.6912637837633466\n",
      "Epoch 143/3000, Training Loss: 0.6917429696463621, Validation Loss: 0.6912542399440491\n",
      "Epoch 144/3000, Training Loss: 0.6917324263757876, Validation Loss: 0.6912446356853219\n",
      "Epoch 145/3000, Training Loss: 0.6917218185593492, Validation Loss: 0.6912349702662134\n",
      "Epoch 146/3000, Training Loss: 0.6917111455061818, Validation Loss: 0.6912252429381698\n",
      "Epoch 147/3000, Training Loss: 0.6917004065174365, Validation Loss: 0.691215453135706\n",
      "Epoch 148/3000, Training Loss: 0.6916896010100034, Validation Loss: 0.691205600191491\n",
      "Epoch 149/3000, Training Loss: 0.6916787293651112, Validation Loss: 0.691195683068379\n",
      "Epoch 150/3000, Training Loss: 0.6916677898539748, Validation Loss: 0.6911857014857135\n",
      "Epoch 151/3000, Training Loss: 0.6916567851788314, Validation Loss: 0.6911756554796396\n",
      "Epoch 152/3000, Training Loss: 0.6916457148245965, Validation Loss: 0.6911655437904228\n",
      "Epoch 153/3000, Training Loss: 0.6916345757259542, Validation Loss: 0.6911553656395429\n",
      "Epoch 154/3000, Training Loss: 0.6916233645056228, Validation Loss: 0.6911451199050979\n",
      "Epoch 155/3000, Training Loss: 0.6916120792761327, Validation Loss: 0.6911348054105312\n",
      "Epoch 156/3000, Training Loss: 0.6916007215206454, Validation Loss: 0.6911244216685188\n",
      "Epoch 157/3000, Training Loss: 0.691589290697369, Validation Loss: 0.6911139680295901\n",
      "Epoch 158/3000, Training Loss: 0.6915777867812847, Validation Loss: 0.6911034439417287\n",
      "Epoch 159/3000, Training Loss: 0.691566209625263, Validation Loss: 0.6910928487223311\n",
      "Epoch 160/3000, Training Loss: 0.6915545568124688, Validation Loss: 0.6910821817053006\n",
      "Epoch 161/3000, Training Loss: 0.6915428266935766, Validation Loss: 0.691071442047332\n",
      "Epoch 162/3000, Training Loss: 0.6915310174741742, Validation Loss: 0.6910606289317638\n",
      "Epoch 163/3000, Training Loss: 0.6915191290966852, Validation Loss: 0.6910497413009642\n",
      "Epoch 164/3000, Training Loss: 0.6915071616443145, Validation Loss: 0.6910387784912203\n",
      "Epoch 165/3000, Training Loss: 0.6914951149484163, Validation Loss: 0.6910277399132972\n",
      "Epoch 166/3000, Training Loss: 0.691482989712305, Validation Loss: 0.691016624633653\n",
      "Epoch 167/3000, Training Loss: 0.6914707828167264, Validation Loss: 0.6910054319649069\n",
      "Epoch 168/3000, Training Loss: 0.6914584933652345, Validation Loss: 0.6909941611994141\n",
      "Epoch 169/3000, Training Loss: 0.6914461204502271, Validation Loss: 0.6909828112172278\n",
      "Epoch 170/3000, Training Loss: 0.6914336631527706, Validation Loss: 0.6909713813396086\n",
      "Epoch 171/3000, Training Loss: 0.6914211205424186, Validation Loss: 0.6909598708703363\n",
      "Epoch 172/3000, Training Loss: 0.6914084916770324, Validation Loss: 0.6909482789153966\n",
      "Epoch 173/3000, Training Loss: 0.6913957765050194, Validation Loss: 0.6909366048480488\n",
      "Epoch 174/3000, Training Loss: 0.691382976974392, Validation Loss: 0.6909248486847342\n",
      "Epoch 175/3000, Training Loss: 0.6913700945846232, Validation Loss: 0.6909130094121527\n",
      "Epoch 176/3000, Training Loss: 0.6913571222510436, Validation Loss: 0.6909010859806752\n",
      "Epoch 177/3000, Training Loss: 0.6913440589728286, Validation Loss: 0.6908890773520303\n",
      "Epoch 178/3000, Training Loss: 0.6913309037363367, Validation Loss: 0.6908769824976868\n",
      "Epoch 179/3000, Training Loss: 0.6913176555149039, Validation Loss: 0.6908648004528386\n",
      "Epoch 180/3000, Training Loss: 0.6913043132686334, Validation Loss: 0.6908525301849687\n",
      "Epoch 181/3000, Training Loss: 0.6912908759441817, Validation Loss: 0.6908401711785743\n",
      "Epoch 182/3000, Training Loss: 0.6912773424745411, Validation Loss: 0.6908277212937952\n",
      "Epoch 183/3000, Training Loss: 0.6912637119873909, Validation Loss: 0.6908151795596364\n",
      "Epoch 184/3000, Training Loss: 0.691249983507452, Validation Loss: 0.6908025453975788\n",
      "Epoch 185/3000, Training Loss: 0.6912361520950291, Validation Loss: 0.6907898172504322\n",
      "Epoch 186/3000, Training Loss: 0.6912222211753024, Validation Loss: 0.6907769938745189\n",
      "Epoch 187/3000, Training Loss: 0.6912081869605867, Validation Loss: 0.6907640738928659\n",
      "Epoch 188/3000, Training Loss: 0.6911940465592378, Validation Loss: 0.6907510564029299\n",
      "Epoch 189/3000, Training Loss: 0.6911798019526004, Validation Loss: 0.6907379403655893\n",
      "Epoch 190/3000, Training Loss: 0.6911654519529339, Validation Loss: 0.6907247247536666\n",
      "Epoch 191/3000, Training Loss: 0.6911509931648078, Validation Loss: 0.6907114079085342\n",
      "Epoch 192/3000, Training Loss: 0.6911364177793209, Validation Loss: 0.6906979868508906\n",
      "Epoch 193/3000, Training Loss: 0.691121733086582, Validation Loss: 0.6906844608968902\n",
      "Epoch 194/3000, Training Loss: 0.691106937446467, Validation Loss: 0.6906708301290452\n",
      "Epoch 195/3000, Training Loss: 0.6910920285772525, Validation Loss: 0.6906570930489125\n",
      "Epoch 196/3000, Training Loss: 0.6910770065536321, Validation Loss: 0.6906432483665468\n",
      "Epoch 197/3000, Training Loss: 0.6910618700665185, Validation Loss: 0.6906292950942864\n",
      "Epoch 198/3000, Training Loss: 0.6910466177889131, Validation Loss: 0.6906152323307605\n",
      "Epoch 199/3000, Training Loss: 0.6910312482130871, Validation Loss: 0.690601059082351\n",
      "Epoch 200/3000, Training Loss: 0.6910157570933602, Validation Loss: 0.69058677387026\n",
      "Epoch 201/3000, Training Loss: 0.6910001432029527, Validation Loss: 0.6905723754273604\n",
      "Epoch 202/3000, Training Loss: 0.6909844068483495, Validation Loss: 0.6905578629201783\n",
      "Epoch 203/3000, Training Loss: 0.6909685423982898, Validation Loss: 0.6905432332423009\n",
      "Epoch 204/3000, Training Loss: 0.6909525527689143, Validation Loss: 0.6905284848864385\n",
      "Epoch 205/3000, Training Loss: 0.6909364371987989, Validation Loss: 0.6905136165633348\n",
      "Epoch 206/3000, Training Loss: 0.6909201944705692, Validation Loss: 0.6904986272870773\n",
      "Epoch 207/3000, Training Loss: 0.690903823429999, Validation Loss: 0.6904835161375293\n",
      "Epoch 208/3000, Training Loss: 0.6908873243820407, Validation Loss: 0.6904682830486352\n",
      "Epoch 209/3000, Training Loss: 0.6908706990396465, Validation Loss: 0.6904529260930666\n",
      "Epoch 210/3000, Training Loss: 0.6908539287812331, Validation Loss: 0.6904374413334328\n",
      "Epoch 211/3000, Training Loss: 0.690837015219041, Validation Loss: 0.6904218268258843\n",
      "Epoch 212/3000, Training Loss: 0.690819964536155, Validation Loss: 0.6904060815968497\n",
      "Epoch 213/3000, Training Loss: 0.6908027750843191, Validation Loss: 0.6903902047670885\n",
      "Epoch 214/3000, Training Loss: 0.6907854451914751, Validation Loss: 0.6903741949814586\n",
      "Epoch 215/3000, Training Loss: 0.6907679731613415, Validation Loss: 0.6903580509401972\n",
      "Epoch 216/3000, Training Loss: 0.6907503572729847, Validation Loss: 0.690341771267679\n",
      "Epoch 217/3000, Training Loss: 0.69073259578038, Validation Loss: 0.690325354364512\n",
      "Epoch 218/3000, Training Loss: 0.6907146872664814, Validation Loss: 0.6903087989935094\n",
      "Epoch 219/3000, Training Loss: 0.6906966302900792, Validation Loss: 0.6902921036087777\n",
      "Epoch 220/3000, Training Loss: 0.6906784214633839, Validation Loss: 0.6902752662565531\n",
      "Epoch 221/3000, Training Loss: 0.6906600544179593, Validation Loss: 0.6902582839587557\n",
      "Epoch 222/3000, Training Loss: 0.6906415328337813, Validation Loss: 0.6902411550627031\n",
      "Epoch 223/3000, Training Loss: 0.6906228533095914, Validation Loss: 0.6902238782893991\n",
      "Epoch 224/3000, Training Loss: 0.6906040160298028, Validation Loss: 0.6902064514636631\n",
      "Epoch 225/3000, Training Loss: 0.6905850152832053, Validation Loss: 0.6901888717817766\n",
      "Epoch 226/3000, Training Loss: 0.6905658473108409, Validation Loss: 0.6901711374573384\n",
      "Epoch 227/3000, Training Loss: 0.6905465144591029, Validation Loss: 0.6901532474134598\n",
      "Epoch 228/3000, Training Loss: 0.6905270116191419, Validation Loss: 0.6901351991377082\n",
      "Epoch 229/3000, Training Loss: 0.6905073350230204, Validation Loss: 0.6901169910821595\n",
      "Epoch 230/3000, Training Loss: 0.6904874870717436, Validation Loss: 0.6900986212688554\n",
      "Epoch 231/3000, Training Loss: 0.6904674621663147, Validation Loss: 0.6900800870660629\n",
      "Epoch 232/3000, Training Loss: 0.690447257623849, Validation Loss: 0.690061385963589\n",
      "Epoch 233/3000, Training Loss: 0.6904268749537517, Validation Loss: 0.6900425170045057\n",
      "Epoch 234/3000, Training Loss: 0.6904063129713585, Validation Loss: 0.6900234794376029\n",
      "Epoch 235/3000, Training Loss: 0.690385570764958, Validation Loss: 0.6900042707427277\n",
      "Epoch 236/3000, Training Loss: 0.6903646446277003, Validation Loss: 0.6899848885233408\n",
      "Epoch 237/3000, Training Loss: 0.690343530497414, Validation Loss: 0.6899653307802206\n",
      "Epoch 238/3000, Training Loss: 0.6903222253893738, Validation Loss: 0.689945594915618\n",
      "Epoch 239/3000, Training Loss: 0.6903007247858908, Validation Loss: 0.6899256779626071\n",
      "Epoch 240/3000, Training Loss: 0.6902790221771066, Validation Loss: 0.6899055770760137\n",
      "Epoch 241/3000, Training Loss: 0.6902571219757244, Validation Loss: 0.6898852904553674\n",
      "Epoch 242/3000, Training Loss: 0.6902350215125035, Validation Loss: 0.689864815655907\n",
      "Epoch 243/3000, Training Loss: 0.6902127137353723, Validation Loss: 0.6898441499632545\n",
      "Epoch 244/3000, Training Loss: 0.6901901952820557, Validation Loss: 0.6898232913298216\n",
      "Epoch 245/3000, Training Loss: 0.6901674654607147, Validation Loss: 0.6898022368642356\n",
      "Epoch 246/3000, Training Loss: 0.6901445165160142, Validation Loss: 0.6897809842890776\n",
      "Epoch 247/3000, Training Loss: 0.6901213539975533, Validation Loss: 0.6897595320822543\n",
      "Epoch 248/3000, Training Loss: 0.6900979820216221, Validation Loss: 0.6897378775930721\n",
      "Epoch 249/3000, Training Loss: 0.6900743904412048, Validation Loss: 0.6897160181599781\n",
      "Epoch 250/3000, Training Loss: 0.6900505794016665, Validation Loss: 0.68969395260506\n",
      "Epoch 251/3000, Training Loss: 0.6900265542455739, Validation Loss: 0.6896716804453664\n",
      "Epoch 252/3000, Training Loss: 0.6900023008263855, Validation Loss: 0.6896491972378154\n",
      "Epoch 253/3000, Training Loss: 0.6899778171709104, Validation Loss: 0.6896265007650794\n",
      "Epoch 254/3000, Training Loss: 0.6899531004036799, Validation Loss: 0.6896035857914807\n",
      "Epoch 255/3000, Training Loss: 0.6899281391831203, Validation Loss: 0.6895804482875274\n",
      "Epoch 256/3000, Training Loss: 0.689902936877948, Validation Loss: 0.6895570901862503\n",
      "Epoch 257/3000, Training Loss: 0.6898774903767446, Validation Loss: 0.6895335052469543\n",
      "Epoch 258/3000, Training Loss: 0.6898518010985876, Validation Loss: 0.6895096901654089\n",
      "Epoch 259/3000, Training Loss: 0.6898258608224815, Validation Loss: 0.6894856419838259\n",
      "Epoch 260/3000, Training Loss: 0.689799661496733, Validation Loss: 0.6894613558754941\n",
      "Epoch 261/3000, Training Loss: 0.6897731949900378, Validation Loss: 0.6894368272009196\n",
      "Epoch 262/3000, Training Loss: 0.6897464632380995, Validation Loss: 0.6894120528249351\n",
      "Epoch 263/3000, Training Loss: 0.6897194642956719, Validation Loss: 0.6893706012432326\n",
      "Epoch 264/3000, Training Loss: 0.6896922004147399, Validation Loss: 0.6893482905038725\n",
      "Epoch 265/3000, Training Loss: 0.68966464204406, Validation Loss: 0.6893251885436537\n",
      "Epoch 266/3000, Training Loss: 0.6896368084597753, Validation Loss: 0.6893013903873448\n",
      "Epoch 267/3000, Training Loss: 0.6896087010179788, Validation Loss: 0.6892769733673942\n",
      "Epoch 268/3000, Training Loss: 0.6895803260499471, Validation Loss: 0.6892352951465955\n",
      "Epoch 269/3000, Training Loss: 0.6895516675025934, Validation Loss: 0.689212838126868\n",
      "Epoch 270/3000, Training Loss: 0.6895227069821003, Validation Loss: 0.6891893622611078\n",
      "Epoch 271/3000, Training Loss: 0.6894934495065304, Validation Loss: 0.6891649992083241\n",
      "Epoch 272/3000, Training Loss: 0.6894638916505068, Validation Loss: 0.6891398569517999\n",
      "Epoch 273/3000, Training Loss: 0.6894340493305702, Validation Loss: 0.6890970366039443\n",
      "Epoch 274/3000, Training Loss: 0.6894038803074527, Validation Loss: 0.6890736714635276\n",
      "Epoch 275/3000, Training Loss: 0.6893733949224942, Validation Loss: 0.6890491761103918\n",
      "Epoch 276/3000, Training Loss: 0.6893425817750153, Validation Loss: 0.6890236995950182\n",
      "Epoch 277/3000, Training Loss: 0.6893114673544951, Validation Loss: 0.6889801382314493\n",
      "Epoch 278/3000, Training Loss: 0.6892800195073584, Validation Loss: 0.6889561798573338\n",
      "Epoch 279/3000, Training Loss: 0.6892482304332178, Validation Loss: 0.6889309619689353\n",
      "Epoch 280/3000, Training Loss: 0.6892161042061014, Validation Loss: 0.6889046527708355\n",
      "Epoch 281/3000, Training Loss: 0.6891836506763618, Validation Loss: 0.6888599292483393\n",
      "Epoch 282/3000, Training Loss: 0.6891508540947046, Validation Loss: 0.6888350285822977\n",
      "Epoch 283/3000, Training Loss: 0.6891176928576406, Validation Loss: 0.688808787583007\n",
      "Epoch 284/3000, Training Loss: 0.6890841729532396, Validation Loss: 0.6887813891969659\n",
      "Epoch 285/3000, Training Loss: 0.6890502923113001, Validation Loss: 0.6887352728557007\n",
      "Epoch 286/3000, Training Loss: 0.6890160505766908, Validation Loss: 0.6887092316750354\n",
      "Epoch 287/3000, Training Loss: 0.6889814199057439, Validation Loss: 0.6886817923670728\n",
      "Epoch 288/3000, Training Loss: 0.6889464090723653, Validation Loss: 0.6886531431826827\n",
      "Epoch 289/3000, Training Loss: 0.6889110191430758, Validation Loss: 0.6886054762056091\n",
      "Epoch 290/3000, Training Loss: 0.6888752479755454, Validation Loss: 0.688578154989986\n",
      "Epoch 291/3000, Training Loss: 0.6888390640470261, Validation Loss: 0.6885493851073882\n",
      "Epoch 292/3000, Training Loss: 0.6888024763172674, Validation Loss: 0.6885193610782552\n",
      "Epoch 293/3000, Training Loss: 0.6887654881188436, Validation Loss: 0.6884700210480822\n",
      "Epoch 294/3000, Training Loss: 0.6887280897673622, Validation Loss: 0.6884413110659093\n",
      "Epoch 295/3000, Training Loss: 0.6886902571364661, Validation Loss: 0.6884111050129388\n",
      "Epoch 296/3000, Training Loss: 0.6886519950529646, Validation Loss: 0.6883796020461678\n",
      "Epoch 297/3000, Training Loss: 0.6886133105038159, Validation Loss: 0.6883284795976344\n",
      "Epoch 298/3000, Training Loss: 0.6885741798351734, Validation Loss: 0.6882982811427533\n",
      "Epoch 299/3000, Training Loss: 0.6885345975061061, Validation Loss: 0.6882665402598276\n",
      "Epoch 300/3000, Training Loss: 0.6884945646697826, Validation Loss: 0.6882334613552502\n",
      "Epoch 301/3000, Training Loss: 0.6884541052307518, Validation Loss: 0.6881616869882945\n",
      "Epoch 302/3000, Training Loss: 0.6884131603860085, Validation Loss: 0.6881334092449048\n",
      "Epoch 303/3000, Training Loss: 0.6883717197681266, Validation Loss: 0.6881028851347564\n",
      "Epoch 304/3000, Training Loss: 0.6883297931587861, Validation Loss: 0.6880704440358969\n",
      "Epoch 305/3000, Training Loss: 0.6882873620851081, Validation Loss: 0.68803634678695\n",
      "Epoch 306/3000, Training Loss: 0.6882444168807708, Validation Loss: 0.6880008060738771\n",
      "Epoch 307/3000, Training Loss: 0.6882009658667299, Validation Loss: 0.6879640033162806\n",
      "Epoch 308/3000, Training Loss: 0.6881570463347695, Validation Loss: 0.6878875708843148\n",
      "Epoch 309/3000, Training Loss: 0.6881125595462572, Validation Loss: 0.6878558970692411\n",
      "Epoch 310/3000, Training Loss: 0.6880675299560024, Validation Loss: 0.6878219324892003\n",
      "Epoch 311/3000, Training Loss: 0.6880219559076939, Validation Loss: 0.6877860121081547\n",
      "Epoch 312/3000, Training Loss: 0.6879758276888692, Validation Loss: 0.6877483960892155\n",
      "Epoch 313/3000, Training Loss: 0.6879291386784193, Validation Loss: 0.6877093014909264\n",
      "Epoch 314/3000, Training Loss: 0.6878819171028613, Validation Loss: 0.687629510889397\n",
      "Epoch 315/3000, Training Loss: 0.687834126848608, Validation Loss: 0.6875954288389804\n",
      "Epoch 316/3000, Training Loss: 0.6877857075825445, Validation Loss: 0.6875588649256822\n",
      "Epoch 317/3000, Training Loss: 0.6877366875741068, Validation Loss: 0.6875201845126495\n",
      "Epoch 318/3000, Training Loss: 0.687687056614799, Validation Loss: 0.6874796746513581\n",
      "Epoch 319/3000, Training Loss: 0.6876368040891799, Validation Loss: 0.6874375621776565\n",
      "Epoch 320/3000, Training Loss: 0.6875859397816012, Validation Loss: 0.6873738547423811\n",
      "Epoch 321/3000, Training Loss: 0.6875344219355034, Validation Loss: 0.687332926722239\n",
      "Epoch 322/3000, Training Loss: 0.6874822420125497, Validation Loss: 0.6872901099240866\n",
      "Epoch 323/3000, Training Loss: 0.6874294255667753, Validation Loss: 0.6872252211900564\n",
      "Epoch 324/3000, Training Loss: 0.6873759152066227, Validation Loss: 0.6871832102747674\n",
      "Epoch 325/3000, Training Loss: 0.6873216987621087, Validation Loss: 0.6871391311021406\n",
      "Epoch 326/3000, Training Loss: 0.6872668217561169, Validation Loss: 0.6870725861430361\n",
      "Epoch 327/3000, Training Loss: 0.687211212436635, Validation Loss: 0.6870290962059045\n",
      "Epoch 328/3000, Training Loss: 0.687154881340916, Validation Loss: 0.6869625942673396\n",
      "Epoch 329/3000, Training Loss: 0.6870978533788747, Validation Loss: 0.6869190287119576\n",
      "Epoch 330/3000, Training Loss: 0.6870400534853853, Validation Loss: 0.6868730090397602\n",
      "Epoch 331/3000, Training Loss: 0.6869815035639775, Validation Loss: 0.6868037829307693\n",
      "Epoch 332/3000, Training Loss: 0.6869222002942416, Validation Loss: 0.6867578477148136\n",
      "Epoch 333/3000, Training Loss: 0.6868621106243482, Validation Loss: 0.6867094615292102\n",
      "Epoch 334/3000, Training Loss: 0.6868012392387525, Validation Loss: 0.6866376093819098\n",
      "Epoch 335/3000, Training Loss: 0.6867395445370585, Validation Loss: 0.686589366338566\n",
      "Epoch 336/3000, Training Loss: 0.6866770024992512, Validation Loss: 0.6865386449369403\n",
      "Epoch 337/3000, Training Loss: 0.686613651403274, Validation Loss: 0.6864641622355326\n",
      "Epoch 338/3000, Training Loss: 0.6865494068610454, Validation Loss: 0.6864135917999121\n",
      "Epoch 339/3000, Training Loss: 0.6864842921167585, Validation Loss: 0.6863387206095669\n",
      "Epoch 340/3000, Training Loss: 0.6864183090259945, Validation Loss: 0.6862876840730776\n",
      "Epoch 341/3000, Training Loss: 0.6863514202471213, Validation Loss: 0.6862338605741884\n",
      "Epoch 342/3000, Training Loss: 0.6862836368564473, Validation Loss: 0.6861555724791706\n",
      "Epoch 343/3000, Training Loss: 0.6862148895240867, Validation Loss: 0.6861015460297974\n",
      "Epoch 344/3000, Training Loss: 0.6861451791895348, Validation Loss: 0.6860225298657101\n",
      "Epoch 345/3000, Training Loss: 0.6860745087606179, Validation Loss: 0.6859677392141029\n",
      "Epoch 346/3000, Training Loss: 0.686002806516769, Validation Loss: 0.6859099412950411\n",
      "Epoch 347/3000, Training Loss: 0.6859301214184446, Validation Loss: 0.6858270220701198\n",
      "Epoch 348/3000, Training Loss: 0.6858563719705489, Validation Loss: 0.6857688051834078\n",
      "Epoch 349/3000, Training Loss: 0.6857815740024684, Validation Loss: 0.6856849355009239\n",
      "Epoch 350/3000, Training Loss: 0.6857057103668044, Validation Loss: 0.6856257475317646\n",
      "Epoch 351/3000, Training Loss: 0.6856287472176184, Validation Loss: 0.6855404680579413\n",
      "Epoch 352/3000, Training Loss: 0.6855507078532074, Validation Loss: 0.6854799346258031\n",
      "Epoch 353/3000, Training Loss: 0.6854714822179269, Validation Loss: 0.6854160357203911\n",
      "Epoch 354/3000, Training Loss: 0.6853911275331519, Validation Loss: 0.6853260032855133\n",
      "Epoch 355/3000, Training Loss: 0.6853095689266744, Validation Loss: 0.6852612925367353\n",
      "Epoch 356/3000, Training Loss: 0.6852268230878543, Validation Loss: 0.6851699267314089\n",
      "Epoch 357/3000, Training Loss: 0.6851428804862467, Validation Loss: 0.685103892499275\n",
      "Epoch 358/3000, Training Loss: 0.685057662459348, Validation Loss: 0.6850107565021076\n",
      "Epoch 359/3000, Training Loss: 0.6849712082225836, Validation Loss: 0.6849430311410487\n",
      "Epoch 360/3000, Training Loss: 0.6848834054441205, Validation Loss: 0.6848478043588087\n",
      "Epoch 361/3000, Training Loss: 0.6847943202429707, Validation Loss: 0.6847781111492783\n",
      "Epoch 362/3000, Training Loss: 0.6847038312439337, Validation Loss: 0.6847046216532845\n",
      "Epoch 363/3000, Training Loss: 0.6846120254370001, Validation Loss: 0.6846036544492646\n",
      "Epoch 364/3000, Training Loss: 0.6845187800601865, Validation Loss: 0.6845288995390619\n",
      "Epoch 365/3000, Training Loss: 0.6844241294502773, Validation Loss: 0.6844261196547797\n",
      "Epoch 366/3000, Training Loss: 0.6843280220178403, Validation Loss: 0.6843495734295263\n",
      "Epoch 367/3000, Training Loss: 0.6842304301135556, Validation Loss: 0.6842445257196244\n",
      "Epoch 368/3000, Training Loss: 0.6841312653461802, Validation Loss: 0.6841658032361762\n",
      "Epoch 369/3000, Training Loss: 0.6840305193153187, Validation Loss: 0.6840581517280885\n",
      "Epoch 370/3000, Training Loss: 0.6839281719931296, Validation Loss: 0.6839769554976668\n",
      "Epoch 371/3000, Training Loss: 0.6838242012089227, Validation Loss: 0.6838664304718628\n",
      "Epoch 372/3000, Training Loss: 0.683718555724705, Validation Loss: 0.6837825287727944\n",
      "Epoch 373/3000, Training Loss: 0.6836112143738314, Validation Loss: 0.6836689133211352\n",
      "Epoch 374/3000, Training Loss: 0.6835021196597948, Validation Loss: 0.6835821092129292\n",
      "Epoch 375/3000, Training Loss: 0.6833912605652673, Validation Loss: 0.6834652586652219\n",
      "Epoch 376/3000, Training Loss: 0.6832785657273698, Validation Loss: 0.6833753621047405\n",
      "Epoch 377/3000, Training Loss: 0.6831640491350253, Validation Loss: 0.6832550062345338\n",
      "Epoch 378/3000, Training Loss: 0.6830476308924865, Validation Loss: 0.6831618758068753\n",
      "Epoch 379/3000, Training Loss: 0.6829293139271977, Validation Loss: 0.6830379056150303\n",
      "Epoch 380/3000, Training Loss: 0.6828089993704006, Validation Loss: 0.6829151528643038\n",
      "Epoch 381/3000, Training Loss: 0.6826867553873462, Validation Loss: 0.6828193308770456\n",
      "Epoch 382/3000, Training Loss: 0.6825624395283081, Validation Loss: 0.682691565796612\n",
      "Epoch 383/3000, Training Loss: 0.6824360628498413, Validation Loss: 0.6825911889358035\n",
      "Epoch 384/3000, Training Loss: 0.682307547725178, Validation Loss: 0.6824586653273823\n",
      "Epoch 385/3000, Training Loss: 0.6821768711963498, Validation Loss: 0.6823539032748855\n",
      "Epoch 386/3000, Training Loss: 0.6820439241285091, Validation Loss: 0.6822166960937327\n",
      "Epoch 387/3000, Training Loss: 0.6819085885734768, Validation Loss: 0.6821075529580689\n",
      "Epoch 388/3000, Training Loss: 0.6817709560725707, Validation Loss: 0.6819656165194621\n",
      "Epoch 389/3000, Training Loss: 0.6816308796533238, Validation Loss: 0.6818245896186537\n",
      "Epoch 390/3000, Training Loss: 0.6814883680255649, Validation Loss: 0.6817113518354357\n",
      "Epoch 391/3000, Training Loss: 0.6813433030331835, Validation Loss: 0.6815641598652404\n",
      "Epoch 392/3000, Training Loss: 0.6811956388620438, Validation Loss: 0.6814452960154243\n",
      "Epoch 393/3000, Training Loss: 0.6810453861216929, Validation Loss: 0.6812922730449308\n",
      "Epoch 394/3000, Training Loss: 0.6808923852770494, Validation Loss: 0.681139815513507\n",
      "Epoch 395/3000, Training Loss: 0.6807366590510123, Validation Loss: 0.6810155362932276\n",
      "Epoch 396/3000, Training Loss: 0.680578078004456, Validation Loss: 0.6808559783062451\n",
      "Epoch 397/3000, Training Loss: 0.6804165491100865, Validation Loss: 0.6806965664394363\n",
      "Epoch 398/3000, Training Loss: 0.6802521023621245, Validation Loss: 0.6805655211534467\n",
      "Epoch 399/3000, Training Loss: 0.6800845775891743, Validation Loss: 0.680398124270255\n",
      "Epoch 400/3000, Training Loss: 0.6799139261534144, Validation Loss: 0.6802306228143149\n",
      "Epoch 401/3000, Training Loss: 0.679740139240498, Validation Loss: 0.6800916320813915\n",
      "Epoch 402/3000, Training Loss: 0.6795630353521516, Validation Loss: 0.679915530580057\n",
      "Epoch 403/3000, Training Loss: 0.6793825658481256, Validation Loss: 0.6797390997884886\n",
      "Epoch 404/3000, Training Loss: 0.6791986910686901, Validation Loss: 0.6795914166912544\n",
      "Epoch 405/3000, Training Loss: 0.6790112756957853, Validation Loss: 0.6794057380565424\n",
      "Epoch 406/3000, Training Loss: 0.6788202473107308, Validation Loss: 0.6792195625928483\n",
      "Epoch 407/3000, Training Loss: 0.6786255846045942, Validation Loss: 0.6790624416886378\n",
      "Epoch 408/3000, Training Loss: 0.6784271665391126, Validation Loss: 0.6788664263952641\n",
      "Epoch 409/3000, Training Loss: 0.6782249083695695, Validation Loss: 0.6786697142565259\n",
      "Epoch 410/3000, Training Loss: 0.6780186604631733, Validation Loss: 0.6785023505260607\n",
      "Epoch 411/3000, Training Loss: 0.6778083422445199, Validation Loss: 0.6782951496746594\n",
      "Epoch 412/3000, Training Loss: 0.6775938128448789, Validation Loss: 0.6780870904595198\n",
      "Epoch 413/3000, Training Loss: 0.6773750106154048, Validation Loss: 0.6778772855804932\n",
      "Epoch 414/3000, Training Loss: 0.6771518366281029, Validation Loss: 0.6776966954867952\n",
      "Epoch 415/3000, Training Loss: 0.6769241746098884, Validation Loss: 0.6774745115791921\n",
      "Epoch 416/3000, Training Loss: 0.6766918293927847, Validation Loss: 0.677250790716554\n",
      "Epoch 417/3000, Training Loss: 0.6764546514336719, Validation Loss: 0.6770247740199078\n",
      "Epoch 418/3000, Training Loss: 0.6762123986739275, Validation Loss: 0.6767959027753209\n",
      "Epoch 419/3000, Training Loss: 0.6759651244905851, Validation Loss: 0.6765960004367133\n",
      "Epoch 420/3000, Training Loss: 0.6757126955850279, Validation Loss: 0.6763521967519784\n",
      "Epoch 421/3000, Training Loss: 0.6754549143368898, Validation Loss: 0.6761060132456296\n",
      "Epoch 422/3000, Training Loss: 0.6751916571910497, Validation Loss: 0.6758567243674525\n",
      "Epoch 423/3000, Training Loss: 0.67492278637963, Validation Loss: 0.6756037434209654\n",
      "Epoch 424/3000, Training Loss: 0.6746481613334148, Validation Loss: 0.6753466637947151\n",
      "Epoch 425/3000, Training Loss: 0.6743676422070044, Validation Loss: 0.675118599797345\n",
      "Epoch 426/3000, Training Loss: 0.6740810216634333, Validation Loss: 0.6748438942946691\n",
      "Epoch 427/3000, Training Loss: 0.6737881411465733, Validation Loss: 0.6745659028534952\n",
      "Epoch 428/3000, Training Loss: 0.6734888199003676, Validation Loss: 0.6742838498262342\n",
      "Epoch 429/3000, Training Loss: 0.6731828118925895, Validation Loss: 0.6739971641796441\n",
      "Epoch 430/3000, Training Loss: 0.6728699214230125, Validation Loss: 0.6737051798767473\n",
      "Epoch 431/3000, Training Loss: 0.6725500634753936, Validation Loss: 0.673431487612752\n",
      "Epoch 432/3000, Training Loss: 0.6722229932645156, Validation Loss: 0.6731219894691616\n",
      "Epoch 433/3000, Training Loss: 0.6718885387348091, Validation Loss: 0.6728319955832948\n",
      "Epoch 434/3000, Training Loss: 0.6715464279169456, Validation Loss: 0.6725305582083986\n",
      "Epoch 435/3000, Training Loss: 0.6711966036680705, Validation Loss: 0.6721937485598827\n",
      "Epoch 436/3000, Training Loss: 0.6708386752389737, Validation Loss: 0.671878010719909\n",
      "Epoch 437/3000, Training Loss: 0.6704724718171997, Validation Loss: 0.6715259067899038\n",
      "Epoch 438/3000, Training Loss: 0.6700977106052753, Validation Loss: 0.6711950493227974\n",
      "Epoch 439/3000, Training Loss: 0.6697141271280512, Validation Loss: 0.6708525183508409\n",
      "Epoch 440/3000, Training Loss: 0.6693215004304033, Validation Loss: 0.6704729469003665\n",
      "Epoch 441/3000, Training Loss: 0.668919532025019, Validation Loss: 0.6701154175526352\n",
      "Epoch 442/3000, Training Loss: 0.6685079358175777, Validation Loss: 0.6697459445954923\n",
      "Epoch 443/3000, Training Loss: 0.6680864333772768, Validation Loss: 0.6693380112138014\n",
      "Epoch 444/3000, Training Loss: 0.6676546668036475, Validation Loss: 0.6689525509602786\n",
      "Epoch 445/3000, Training Loss: 0.6672124055948997, Validation Loss: 0.6685544258554961\n",
      "Epoch 446/3000, Training Loss: 0.6667592856270612, Validation Loss: 0.6681163244377977\n",
      "Epoch 447/3000, Training Loss: 0.6662949707824531, Validation Loss: 0.6677009209664656\n",
      "Epoch 448/3000, Training Loss: 0.6658191328636435, Validation Loss: 0.6672718269407292\n",
      "Epoch 449/3000, Training Loss: 0.6653314953895374, Validation Loss: 0.6668293314968108\n",
      "Epoch 450/3000, Training Loss: 0.6648315374507858, Validation Loss: 0.6663737377595225\n",
      "Epoch 451/3000, Training Loss: 0.664318889461529, Validation Loss: 0.6658760755717513\n",
      "Epoch 452/3000, Training Loss: 0.6637931266549736, Validation Loss: 0.6654020942927888\n",
      "Epoch 453/3000, Training Loss: 0.6632538742131908, Validation Loss: 0.664912804981135\n",
      "Epoch 454/3000, Training Loss: 0.6627007203397295, Validation Loss: 0.6644086641773853\n",
      "Epoch 455/3000, Training Loss: 0.6621329368470263, Validation Loss: 0.6638483029505204\n",
      "Epoch 456/3000, Training Loss: 0.6615503329222518, Validation Loss: 0.6633255722026229\n",
      "Epoch 457/3000, Training Loss: 0.6609523635055293, Validation Loss: 0.662784820471955\n",
      "Epoch 458/3000, Training Loss: 0.6603385470889698, Validation Loss: 0.6621952952780485\n",
      "Epoch 459/3000, Training Loss: 0.6597084035132724, Validation Loss: 0.6615857634537979\n",
      "Epoch 460/3000, Training Loss: 0.6590613441695982, Validation Loss: 0.661011102286566\n",
      "Epoch 461/3000, Training Loss: 0.6583968296526306, Validation Loss: 0.660414013454031\n",
      "Epoch 462/3000, Training Loss: 0.6577143289820616, Validation Loss: 0.6597523597751979\n",
      "Epoch 463/3000, Training Loss: 0.6570131837019424, Validation Loss: 0.6591258974801993\n",
      "Epoch 464/3000, Training Loss: 0.6562927643558981, Validation Loss: 0.6584316678390478\n",
      "Epoch 465/3000, Training Loss: 0.6555524893432424, Validation Loss: 0.6577714778813768\n",
      "Epoch 466/3000, Training Loss: 0.6547915072077808, Validation Loss: 0.6570856562099876\n",
      "Epoch 467/3000, Training Loss: 0.654009145757341, Validation Loss: 0.6563299472829137\n",
      "Epoch 468/3000, Training Loss: 0.6532043644207328, Validation Loss: 0.655608804223696\n",
      "Epoch 469/3000, Training Loss: 0.6523768730515568, Validation Loss: 0.654814199773169\n",
      "Epoch 470/3000, Training Loss: 0.6515258327651217, Validation Loss: 0.6540527038266944\n",
      "Epoch 471/3000, Training Loss: 0.6506507395427995, Validation Loss: 0.6533115086590499\n",
      "Epoch 472/3000, Training Loss: 0.6497508785617635, Validation Loss: 0.6525744129901815\n",
      "Epoch 473/3000, Training Loss: 0.6488252698348852, Validation Loss: 0.6517378940092279\n",
      "Epoch 474/3000, Training Loss: 0.6478727869809231, Validation Loss: 0.6508707254105309\n",
      "Epoch 475/3000, Training Loss: 0.6468924985624799, Validation Loss: 0.6500216875225254\n",
      "Epoch 476/3000, Training Loss: 0.6458833973633924, Validation Loss: 0.6490801578685387\n",
      "Epoch 477/3000, Training Loss: 0.6448445479432935, Validation Loss: 0.6481623011467038\n",
      "Epoch 478/3000, Training Loss: 0.6437749753045942, Validation Loss: 0.6471530741828967\n",
      "Epoch 479/3000, Training Loss: 0.6426732609806108, Validation Loss: 0.6461200340021441\n",
      "Epoch 480/3000, Training Loss: 0.6415385742080263, Validation Loss: 0.6451103748342288\n",
      "Epoch 481/3000, Training Loss: 0.6403697861073806, Validation Loss: 0.6440053592907042\n",
      "Epoch 482/3000, Training Loss: 0.6391652335697833, Validation Loss: 0.6428742384195466\n",
      "Epoch 483/3000, Training Loss: 0.6379233280478681, Validation Loss: 0.6417134367113299\n",
      "Epoch 484/3000, Training Loss: 0.6366433363749227, Validation Loss: 0.640571963299848\n",
      "Epoch 485/3000, Training Loss: 0.6353238676534363, Validation Loss: 0.6393266960187146\n",
      "Epoch 486/3000, Training Loss: 0.6339633706402636, Validation Loss: 0.63805027820744\n",
      "Epoch 487/3000, Training Loss: 0.6325604487694388, Validation Loss: 0.6367387938437472\n",
      "Epoch 488/3000, Training Loss: 0.6311137090302977, Validation Loss: 0.6353893700023516\n",
      "Epoch 489/3000, Training Loss: 0.6296215417252623, Validation Loss: 0.6339997190991913\n",
      "Epoch 490/3000, Training Loss: 0.6280823034565185, Validation Loss: 0.6325673843641396\n",
      "Epoch 491/3000, Training Loss: 0.6264941987273787, Validation Loss: 0.6310903764407751\n",
      "Epoch 492/3000, Training Loss: 0.624855392595625, Validation Loss: 0.6295666501153233\n",
      "Epoch 493/3000, Training Loss: 0.6231640600364958, Validation Loss: 0.6279943018362231\n",
      "Epoch 494/3000, Training Loss: 0.6214180078068728, Validation Loss: 0.6263714917041365\n",
      "Epoch 495/3000, Training Loss: 0.6196150255454584, Validation Loss: 0.6246958320242619\n",
      "Epoch 496/3000, Training Loss: 0.6177536421336165, Validation Loss: 0.6229655239764351\n",
      "Epoch 497/3000, Training Loss: 0.6158324273124616, Validation Loss: 0.6213025156060044\n",
      "Epoch 498/3000, Training Loss: 0.6138501632204195, Validation Loss: 0.6195366149558736\n",
      "Epoch 499/3000, Training Loss: 0.6118033354483196, Validation Loss: 0.6176814218541259\n",
      "Epoch 500/3000, Training Loss: 0.6096894598501167, Validation Loss: 0.6157451728977822\n",
      "Epoch 501/3000, Training Loss: 0.6075062949240534, Validation Loss: 0.6137320767073365\n",
      "Epoch 502/3000, Training Loss: 0.6052524589783219, Validation Loss: 0.6116439943167715\n",
      "Epoch 503/3000, Training Loss: 0.6029244309961468, Validation Loss: 0.6094815165121512\n",
      "Epoch 504/3000, Training Loss: 0.600519595872997, Validation Loss: 0.6072440409208243\n",
      "Epoch 505/3000, Training Loss: 0.5980354572313268, Validation Loss: 0.6049833083344248\n",
      "Epoch 506/3000, Training Loss: 0.5954694694802231, Validation Loss: 0.6026771959276604\n",
      "Epoch 507/3000, Training Loss: 0.5928186616034196, Validation Loss: 0.6001945816436495\n",
      "Epoch 508/3000, Training Loss: 0.5900801211326687, Validation Loss: 0.5976969286798294\n",
      "Epoch 509/3000, Training Loss: 0.5872507176236045, Validation Loss: 0.5950275030090001\n",
      "Epoch 510/3000, Training Loss: 0.5843277844610905, Validation Loss: 0.5923459027442644\n",
      "Epoch 511/3000, Training Loss: 0.581308528030932, Validation Loss: 0.5896037712285555\n",
      "Epoch 512/3000, Training Loss: 0.5781900474366868, Validation Loss: 0.5866865899356764\n",
      "Epoch 513/3000, Training Loss: 0.5749691333264261, Validation Loss: 0.5836087380775717\n",
      "Epoch 514/3000, Training Loss: 0.5716428626240793, Validation Loss: 0.5805775142795465\n",
      "Epoch 515/3000, Training Loss: 0.5682085154860492, Validation Loss: 0.5774367430389086\n",
      "Epoch 516/3000, Training Loss: 0.5646638818277578, Validation Loss: 0.5742796455795581\n",
      "Epoch 517/3000, Training Loss: 0.5610053479926439, Validation Loss: 0.5711168981176228\n",
      "Epoch 518/3000, Training Loss: 0.55722969451916, Validation Loss: 0.567918149554243\n",
      "Epoch 519/3000, Training Loss: 0.5533348178971853, Validation Loss: 0.5640944990139712\n",
      "Epoch 520/3000, Training Loss: 0.5493167081496949, Validation Loss: 0.5606777733375136\n",
      "Epoch 521/3000, Training Loss: 0.545171641566838, Validation Loss: 0.5568356992844923\n",
      "Epoch 522/3000, Training Loss: 0.5408968594919604, Validation Loss: 0.5528537371165142\n",
      "Epoch 523/3000, Training Loss: 0.5364891719280809, Validation Loss: 0.5487382172603146\n",
      "Epoch 524/3000, Training Loss: 0.5319468954472109, Validation Loss: 0.5447037019163296\n",
      "Epoch 525/3000, Training Loss: 0.5272688265434771, Validation Loss: 0.5403726129051077\n",
      "Epoch 526/3000, Training Loss: 0.5224519334463474, Validation Loss: 0.535986664208989\n",
      "Epoch 527/3000, Training Loss: 0.51749448378979, Validation Loss: 0.5314242172663916\n",
      "Epoch 528/3000, Training Loss: 0.512394692641588, Validation Loss: 0.5267241616175685\n",
      "Epoch 529/3000, Training Loss: 0.5071517420989835, Validation Loss: 0.5218469487617293\n",
      "Epoch 530/3000, Training Loss: 0.50176158477582, Validation Loss: 0.5168315321213648\n",
      "Epoch 531/3000, Training Loss: 0.4962258097732353, Validation Loss: 0.5117744668359889\n",
      "Epoch 532/3000, Training Loss: 0.49054632709786167, Validation Loss: 0.5065636546158334\n",
      "Epoch 533/3000, Training Loss: 0.4847229224940519, Validation Loss: 0.5011542492832939\n",
      "Epoch 534/3000, Training Loss: 0.4787514094831957, Validation Loss: 0.49557494400373264\n",
      "Epoch 535/3000, Training Loss: 0.4726324651064553, Validation Loss: 0.49010075448099155\n",
      "Epoch 536/3000, Training Loss: 0.46636780560609653, Validation Loss: 0.48427633360368144\n",
      "Epoch 537/3000, Training Loss: 0.4599625055333073, Validation Loss: 0.47851154682222485\n",
      "Epoch 538/3000, Training Loss: 0.4534183821753799, Validation Loss: 0.4724830437157538\n",
      "Epoch 539/3000, Training Loss: 0.4467354990608458, Validation Loss: 0.4663067813314372\n",
      "Epoch 540/3000, Training Loss: 0.43991765553603734, Validation Loss: 0.459964818079379\n",
      "Epoch 541/3000, Training Loss: 0.43297695289852095, Validation Loss: 0.4538835030462952\n",
      "Epoch 542/3000, Training Loss: 0.42591886523674033, Validation Loss: 0.44757204160886893\n",
      "Epoch 543/3000, Training Loss: 0.41874124905892895, Validation Loss: 0.44100342012996463\n",
      "Epoch 544/3000, Training Loss: 0.41145380236615897, Validation Loss: 0.4345052143902287\n",
      "Epoch 545/3000, Training Loss: 0.4040651037930667, Validation Loss: 0.4281347548584175\n",
      "Epoch 546/3000, Training Loss: 0.3965829078748785, Validation Loss: 0.4213101832153366\n",
      "Epoch 547/3000, Training Loss: 0.38901535130195036, Validation Loss: 0.4143962604119219\n",
      "Epoch 548/3000, Training Loss: 0.3813709446203619, Validation Loss: 0.40743260067680864\n",
      "Epoch 549/3000, Training Loss: 0.3736597155131151, Validation Loss: 0.4003137226818935\n",
      "Epoch 550/3000, Training Loss: 0.36589477889349864, Validation Loss: 0.3933466511439415\n",
      "Epoch 551/3000, Training Loss: 0.3580865735470046, Validation Loss: 0.38612446428478947\n",
      "Epoch 552/3000, Training Loss: 0.3502429087794914, Validation Loss: 0.37903054342240916\n",
      "Epoch 553/3000, Training Loss: 0.34238007623486444, Validation Loss: 0.3719369332956107\n",
      "Epoch 554/3000, Training Loss: 0.3345107527890685, Validation Loss: 0.3647071833910004\n",
      "Epoch 555/3000, Training Loss: 0.3266435829510183, Validation Loss: 0.35745785111010114\n",
      "Epoch 556/3000, Training Loss: 0.31879622274671393, Validation Loss: 0.34995963691011833\n",
      "Epoch 557/3000, Training Loss: 0.310985430025867, Validation Loss: 0.34324468014890563\n",
      "Epoch 558/3000, Training Loss: 0.3032203200585022, Validation Loss: 0.3361517073540636\n",
      "Epoch 559/3000, Training Loss: 0.295524184938984, Validation Loss: 0.3295880551181341\n",
      "Epoch 560/3000, Training Loss: 0.28789631768337515, Validation Loss: 0.3227493071852271\n",
      "Epoch 561/3000, Training Loss: 0.28034480366349973, Validation Loss: 0.31626585716401795\n",
      "Epoch 562/3000, Training Loss: 0.27287730424932205, Validation Loss: 0.3095294022698461\n",
      "Epoch 563/3000, Training Loss: 0.2655078815248215, Validation Loss: 0.3031948390496808\n",
      "Epoch 564/3000, Training Loss: 0.2582466877911732, Validation Loss: 0.2970489184857297\n",
      "Epoch 565/3000, Training Loss: 0.2511057530823783, Validation Loss: 0.2908006253726228\n",
      "Epoch 566/3000, Training Loss: 0.24409356332627133, Validation Loss: 0.28508353093419747\n",
      "Epoch 567/3000, Training Loss: 0.23721231167670342, Validation Loss: 0.27906837896440734\n",
      "Epoch 568/3000, Training Loss: 0.23046826544191856, Validation Loss: 0.27351436809845076\n",
      "Epoch 569/3000, Training Loss: 0.22386673543169142, Validation Loss: 0.2679617409280922\n",
      "Epoch 570/3000, Training Loss: 0.21741533594682078, Validation Loss: 0.2622961234598183\n",
      "Epoch 571/3000, Training Loss: 0.21111198239233667, Validation Loss: 0.25703450942393974\n",
      "Epoch 572/3000, Training Loss: 0.20495951848654978, Validation Loss: 0.25179284878574354\n",
      "Epoch 573/3000, Training Loss: 0.19896321547042858, Validation Loss: 0.24675560569641186\n",
      "Epoch 574/3000, Training Loss: 0.19312924067658865, Validation Loss: 0.24193776677331558\n",
      "Epoch 575/3000, Training Loss: 0.18745582965493107, Validation Loss: 0.23705727615077582\n",
      "Epoch 576/3000, Training Loss: 0.18193901094439793, Validation Loss: 0.23245520395948863\n",
      "Epoch 577/3000, Training Loss: 0.1765820925328465, Validation Loss: 0.22799294392627087\n",
      "Epoch 578/3000, Training Loss: 0.17138241630909623, Validation Loss: 0.22354958403359498\n",
      "Epoch 579/3000, Training Loss: 0.1663343778484865, Validation Loss: 0.21929694740955372\n",
      "Epoch 580/3000, Training Loss: 0.16143978150637237, Validation Loss: 0.21510888003866563\n",
      "Epoch 581/3000, Training Loss: 0.15669575557005067, Validation Loss: 0.21114211961513202\n",
      "Epoch 582/3000, Training Loss: 0.15210041429446505, Validation Loss: 0.20725855204007718\n",
      "Epoch 583/3000, Training Loss: 0.1476511356540773, Validation Loss: 0.2035047836314417\n",
      "Epoch 584/3000, Training Loss: 0.14334490162040875, Validation Loss: 0.1999390843712061\n",
      "Epoch 585/3000, Training Loss: 0.1391803554278127, Validation Loss: 0.1965161515363905\n",
      "Epoch 586/3000, Training Loss: 0.13515470176267985, Validation Loss: 0.19320732641561075\n",
      "Epoch 587/3000, Training Loss: 0.13126352235334565, Validation Loss: 0.18999761567140122\n",
      "Epoch 588/3000, Training Loss: 0.1275029380234824, Validation Loss: 0.1869669200700637\n",
      "Epoch 589/3000, Training Loss: 0.12386972225185985, Validation Loss: 0.18402698079214408\n",
      "Epoch 590/3000, Training Loss: 0.120360340193275, Validation Loss: 0.18114743573130726\n",
      "Epoch 591/3000, Training Loss: 0.11697094523620213, Validation Loss: 0.17843511919674684\n",
      "Epoch 592/3000, Training Loss: 0.11370010585379012, Validation Loss: 0.17585233120941643\n",
      "Epoch 593/3000, Training Loss: 0.11054206396454873, Validation Loss: 0.17341355092698277\n",
      "Epoch 594/3000, Training Loss: 0.10749353634292075, Validation Loss: 0.17100776326572367\n",
      "Epoch 595/3000, Training Loss: 0.10455051785299554, Validation Loss: 0.1687859790091547\n",
      "Epoch 596/3000, Training Loss: 0.10170905719207249, Validation Loss: 0.1665473882600674\n",
      "Epoch 597/3000, Training Loss: 0.09896501094245982, Validation Loss: 0.16441196994055382\n",
      "Epoch 598/3000, Training Loss: 0.0963150572695573, Validation Loss: 0.16236795959736372\n",
      "Epoch 599/3000, Training Loss: 0.093755940240688, Validation Loss: 0.16033439964493082\n",
      "Epoch 600/3000, Training Loss: 0.0912836612543522, Validation Loss: 0.15841237807755928\n",
      "Epoch 601/3000, Training Loss: 0.08889533479845634, Validation Loss: 0.15651872602815983\n",
      "Epoch 602/3000, Training Loss: 0.08658808131426186, Validation Loss: 0.15474032514422148\n",
      "Epoch 603/3000, Training Loss: 0.08435902447989187, Validation Loss: 0.152969615855205\n",
      "Epoch 604/3000, Training Loss: 0.08220473701886612, Validation Loss: 0.1512299178247743\n",
      "Epoch 605/3000, Training Loss: 0.08012262987943387, Validation Loss: 0.14962425085027992\n",
      "Epoch 606/3000, Training Loss: 0.07811053706512389, Validation Loss: 0.14807971527755476\n",
      "Epoch 607/3000, Training Loss: 0.07616653581467063, Validation Loss: 0.14658235884367413\n",
      "Epoch 608/3000, Training Loss: 0.07428640013227851, Validation Loss: 0.1451526847128164\n",
      "Epoch 609/3000, Training Loss: 0.07246743816422342, Validation Loss: 0.1437426489663915\n",
      "Epoch 610/3000, Training Loss: 0.07070752889807569, Validation Loss: 0.14241833527753026\n",
      "Epoch 611/3000, Training Loss: 0.06900574028912212, Validation Loss: 0.14110773731341547\n",
      "Epoch 612/3000, Training Loss: 0.06735923898338665, Validation Loss: 0.139847649292118\n",
      "Epoch 613/3000, Training Loss: 0.06576582975484946, Validation Loss: 0.13862837394593888\n",
      "Epoch 614/3000, Training Loss: 0.06422388908713857, Validation Loss: 0.13745290900397716\n",
      "Epoch 615/3000, Training Loss: 0.06273090527850073, Validation Loss: 0.1363236288687502\n",
      "Epoch 616/3000, Training Loss: 0.06128517980390892, Validation Loss: 0.13523178446804393\n",
      "Epoch 617/3000, Training Loss: 0.0598851536811131, Validation Loss: 0.1341742140508861\n",
      "Epoch 618/3000, Training Loss: 0.058529109408995236, Validation Loss: 0.13315163365250746\n",
      "Epoch 619/3000, Training Loss: 0.0572151750252848, Validation Loss: 0.13216200431849842\n",
      "Epoch 620/3000, Training Loss: 0.05594183247001447, Validation Loss: 0.13120143174154542\n",
      "Epoch 621/3000, Training Loss: 0.054707569617874145, Validation Loss: 0.13030437017837027\n",
      "Epoch 622/3000, Training Loss: 0.05351105840023333, Validation Loss: 0.12942815675634345\n",
      "Epoch 623/3000, Training Loss: 0.052350837974987366, Validation Loss: 0.12858364080934037\n",
      "Epoch 624/3000, Training Loss: 0.05122583554002201, Validation Loss: 0.12780247120105662\n",
      "Epoch 625/3000, Training Loss: 0.05013499649022346, Validation Loss: 0.12704706834342847\n",
      "Epoch 626/3000, Training Loss: 0.04907678462842883, Validation Loss: 0.126304333747349\n",
      "Epoch 627/3000, Training Loss: 0.04804999271694801, Validation Loss: 0.12558632455868038\n",
      "Epoch 628/3000, Training Loss: 0.047053270103112624, Validation Loss: 0.12489931574795313\n",
      "Epoch 629/3000, Training Loss: 0.04608618027739146, Validation Loss: 0.12423944783727661\n",
      "Epoch 630/3000, Training Loss: 0.045147739850655406, Validation Loss: 0.12360197687725481\n",
      "Epoch 631/3000, Training Loss: 0.04423651700165175, Validation Loss: 0.12298459862452187\n",
      "Epoch 632/3000, Training Loss: 0.04335154118056537, Validation Loss: 0.12238537588840658\n",
      "Epoch 633/3000, Training Loss: 0.04249177514490002, Validation Loss: 0.12180991520032426\n",
      "Epoch 634/3000, Training Loss: 0.041656542343133665, Validation Loss: 0.12125725810945506\n",
      "Epoch 635/3000, Training Loss: 0.040844885119468814, Validation Loss: 0.12070999791040306\n",
      "Epoch 636/3000, Training Loss: 0.04005599002043347, Validation Loss: 0.12018868222358363\n",
      "Epoch 637/3000, Training Loss: 0.03928912343551966, Validation Loss: 0.11969805739617342\n",
      "Epoch 638/3000, Training Loss: 0.03854441520510162, Validation Loss: 0.11923756807308832\n",
      "Epoch 639/3000, Training Loss: 0.03782015670124878, Validation Loss: 0.11878759248126924\n",
      "Epoch 640/3000, Training Loss: 0.037115529670096487, Validation Loss: 0.11834987124537287\n",
      "Epoch 641/3000, Training Loss: 0.03642990566829459, Validation Loss: 0.11792896162237343\n",
      "Epoch 642/3000, Training Loss: 0.035762783834324394, Validation Loss: 0.11752306309275057\n",
      "Epoch 643/3000, Training Loss: 0.0351143113046078, Validation Loss: 0.11713140444347822\n",
      "Epoch 644/3000, Training Loss: 0.03448302326546825, Validation Loss: 0.11674387236034478\n",
      "Epoch 645/3000, Training Loss: 0.033868410946626876, Validation Loss: 0.11636745364745192\n",
      "Epoch 646/3000, Training Loss: 0.03326984027939858, Validation Loss: 0.11599871494460379\n",
      "Epoch 647/3000, Training Loss: 0.03268695185932126, Validation Loss: 0.11564759352296697\n",
      "Epoch 648/3000, Training Loss: 0.032119120418236474, Validation Loss: 0.11531073970429256\n",
      "Epoch 649/3000, Training Loss: 0.03156581400881726, Validation Loss: 0.11498577144217532\n",
      "Epoch 650/3000, Training Loss: 0.031026565939011794, Validation Loss: 0.11467603442498443\n",
      "Epoch 651/3000, Training Loss: 0.03050099763335765, Validation Loss: 0.11437034248346029\n",
      "Epoch 652/3000, Training Loss: 0.029988641824664278, Validation Loss: 0.11407078733354896\n",
      "Epoch 653/3000, Training Loss: 0.02948908434432355, Validation Loss: 0.11377173650071008\n",
      "Epoch 654/3000, Training Loss: 0.029001951454374356, Validation Loss: 0.11349872170952102\n",
      "Epoch 655/3000, Training Loss: 0.028526885981866694, Validation Loss: 0.1132298139422923\n",
      "Epoch 656/3000, Training Loss: 0.028063456992914692, Validation Loss: 0.11297412325005925\n",
      "Epoch 657/3000, Training Loss: 0.027611315576628402, Validation Loss: 0.11273141150995518\n",
      "Epoch 658/3000, Training Loss: 0.027170188432003754, Validation Loss: 0.11249976336105749\n",
      "Epoch 659/3000, Training Loss: 0.026739659941256093, Validation Loss: 0.1122777523644323\n",
      "Epoch 660/3000, Training Loss: 0.026319476836062776, Validation Loss: 0.11206328699670472\n",
      "Epoch 661/3000, Training Loss: 0.02590944151562573, Validation Loss: 0.11185389148217971\n",
      "Epoch 662/3000, Training Loss: 0.02550907270484792, Validation Loss: 0.11165222957131994\n",
      "Epoch 663/3000, Training Loss: 0.025117996763949716, Validation Loss: 0.11145829149518788\n",
      "Epoch 664/3000, Training Loss: 0.024735928730274727, Validation Loss: 0.11127092097523286\n",
      "Epoch 665/3000, Training Loss: 0.02436223092167069, Validation Loss: 0.11109170501214177\n",
      "Epoch 666/3000, Training Loss: 0.023997019353198947, Validation Loss: 0.11091585610882644\n",
      "Epoch 667/3000, Training Loss: 0.023640162952674727, Validation Loss: 0.11074333379618409\n",
      "Epoch 668/3000, Training Loss: 0.023291453955147163, Validation Loss: 0.11057862067759273\n",
      "Epoch 669/3000, Training Loss: 0.02295063795025207, Validation Loss: 0.11042146419003032\n",
      "Epoch 670/3000, Training Loss: 0.022617448552150567, Validation Loss: 0.11027181198317813\n",
      "Epoch 671/3000, Training Loss: 0.022291669087989872, Validation Loss: 0.11012659852942516\n",
      "Epoch 672/3000, Training Loss: 0.021973039561255688, Validation Loss: 0.10998798634534468\n",
      "Epoch 673/3000, Training Loss: 0.021661388555786625, Validation Loss: 0.10985551685274768\n",
      "Epoch 674/3000, Training Loss: 0.021356519454469824, Validation Loss: 0.10972172262642327\n",
      "Epoch 675/3000, Training Loss: 0.021058212339917216, Validation Loss: 0.10959283928761461\n",
      "Epoch 676/3000, Training Loss: 0.020766369358326014, Validation Loss: 0.10947412592215852\n",
      "Epoch 677/3000, Training Loss: 0.02048080778262381, Validation Loss: 0.10936254701542703\n",
      "Epoch 678/3000, Training Loss: 0.020201333496199382, Validation Loss: 0.10925233146460707\n",
      "Epoch 679/3000, Training Loss: 0.0199276744453062, Validation Loss: 0.10914744941210586\n",
      "Epoch 680/3000, Training Loss: 0.01965977395996174, Validation Loss: 0.10904833595841475\n",
      "Epoch 681/3000, Training Loss: 0.01939750245770287, Validation Loss: 0.10895385392027877\n",
      "Epoch 682/3000, Training Loss: 0.019140732609720906, Validation Loss: 0.10886783109157461\n",
      "Epoch 683/3000, Training Loss: 0.018889273447117067, Validation Loss: 0.10878451946707358\n",
      "Epoch 684/3000, Training Loss: 0.018642932237047473, Validation Loss: 0.10870495933157857\n",
      "Epoch 685/3000, Training Loss: 0.018401614188203644, Validation Loss: 0.10863148816511048\n",
      "Epoch 686/3000, Training Loss: 0.01816514065897177, Validation Loss: 0.10855591708954579\n",
      "Epoch 687/3000, Training Loss: 0.017933400841437016, Validation Loss: 0.10848151619177784\n",
      "Epoch 688/3000, Training Loss: 0.017706271941018908, Validation Loss: 0.10840564506974405\n",
      "Epoch 689/3000, Training Loss: 0.017483646502263906, Validation Loss: 0.10833232882514995\n",
      "Epoch 690/3000, Training Loss: 0.017265442511428718, Validation Loss: 0.10826403984016435\n",
      "Epoch 691/3000, Training Loss: 0.017051529994371154, Validation Loss: 0.10820090345234383\n",
      "Epoch 692/3000, Training Loss: 0.016841793947368556, Validation Loss: 0.10814193783569298\n",
      "Epoch 693/3000, Training Loss: 0.016636099161885864, Validation Loss: 0.10808531169086827\n",
      "Epoch 694/3000, Training Loss: 0.016434375962019448, Validation Loss: 0.10802911673603019\n",
      "Epoch 695/3000, Training Loss: 0.016236527260065584, Validation Loss: 0.10797847240115481\n",
      "Epoch 696/3000, Training Loss: 0.016042455736169612, Validation Loss: 0.1079284792483509\n",
      "Epoch 697/3000, Training Loss: 0.01585206405175056, Validation Loss: 0.10788271144072942\n",
      "Epoch 698/3000, Training Loss: 0.015665261331772727, Validation Loss: 0.10784059350088102\n",
      "Epoch 699/3000, Training Loss: 0.015481959408308052, Validation Loss: 0.10780188132854834\n",
      "Epoch 700/3000, Training Loss: 0.015302072843048372, Validation Loss: 0.10776326010186495\n",
      "Epoch 701/3000, Training Loss: 0.01512551674874612, Validation Loss: 0.10772815718129931\n",
      "Epoch 702/3000, Training Loss: 0.014952207560116415, Validation Loss: 0.1076930181874752\n",
      "Epoch 703/3000, Training Loss: 0.014782074103179537, Validation Loss: 0.1076623552931325\n",
      "Epoch 704/3000, Training Loss: 0.014615033882069979, Validation Loss: 0.10763308529367199\n",
      "Epoch 705/3000, Training Loss: 0.014451010340601912, Validation Loss: 0.10760687825697003\n",
      "Epoch 706/3000, Training Loss: 0.014289921723346189, Validation Loss: 0.10758349084234492\n",
      "Epoch 707/3000, Training Loss: 0.014131707066465448, Validation Loss: 0.1075627629308271\n",
      "Epoch 708/3000, Training Loss: 0.013976296705183154, Validation Loss: 0.10754454671993223\n",
      "Epoch 709/3000, Training Loss: 0.013823633545035711, Validation Loss: 0.10752880963656897\n",
      "Epoch 710/3000, Training Loss: 0.013673654122090761, Validation Loss: 0.10751633239534401\n",
      "Epoch 711/3000, Training Loss: 0.013526258942970595, Validation Loss: 0.10750341828911406\n",
      "Epoch 712/3000, Training Loss: 0.013381337627296193, Validation Loss: 0.10749145940086223\n",
      "Epoch 713/3000, Training Loss: 0.013238890863552212, Validation Loss: 0.10748174757882134\n",
      "Epoch 714/3000, Training Loss: 0.013098882936605282, Validation Loss: 0.10747420067049324\n",
      "Epoch 715/3000, Training Loss: 0.012961250434271206, Validation Loss: 0.1074689552753779\n",
      "Epoch 716/3000, Training Loss: 0.012825930495175187, Validation Loss: 0.10746559926105241\n",
      "Epoch 717/3000, Training Loss: 0.012692867936384768, Validation Loss: 0.10746125868107256\n",
      "Epoch 718/3000, Training Loss: 0.012562033591098608, Validation Loss: 0.10745892269350069\n",
      "Epoch 719/3000, Training Loss: 0.012433367562535555, Validation Loss: 0.10745848892298027\n",
      "Epoch 720/3000, Training Loss: 0.012306812793159178, Validation Loss: 0.10745749743400859\n",
      "Epoch 721/3000, Training Loss: 0.012182330624287805, Validation Loss: 0.10745836694270104\n",
      "Epoch 722/3000, Training Loss: 0.012059869147024487, Validation Loss: 0.10746092212203061\n",
      "Epoch 723/3000, Training Loss: 0.011939396199931441, Validation Loss: 0.10746518355536609\n",
      "Epoch 724/3000, Training Loss: 0.01182087382488402, Validation Loss: 0.10747113655148975\n",
      "Epoch 725/3000, Training Loss: 0.011704246926696592, Validation Loss: 0.10747931009014428\n",
      "Epoch 726/3000, Training Loss: 0.011589519763148834, Validation Loss: 0.10748680960599315\n",
      "Epoch 727/3000, Training Loss: 0.011476564123137196, Validation Loss: 0.10749652150961228\n",
      "Epoch 728/3000, Training Loss: 0.011365426662548743, Validation Loss: 0.10750684826947902\n",
      "Epoch 729/3000, Training Loss: 0.011255991809065799, Validation Loss: 0.1075191648263823\n",
      "Epoch 730/3000, Training Loss: 0.011148287897829252, Validation Loss: 0.10753208207528799\n",
      "Epoch 731/3000, Training Loss: 0.011042239821214215, Validation Loss: 0.10754692431149776\n",
      "Epoch 732/3000, Training Loss: 0.010937846067032674, Validation Loss: 0.10756247125866915\n",
      "Epoch 733/3000, Training Loss: 0.010835034421933653, Validation Loss: 0.10757976621206151\n",
      "Epoch 734/3000, Training Loss: 0.010733817325604772, Validation Loss: 0.10759754260545219\n",
      "Epoch 735/3000, Training Loss: 0.010634149085253397, Validation Loss: 0.1076149175977097\n",
      "Epoch 736/3000, Training Loss: 0.010536000544215439, Validation Loss: 0.10763142887119699\n",
      "Epoch 737/3000, Training Loss: 0.01043934512575255, Validation Loss: 0.10764965190172651\n",
      "Epoch 738/3000, Training Loss: 0.010344138636198206, Validation Loss: 0.10766896246729851\n",
      "Epoch 739/3000, Training Loss: 0.010250373158371729, Validation Loss: 0.10769004374246897\n",
      "Epoch 740/3000, Training Loss: 0.010157995067564965, Validation Loss: 0.10771091219546315\n",
      "Epoch 741/3000, Training Loss: 0.010067021164601118, Validation Loss: 0.10773162088180988\n",
      "Epoch 742/3000, Training Loss: 0.00997737342710528, Validation Loss: 0.10775123933699224\n",
      "Epoch 743/3000, Training Loss: 0.009889068645803063, Validation Loss: 0.10777264318267298\n",
      "Epoch 744/3000, Training Loss: 0.009802028297870798, Validation Loss: 0.1077945046200154\n",
      "Epoch 745/3000, Training Loss: 0.00971628705529769, Validation Loss: 0.107816090159219\n",
      "Epoch 746/3000, Training Loss: 0.009631755896322076, Validation Loss: 0.10783819808378313\n",
      "Epoch 747/3000, Training Loss: 0.009548471064957867, Validation Loss: 0.10786209449559526\n",
      "Epoch 748/3000, Training Loss: 0.009466344073756516, Validation Loss: 0.10788630063229317\n",
      "Epoch 749/3000, Training Loss: 0.009385417614661813, Validation Loss: 0.10791221370403738\n",
      "Epoch 750/3000, Training Loss: 0.009305606877899122, Validation Loss: 0.10793836730174752\n",
      "Epoch 751/3000, Training Loss: 0.009226951689304467, Validation Loss: 0.10796666378280481\n",
      "Epoch 752/3000, Training Loss: 0.00914936050699863, Validation Loss: 0.1079950889044131\n",
      "Epoch 753/3000, Training Loss: 0.009072884428188268, Validation Loss: 0.10802524377262172\n",
      "Epoch 754/3000, Training Loss: 0.008997456916233023, Validation Loss: 0.10805563143601259\n",
      "Epoch 755/3000, Training Loss: 0.008923102606364714, Validation Loss: 0.10808731297321789\n",
      "Epoch 756/3000, Training Loss: 0.008849738177098623, Validation Loss: 0.10811795075441913\n",
      "Epoch 757/3000, Training Loss: 0.008777407641591668, Validation Loss: 0.10814685890897095\n",
      "Epoch 758/3000, Training Loss: 0.008706033929351504, Validation Loss: 0.1081760230742783\n",
      "Epoch 759/3000, Training Loss: 0.008635652135891948, Validation Loss: 0.1082066039446057\n",
      "Epoch 760/3000, Training Loss: 0.008566190210284442, Validation Loss: 0.10823739691077533\n",
      "Epoch 761/3000, Training Loss: 0.008497684887199349, Validation Loss: 0.10826790010559634\n",
      "Epoch 762/3000, Training Loss: 0.00843007051218484, Validation Loss: 0.10829871664166314\n",
      "Epoch 763/3000, Training Loss: 0.008363373363461478, Validation Loss: 0.10833088220700485\n",
      "Epoch 764/3000, Training Loss: 0.00829753675482248, Validation Loss: 0.10836223058184506\n",
      "Epoch 765/3000, Training Loss: 0.00823257972789352, Validation Loss: 0.10839479895539475\n",
      "Epoch 766/3000, Training Loss: 0.008168454437832812, Validation Loss: 0.10842569861619979\n",
      "Epoch 767/3000, Training Loss: 0.008105173888733785, Validation Loss: 0.10845799484826703\n",
      "Epoch 768/3000, Training Loss: 0.008042702997214777, Validation Loss: 0.10849054617372321\n",
      "Epoch 769/3000, Training Loss: 0.007981041354720388, Validation Loss: 0.10852530509063478\n",
      "Epoch 770/3000, Training Loss: 0.007920169873252635, Validation Loss: 0.10856016931653727\n",
      "Epoch 771/3000, Training Loss: 0.007860074062352855, Validation Loss: 0.10859624808153437\n",
      "Epoch 772/3000, Training Loss: 0.007800744182899514, Validation Loss: 0.1086326410230111\n",
      "Epoch 773/3000, Training Loss: 0.007742159408672291, Validation Loss: 0.10867016130155865\n",
      "Epoch 774/3000, Training Loss: 0.007684320051503333, Validation Loss: 0.10870774005187811\n",
      "Epoch 775/3000, Training Loss: 0.00762719163615975, Validation Loss: 0.10874637833124236\n",
      "Epoch 776/3000, Training Loss: 0.00757079108946022, Validation Loss: 0.10878491954229517\n",
      "Epoch 777/3000, Training Loss: 0.007515069541927307, Validation Loss: 0.10882297642647673\n",
      "Epoch 778/3000, Training Loss: 0.007460060827993243, Validation Loss: 0.10886033038232651\n",
      "Epoch 779/3000, Training Loss: 0.0074057018070611946, Validation Loss: 0.10889754199741385\n",
      "Epoch 780/3000, Training Loss: 0.007352036922820707, Validation Loss: 0.10893609676655268\n",
      "Epoch 781/3000, Training Loss: 0.007298998451001022, Validation Loss: 0.10897323797613734\n",
      "Epoch 782/3000, Training Loss: 0.00724662633496514, Validation Loss: 0.10901144397967051\n",
      "Epoch 783/3000, Training Loss: 0.007194864484465876, Validation Loss: 0.1090496782017707\n",
      "Epoch 784/3000, Training Loss: 0.007143737086875545, Validation Loss: 0.10908894452177517\n",
      "Epoch 785/3000, Training Loss: 0.0070932100347607515, Validation Loss: 0.10912808529391309\n",
      "Epoch 786/3000, Training Loss: 0.007043291243902498, Validation Loss: 0.10916946664511118\n",
      "Epoch 787/3000, Training Loss: 0.006993966358529981, Validation Loss: 0.10921082844671647\n",
      "Epoch 788/3000, Training Loss: 0.00694522002441956, Validation Loss: 0.1092530778780148\n",
      "Epoch 789/3000, Training Loss: 0.00689705346525286, Validation Loss: 0.10929511925904646\n",
      "Epoch 790/3000, Training Loss: 0.006849437990967303, Validation Loss: 0.10933870465707224\n",
      "Epoch 791/3000, Training Loss: 0.006802401858728446, Validation Loss: 0.10938344375277945\n",
      "Epoch 792/3000, Training Loss: 0.006755894375753217, Validation Loss: 0.10942765005961581\n",
      "Epoch 793/3000, Training Loss: 0.006709942460887593, Validation Loss: 0.1094724965683376\n",
      "Epoch 794/3000, Training Loss: 0.006664496159575298, Validation Loss: 0.10951709028788933\n",
      "Epoch 795/3000, Training Loss: 0.006619585420093499, Validation Loss: 0.10956153546814291\n",
      "Epoch 796/3000, Training Loss: 0.006575178965247745, Validation Loss: 0.10960629881254451\n",
      "Epoch 797/3000, Training Loss: 0.006531287210842856, Validation Loss: 0.10965169348584233\n",
      "Epoch 798/3000, Training Loss: 0.006487888671680686, Validation Loss: 0.1096967632659153\n",
      "Epoch 799/3000, Training Loss: 0.006444973488115759, Validation Loss: 0.10974245428120007\n",
      "Epoch 800/3000, Training Loss: 0.0064025462113822505, Validation Loss: 0.10978781385687288\n",
      "Epoch 801/3000, Training Loss: 0.006360577451423535, Validation Loss: 0.10983377283402516\n",
      "Epoch 802/3000, Training Loss: 0.006319091830449567, Validation Loss: 0.10987946392737924\n",
      "Epoch 803/3000, Training Loss: 0.0062780448755849014, Validation Loss: 0.1099260358175714\n",
      "Epoch 804/3000, Training Loss: 0.006237465639290944, Validation Loss: 0.1099731788076132\n",
      "Epoch 805/3000, Training Loss: 0.006197318351454754, Validation Loss: 0.11001997380907517\n",
      "Epoch 806/3000, Training Loss: 0.006157615286894364, Validation Loss: 0.11006718080712997\n",
      "Epoch 807/3000, Training Loss: 0.006118344008435139, Validation Loss: 0.11011352948765589\n",
      "Epoch 808/3000, Training Loss: 0.006079491723939717, Validation Loss: 0.11016045297228819\n",
      "Epoch 809/3000, Training Loss: 0.006041066234343224, Validation Loss: 0.11020705072543788\n",
      "Epoch 810/3000, Training Loss: 0.006003038849198259, Validation Loss: 0.11025375495544511\n",
      "Epoch 811/3000, Training Loss: 0.0059654351640301825, Validation Loss: 0.11030004101243494\n",
      "Epoch 812/3000, Training Loss: 0.005928215761693788, Validation Loss: 0.11034608207217608\n",
      "Epoch 813/3000, Training Loss: 0.005891402339836079, Validation Loss: 0.11039272540421506\n",
      "Epoch 814/3000, Training Loss: 0.0058549708868029324, Validation Loss: 0.11043925567058294\n",
      "Epoch 815/3000, Training Loss: 0.005818923247164233, Validation Loss: 0.11048631366279194\n",
      "Epoch 816/3000, Training Loss: 0.005783256221427947, Validation Loss: 0.1105331136103933\n",
      "Epoch 817/3000, Training Loss: 0.005747949771184397, Validation Loss: 0.11058042236871231\n",
      "Epoch 818/3000, Training Loss: 0.005713023681613856, Validation Loss: 0.11062608525248278\n",
      "Epoch 819/3000, Training Loss: 0.0056784442196722825, Validation Loss: 0.11067253573799067\n",
      "Epoch 820/3000, Training Loss: 0.005644231429299233, Validation Loss: 0.11072084470673199\n",
      "Epoch 821/3000, Training Loss: 0.005610368243648184, Validation Loss: 0.11076876815199804\n",
      "Epoch 822/3000, Training Loss: 0.00557685198904137, Validation Loss: 0.11081784513929019\n",
      "Epoch 823/3000, Training Loss: 0.005543681644908847, Validation Loss: 0.11086650830762167\n",
      "Epoch 824/3000, Training Loss: 0.005510838519496047, Validation Loss: 0.11091557198445144\n",
      "Epoch 825/3000, Training Loss: 0.005478340344177264, Validation Loss: 0.11096315128690289\n",
      "Epoch 826/3000, Training Loss: 0.0054461550909891375, Validation Loss: 0.11101084860025448\n",
      "Epoch 827/3000, Training Loss: 0.005414302386352862, Validation Loss: 0.11105909030405724\n",
      "Epoch 828/3000, Training Loss: 0.005382764973441729, Validation Loss: 0.1111069790323312\n",
      "Epoch 829/3000, Training Loss: 0.005351540280794992, Validation Loss: 0.1111553000925465\n",
      "Epoch 830/3000, Training Loss: 0.0053206304715469, Validation Loss: 0.11120384906835898\n",
      "Epoch 831/3000, Training Loss: 0.005290016024222788, Validation Loss: 0.111251926991539\n",
      "Epoch 832/3000, Training Loss: 0.00525971302654617, Validation Loss: 0.11130055692562395\n",
      "Epoch 833/3000, Training Loss: 0.005229695701508352, Validation Loss: 0.11134887059384946\n",
      "Epoch 834/3000, Training Loss: 0.005199976658326976, Validation Loss: 0.11139756225219984\n",
      "Epoch 835/3000, Training Loss: 0.005170546218670081, Validation Loss: 0.1114463395787058\n",
      "Epoch 836/3000, Training Loss: 0.0051413968075857425, Validation Loss: 0.11149430249889\n",
      "Epoch 837/3000, Training Loss: 0.005112538872921441, Validation Loss: 0.11154244271175692\n",
      "Epoch 838/3000, Training Loss: 0.005083949762535377, Validation Loss: 0.11159069641341529\n",
      "Epoch 839/3000, Training Loss: 0.005055647936867982, Validation Loss: 0.11163935275872967\n",
      "Epoch 840/3000, Training Loss: 0.005027610483702267, Validation Loss: 0.11168781947677778\n",
      "Epoch 841/3000, Training Loss: 0.004999840897329217, Validation Loss: 0.11173721501792684\n",
      "Epoch 842/3000, Training Loss: 0.004972337039563072, Validation Loss: 0.11178565410845705\n",
      "Epoch 843/3000, Training Loss: 0.004945083490522897, Validation Loss: 0.11183414661013097\n",
      "Epoch 844/3000, Training Loss: 0.004918095045935485, Validation Loss: 0.11188303281100655\n",
      "Epoch 845/3000, Training Loss: 0.00489135113080402, Validation Loss: 0.11193157409132752\n",
      "Epoch 846/3000, Training Loss: 0.004864858231027918, Validation Loss: 0.11198053334596854\n",
      "Epoch 847/3000, Training Loss: 0.00483861371456941, Validation Loss: 0.11202912798349272\n",
      "Epoch 848/3000, Training Loss: 0.004812604798985157, Validation Loss: 0.11207785817187399\n",
      "Epoch 849/3000, Training Loss: 0.0047868451902409416, Validation Loss: 0.11212691655546568\n",
      "Epoch 850/3000, Training Loss: 0.004761313221385632, Validation Loss: 0.11217618314300322\n",
      "Epoch 851/3000, Training Loss: 0.0047360196051320115, Validation Loss: 0.11222473725625771\n",
      "Epoch 852/3000, Training Loss: 0.004710954731227234, Validation Loss: 0.11227294024694252\n",
      "Epoch 853/3000, Training Loss: 0.004686108977841622, Validation Loss: 0.11232116702757601\n",
      "Epoch 854/3000, Training Loss: 0.0046614975772367746, Validation Loss: 0.11236919294808873\n",
      "Epoch 855/3000, Training Loss: 0.0046370991994116526, Validation Loss: 0.11241692949194049\n",
      "Epoch 856/3000, Training Loss: 0.004612923566628624, Validation Loss: 0.11246505728048455\n",
      "Epoch 857/3000, Training Loss: 0.00458896479297288, Validation Loss: 0.11251286455484347\n",
      "Epoch 858/3000, Training Loss: 0.004565213506303594, Validation Loss: 0.11256072442618383\n",
      "Epoch 859/3000, Training Loss: 0.004541681848694277, Validation Loss: 0.11260892960505968\n",
      "Epoch 860/3000, Training Loss: 0.004518351871305857, Validation Loss: 0.11265747020546449\n",
      "Epoch 861/3000, Training Loss: 0.004495229782445832, Validation Loss: 0.11270579057320776\n",
      "Epoch 862/3000, Training Loss: 0.004472311991461781, Validation Loss: 0.11275385404433695\n",
      "Epoch 863/3000, Training Loss: 0.004449589488633196, Validation Loss: 0.11280194493595852\n",
      "Epoch 864/3000, Training Loss: 0.0044270729099442725, Validation Loss: 0.11285040093843982\n",
      "Epoch 865/3000, Training Loss: 0.004404746143524015, Validation Loss: 0.11289867277701757\n",
      "Epoch 866/3000, Training Loss: 0.004382614585952737, Validation Loss: 0.11294725383619575\n",
      "Epoch 867/3000, Training Loss: 0.004360677441427533, Validation Loss: 0.11299503362505763\n",
      "Epoch 868/3000, Training Loss: 0.004338922761180968, Validation Loss: 0.11304293088208965\n",
      "Epoch 869/3000, Training Loss: 0.004317362176511911, Validation Loss: 0.11309113637902914\n",
      "Epoch 870/3000, Training Loss: 0.004295981096163965, Validation Loss: 0.11313851003572757\n",
      "Epoch 871/3000, Training Loss: 0.004274780878873876, Validation Loss: 0.11318623252887264\n",
      "Epoch 872/3000, Training Loss: 0.004253766002184663, Validation Loss: 0.11323361129990674\n",
      "Epoch 873/3000, Training Loss: 0.004232922785305678, Validation Loss: 0.11328035053467969\n",
      "Epoch 874/3000, Training Loss: 0.004212260989648064, Validation Loss: 0.1133274467920191\n",
      "Epoch 875/3000, Training Loss: 0.004191771718126797, Validation Loss: 0.11337278602972391\n",
      "Epoch 876/3000, Training Loss: 0.0041714515449932325, Validation Loss: 0.11341866553825695\n",
      "Epoch 877/3000, Training Loss: 0.004151309424708851, Validation Loss: 0.11346546491945683\n",
      "Epoch 878/3000, Training Loss: 0.004131329351844266, Validation Loss: 0.11351150187004737\n",
      "Epoch 879/3000, Training Loss: 0.004111515737170594, Validation Loss: 0.11355789548588896\n",
      "Epoch 880/3000, Training Loss: 0.0040918678909464355, Validation Loss: 0.11360405729387223\n",
      "Epoch 881/3000, Training Loss: 0.004072377169018649, Validation Loss: 0.11365028734279996\n",
      "Epoch 882/3000, Training Loss: 0.004053051689758, Validation Loss: 0.11369686776276387\n",
      "Epoch 883/3000, Training Loss: 0.0040338810084985435, Validation Loss: 0.11374320475352641\n",
      "Epoch 884/3000, Training Loss: 0.004014865522333552, Validation Loss: 0.11378991931003113\n",
      "Epoch 885/3000, Training Loss: 0.003996010500790009, Validation Loss: 0.11383638118296081\n",
      "Epoch 886/3000, Training Loss: 0.003977303567176145, Validation Loss: 0.11388339605920568\n",
      "Epoch 887/3000, Training Loss: 0.00395874966608739, Validation Loss: 0.11393068770767356\n",
      "Epoch 888/3000, Training Loss: 0.003940346799685734, Validation Loss: 0.1139777021370143\n",
      "Epoch 889/3000, Training Loss: 0.003922088869946625, Validation Loss: 0.11402473993312152\n",
      "Epoch 890/3000, Training Loss: 0.0039039818579715718, Validation Loss: 0.11407206487048473\n",
      "Epoch 891/3000, Training Loss: 0.0038860158408670605, Validation Loss: 0.11411910103808798\n",
      "Epoch 892/3000, Training Loss: 0.003868191394876786, Validation Loss: 0.11416641044345903\n",
      "Epoch 893/3000, Training Loss: 0.0038505135962586864, Validation Loss: 0.11421342921572325\n",
      "Epoch 894/3000, Training Loss: 0.003832970912093821, Validation Loss: 0.11426093992142337\n",
      "Epoch 895/3000, Training Loss: 0.0038155675279282508, Validation Loss: 0.11430822612366857\n",
      "Epoch 896/3000, Training Loss: 0.003798302168348788, Validation Loss: 0.1143548354999374\n",
      "Epoch 897/3000, Training Loss: 0.003781168486856052, Validation Loss: 0.11440107884951899\n",
      "Epoch 898/3000, Training Loss: 0.0037641723961519226, Validation Loss: 0.11444762118068963\n",
      "Epoch 899/3000, Training Loss: 0.0037473065876147595, Validation Loss: 0.1144939119977041\n",
      "Epoch 900/3000, Training Loss: 0.0037305694023716603, Validation Loss: 0.11454035307980977\n",
      "Epoch 901/3000, Training Loss: 0.003713966858253508, Validation Loss: 0.11458706896446882\n",
      "Epoch 902/3000, Training Loss: 0.0036974889841148366, Validation Loss: 0.11463398703828952\n",
      "Epoch 903/3000, Training Loss: 0.0036811365717729157, Validation Loss: 0.11468062742796363\n",
      "Epoch 904/3000, Training Loss: 0.0036649128335267487, Validation Loss: 0.11472711058704772\n",
      "Epoch 905/3000, Training Loss: 0.003648808664687029, Validation Loss: 0.1147736047624696\n",
      "Epoch 906/3000, Training Loss: 0.003632829252289844, Validation Loss: 0.11482036161190283\n",
      "Epoch 907/3000, Training Loss: 0.0036169717768032832, Validation Loss: 0.11486683837021466\n",
      "Epoch 908/3000, Training Loss: 0.0036012312269943355, Validation Loss: 0.11491340974145213\n",
      "Epoch 909/3000, Training Loss: 0.003585613462478432, Validation Loss: 0.11496071279186228\n",
      "Epoch 910/3000, Training Loss: 0.003570112355099215, Validation Loss: 0.1150072551904485\n",
      "Epoch 911/3000, Training Loss: 0.0035547240992405433, Validation Loss: 0.11505376388623945\n",
      "Epoch 912/3000, Training Loss: 0.0035394541478145137, Validation Loss: 0.11510054188333839\n",
      "Epoch 913/3000, Training Loss: 0.0035242943095190756, Validation Loss: 0.1151470347363971\n",
      "Epoch 914/3000, Training Loss: 0.0035092466276143357, Validation Loss: 0.11519376146396182\n",
      "Epoch 915/3000, Training Loss: 0.0034943146023678666, Validation Loss: 0.11524023088202424\n",
      "Epoch 916/3000, Training Loss: 0.003479488170671453, Validation Loss: 0.11528596569483404\n",
      "Epoch 917/3000, Training Loss: 0.0034647742960232627, Validation Loss: 0.11533240562233557\n",
      "Epoch 918/3000, Training Loss: 0.003450169408930686, Validation Loss: 0.11537811424097476\n",
      "Epoch 919/3000, Training Loss: 0.0034356673735965924, Validation Loss: 0.11542382353246003\n",
      "Epoch 920/3000, Training Loss: 0.003421273067931393, Validation Loss: 0.1154701069423037\n",
      "Epoch 921/3000, Training Loss: 0.0034069847413654795, Validation Loss: 0.11551570480856758\n",
      "Epoch 922/3000, Training Loss: 0.003392797870025974, Validation Loss: 0.11556206279616472\n",
      "Epoch 923/3000, Training Loss: 0.0033787168386253904, Validation Loss: 0.11560862001283002\n",
      "Epoch 924/3000, Training Loss: 0.003364737609784906, Validation Loss: 0.11565535008631173\n",
      "Epoch 925/3000, Training Loss: 0.003350856341837082, Validation Loss: 0.11570157931852318\n",
      "Epoch 926/3000, Training Loss: 0.0033370771141177273, Validation Loss: 0.1157472286493776\n",
      "Epoch 927/3000, Training Loss: 0.0033233958524939195, Validation Loss: 0.11579346351851458\n",
      "Epoch 928/3000, Training Loss: 0.0033098138228547986, Validation Loss: 0.11583999289128939\n",
      "Epoch 929/3000, Training Loss: 0.0032963328019873744, Validation Loss: 0.11588671400789834\n",
      "Epoch 930/3000, Training Loss: 0.003282943997328508, Validation Loss: 0.1159331295098038\n",
      "Epoch 931/3000, Training Loss: 0.00326965027142346, Validation Loss: 0.11597997445037804\n",
      "Epoch 932/3000, Training Loss: 0.003256454069533904, Validation Loss: 0.11602653114243089\n",
      "Epoch 933/3000, Training Loss: 0.0032433458465527558, Validation Loss: 0.11607286178311736\n",
      "Epoch 934/3000, Training Loss: 0.0032303297171516004, Validation Loss: 0.11611936597343142\n",
      "Epoch 935/3000, Training Loss: 0.0032174077574991223, Validation Loss: 0.11616556048527538\n",
      "Epoch 936/3000, Training Loss: 0.003204572052787844, Validation Loss: 0.11621125865159572\n",
      "Epoch 937/3000, Training Loss: 0.003191826705624219, Validation Loss: 0.1162571528815891\n",
      "Epoch 938/3000, Training Loss: 0.0031791736052332485, Validation Loss: 0.116302558315953\n",
      "Epoch 939/3000, Training Loss: 0.003166603906637884, Validation Loss: 0.11634749254757938\n",
      "Epoch 940/3000, Training Loss: 0.0031541212048110884, Validation Loss: 0.11639265469625953\n",
      "Epoch 941/3000, Training Loss: 0.0031417263282814746, Validation Loss: 0.11643716233522898\n",
      "Epoch 942/3000, Training Loss: 0.00312941334913158, Validation Loss: 0.11648166168187629\n",
      "Epoch 943/3000, Training Loss: 0.0031171855651678988, Validation Loss: 0.11652649449302012\n",
      "Epoch 944/3000, Training Loss: 0.003105042569059384, Validation Loss: 0.11657149116915248\n",
      "Epoch 945/3000, Training Loss: 0.003092980959169571, Validation Loss: 0.11661603170962413\n",
      "Epoch 946/3000, Training Loss: 0.003081000473824451, Validation Loss: 0.1166607714783002\n",
      "Epoch 947/3000, Training Loss: 0.003069101924808837, Validation Loss: 0.11670524245289023\n",
      "Epoch 948/3000, Training Loss: 0.0030572809399566753, Validation Loss: 0.11674968308501177\n",
      "Epoch 949/3000, Training Loss: 0.0030455405720540393, Validation Loss: 0.11679432930399491\n",
      "Epoch 950/3000, Training Loss: 0.0030338796170723084, Validation Loss: 0.11683871293114864\n",
      "Epoch 951/3000, Training Loss: 0.0030222947680715543, Validation Loss: 0.11688348841277224\n",
      "Epoch 952/3000, Training Loss: 0.003010788904738871, Validation Loss: 0.11692802793280105\n",
      "Epoch 953/3000, Training Loss: 0.002999359007276628, Validation Loss: 0.11697230818797522\n",
      "Epoch 954/3000, Training Loss: 0.00298800261407481, Validation Loss: 0.11701656090634956\n",
      "Epoch 955/3000, Training Loss: 0.0029767224852119615, Validation Loss: 0.1170609919557431\n",
      "Epoch 956/3000, Training Loss: 0.002965517279103491, Validation Loss: 0.11710517359028694\n",
      "Epoch 957/3000, Training Loss: 0.002954383675366243, Validation Loss: 0.11714935857242262\n",
      "Epoch 958/3000, Training Loss: 0.002943325527728726, Validation Loss: 0.11719414502986751\n",
      "Epoch 959/3000, Training Loss: 0.0029323395545080505, Validation Loss: 0.11723825475696602\n",
      "Epoch 960/3000, Training Loss: 0.0029214227897425013, Validation Loss: 0.1172823371243141\n",
      "Epoch 961/3000, Training Loss: 0.0029105780178381347, Validation Loss: 0.11732659661387353\n",
      "Epoch 962/3000, Training Loss: 0.0028998043975330683, Validation Loss: 0.1173705904582002\n",
      "Epoch 963/3000, Training Loss: 0.0028890982853920777, Validation Loss: 0.11741455612834899\n",
      "Epoch 964/3000, Training Loss: 0.0028784631921775117, Validation Loss: 0.11745952470161265\n",
      "Epoch 965/3000, Training Loss: 0.0028679025741837364, Validation Loss: 0.11750373829055508\n",
      "Epoch 966/3000, Training Loss: 0.0028574063539003576, Validation Loss: 0.11754791304681916\n",
      "Epoch 967/3000, Training Loss: 0.0028469778946748673, Validation Loss: 0.11759224850770054\n",
      "Epoch 968/3000, Training Loss: 0.0028366173855278293, Validation Loss: 0.11763630424865838\n",
      "Epoch 969/3000, Training Loss: 0.0028263203778658937, Validation Loss: 0.11768031854858992\n",
      "Epoch 970/3000, Training Loss: 0.0028160890546346314, Validation Loss: 0.11772448347935179\n",
      "Epoch 971/3000, Training Loss: 0.002805925768528485, Validation Loss: 0.11776879431664229\n",
      "Epoch 972/3000, Training Loss: 0.002795822821232363, Validation Loss: 0.11781263823001288\n",
      "Epoch 973/3000, Training Loss: 0.002785783458380767, Validation Loss: 0.1178566305038744\n",
      "Epoch 974/3000, Training Loss: 0.0027758094970252683, Validation Loss: 0.11790036387609462\n",
      "Epoch 975/3000, Training Loss: 0.002765895363867676, Validation Loss: 0.1179433765591757\n",
      "Epoch 976/3000, Training Loss: 0.0027560436099028115, Validation Loss: 0.11798640266672533\n",
      "Epoch 977/3000, Training Loss: 0.0027462583127143107, Validation Loss: 0.11803000128450733\n",
      "Epoch 978/3000, Training Loss: 0.0027365308246129, Validation Loss: 0.11807293766215217\n",
      "Epoch 979/3000, Training Loss: 0.0027268628853083627, Validation Loss: 0.11811584983516439\n",
      "Epoch 980/3000, Training Loss: 0.0027172569030502716, Validation Loss: 0.11815892170590868\n",
      "Epoch 981/3000, Training Loss: 0.0027077082290145956, Validation Loss: 0.11820175538647826\n",
      "Epoch 982/3000, Training Loss: 0.002698218959468571, Validation Loss: 0.11824457317409144\n",
      "Epoch 983/3000, Training Loss: 0.0026887913829748197, Validation Loss: 0.11828797981833396\n",
      "Epoch 984/3000, Training Loss: 0.0026794206218092214, Validation Loss: 0.11833072812835076\n",
      "Epoch 985/3000, Training Loss: 0.0026701053904498893, Validation Loss: 0.1183734060540432\n",
      "Epoch 986/3000, Training Loss: 0.002660848354101378, Validation Loss: 0.1184162492396292\n",
      "Epoch 987/3000, Training Loss: 0.002651647156024562, Validation Loss: 0.11845883843166857\n",
      "Epoch 988/3000, Training Loss: 0.0026425004904106885, Validation Loss: 0.11850075800110554\n",
      "Epoch 989/3000, Training Loss: 0.002633410371124898, Validation Loss: 0.1185432705572018\n",
      "Epoch 990/3000, Training Loss: 0.002624376686301035, Validation Loss: 0.11858516459844957\n",
      "Epoch 991/3000, Training Loss: 0.0026153949950987743, Validation Loss: 0.11862704469799808\n",
      "Epoch 992/3000, Training Loss: 0.002606467582626219, Validation Loss: 0.118669085882979\n",
      "Epoch 993/3000, Training Loss: 0.0025975949466105217, Validation Loss: 0.11871090688179839\n",
      "Epoch 994/3000, Training Loss: 0.002588773243175216, Validation Loss: 0.11875270350146559\n",
      "Epoch 995/3000, Training Loss: 0.0025800039866093446, Validation Loss: 0.1187944902246679\n",
      "Epoch 996/3000, Training Loss: 0.00257129051437817, Validation Loss: 0.11883683150337397\n",
      "Epoch 997/3000, Training Loss: 0.0025626248519926387, Validation Loss: 0.11887854560020986\n",
      "Epoch 998/3000, Training Loss: 0.00255401030869566, Validation Loss: 0.11892023957664775\n",
      "Epoch 999/3000, Training Loss: 0.0025454482791026144, Validation Loss: 0.11896209185897834\n",
      "Epoch 1000/3000, Training Loss: 0.0025369389072441946, Validation Loss: 0.11900392201191608\n",
      "Epoch 1001/3000, Training Loss: 0.0025284753549874297, Validation Loss: 0.11904550597676007\n",
      "Epoch 1002/3000, Training Loss: 0.002520067098979785, Validation Loss: 0.11908783259244082\n",
      "Epoch 1003/3000, Training Loss: 0.00251170337751555, Validation Loss: 0.11912941851263396\n",
      "Epoch 1004/3000, Training Loss: 0.0025033904318247794, Validation Loss: 0.1191711367807795\n",
      "Epoch 1005/3000, Training Loss: 0.002495123391642797, Validation Loss: 0.11921288776845027\n",
      "Epoch 1006/3000, Training Loss: 0.0024869082804282543, Validation Loss: 0.11925496437218847\n",
      "Epoch 1007/3000, Training Loss: 0.0024787366756982097, Validation Loss: 0.11929676604161973\n",
      "Epoch 1008/3000, Training Loss: 0.0024706159092173106, Validation Loss: 0.11933931800466406\n",
      "Epoch 1009/3000, Training Loss: 0.0024625405343932094, Validation Loss: 0.11938101712833313\n",
      "Epoch 1010/3000, Training Loss: 0.0024545114759953216, Validation Loss: 0.11942288438951922\n",
      "Epoch 1011/3000, Training Loss: 0.002446526142072195, Validation Loss: 0.11946448785642853\n",
      "Epoch 1012/3000, Training Loss: 0.0024385899299232384, Validation Loss: 0.11950642537515001\n",
      "Epoch 1013/3000, Training Loss: 0.002430695935053465, Validation Loss: 0.11954791802948297\n",
      "Epoch 1014/3000, Training Loss: 0.002422849130289743, Validation Loss: 0.11959002175186909\n",
      "Epoch 1015/3000, Training Loss: 0.002415045283998419, Validation Loss: 0.11963140429037024\n",
      "Epoch 1016/3000, Training Loss: 0.0024072871820547135, Validation Loss: 0.11967278252931238\n",
      "Epoch 1017/3000, Training Loss: 0.002399570464062299, Validation Loss: 0.11971389802125167\n",
      "Epoch 1018/3000, Training Loss: 0.0023918986149627157, Validation Loss: 0.11975536189195062\n",
      "Epoch 1019/3000, Training Loss: 0.002384270144008382, Validation Loss: 0.11979637475063688\n",
      "Epoch 1020/3000, Training Loss: 0.002376684812338626, Validation Loss: 0.11983792458705259\n",
      "Epoch 1021/3000, Training Loss: 0.0023691400280210656, Validation Loss: 0.11987833728580054\n",
      "Epoch 1022/3000, Training Loss: 0.00236163964440285, Validation Loss: 0.1199188335572031\n",
      "Epoch 1023/3000, Training Loss: 0.0023541793542641008, Validation Loss: 0.11995891427100755\n",
      "Epoch 1024/3000, Training Loss: 0.0023467604800166495, Validation Loss: 0.11999915866899528\n",
      "Epoch 1025/3000, Training Loss: 0.002339383241004875, Validation Loss: 0.12003970273848937\n",
      "Epoch 1026/3000, Training Loss: 0.002332048248114479, Validation Loss: 0.12007986690633611\n",
      "Epoch 1027/3000, Training Loss: 0.002324751375765502, Validation Loss: 0.12011978924185575\n",
      "Epoch 1028/3000, Training Loss: 0.002317494643833224, Validation Loss: 0.120160067847836\n",
      "Epoch 1029/3000, Training Loss: 0.0023102794284634107, Validation Loss: 0.1201999288282225\n",
      "Epoch 1030/3000, Training Loss: 0.0023031016492560752, Validation Loss: 0.12023995484197317\n",
      "Epoch 1031/3000, Training Loss: 0.0022959637763090013, Validation Loss: 0.12028011430933491\n",
      "Epoch 1032/3000, Training Loss: 0.002288866135046771, Validation Loss: 0.1203202391612529\n",
      "Epoch 1033/3000, Training Loss: 0.002281806118064329, Validation Loss: 0.12035995709968166\n",
      "Epoch 1034/3000, Training Loss: 0.0022747832515315207, Validation Loss: 0.12039982921280673\n",
      "Epoch 1035/3000, Training Loss: 0.002267800303097262, Validation Loss: 0.12043963007343773\n",
      "Epoch 1036/3000, Training Loss: 0.002260854552565395, Validation Loss: 0.12047942649816835\n",
      "Epoch 1037/3000, Training Loss: 0.0022539467691077386, Validation Loss: 0.12051934496837882\n",
      "Epoch 1038/3000, Training Loss: 0.002247074913449876, Validation Loss: 0.12055905874172279\n",
      "Epoch 1039/3000, Training Loss: 0.002240242157735958, Validation Loss: 0.1205987114862566\n",
      "Epoch 1040/3000, Training Loss: 0.002233442989186643, Validation Loss: 0.12063834635163508\n",
      "Epoch 1041/3000, Training Loss: 0.002226681724192425, Validation Loss: 0.12067775374405212\n",
      "Epoch 1042/3000, Training Loss: 0.0022199551611796096, Validation Loss: 0.12071749078345437\n",
      "Epoch 1043/3000, Training Loss: 0.0022132674368755905, Validation Loss: 0.12075716490616901\n",
      "Epoch 1044/3000, Training Loss: 0.0022066116407284207, Validation Loss: 0.12079664605401508\n",
      "Epoch 1045/3000, Training Loss: 0.002199993091843514, Validation Loss: 0.12083613827610161\n",
      "Epoch 1046/3000, Training Loss: 0.0021934098270883903, Validation Loss: 0.12087605318106256\n",
      "Epoch 1047/3000, Training Loss: 0.0021868658820843804, Validation Loss: 0.12091605426476588\n",
      "Epoch 1048/3000, Training Loss: 0.0021803538589121557, Validation Loss: 0.12095620555031164\n",
      "Epoch 1049/3000, Training Loss: 0.0021738808262675028, Validation Loss: 0.12099662293161463\n",
      "Epoch 1050/3000, Training Loss: 0.0021674377240814722, Validation Loss: 0.12103655928018474\n",
      "Epoch 1051/3000, Training Loss: 0.0021610305185507704, Validation Loss: 0.12107662148662912\n",
      "Epoch 1052/3000, Training Loss: 0.0021546539892820815, Validation Loss: 0.12111643777824298\n",
      "Epoch 1053/3000, Training Loss: 0.0021483145679508557, Validation Loss: 0.12115653305569624\n",
      "Epoch 1054/3000, Training Loss: 0.0021420044787461338, Validation Loss: 0.12119660735733562\n",
      "Epoch 1055/3000, Training Loss: 0.0021357298417952297, Validation Loss: 0.12123662051532126\n",
      "Epoch 1056/3000, Training Loss: 0.0021294855995986624, Validation Loss: 0.12127655978071678\n",
      "Epoch 1057/3000, Training Loss: 0.002123275839561776, Validation Loss: 0.12131646525998886\n",
      "Epoch 1058/3000, Training Loss: 0.002117095437882224, Validation Loss: 0.12135611736767575\n",
      "Epoch 1059/3000, Training Loss: 0.002110948263569749, Validation Loss: 0.12139588976837362\n",
      "Epoch 1060/3000, Training Loss: 0.002104833244278275, Validation Loss: 0.12143591891318258\n",
      "Epoch 1061/3000, Training Loss: 0.002098748941797993, Validation Loss: 0.12147555933216185\n",
      "Epoch 1062/3000, Training Loss: 0.0020926940740517513, Validation Loss: 0.12151494594899437\n",
      "Epoch 1063/3000, Training Loss: 0.0020866716691622382, Validation Loss: 0.12155494328486592\n",
      "Epoch 1064/3000, Training Loss: 0.0020806797542175072, Validation Loss: 0.12159451281081993\n",
      "Epoch 1065/3000, Training Loss: 0.002074718246273165, Validation Loss: 0.1216342774909282\n",
      "Epoch 1066/3000, Training Loss: 0.002068787183339228, Validation Loss: 0.12167344995246378\n",
      "Epoch 1067/3000, Training Loss: 0.0020628866121542534, Validation Loss: 0.12171289863841372\n",
      "Epoch 1068/3000, Training Loss: 0.0020570144883076463, Validation Loss: 0.12175194459274606\n",
      "Epoch 1069/3000, Training Loss: 0.002051171725603199, Validation Loss: 0.1217911153313838\n",
      "Epoch 1070/3000, Training Loss: 0.0020453586384940113, Validation Loss: 0.12183026648644595\n",
      "Epoch 1071/3000, Training Loss: 0.002039575778956771, Validation Loss: 0.12186971312567861\n",
      "Epoch 1072/3000, Training Loss: 0.002033820760614806, Validation Loss: 0.1219085666564013\n",
      "Epoch 1073/3000, Training Loss: 0.002028093525312765, Validation Loss: 0.12194755121498448\n",
      "Epoch 1074/3000, Training Loss: 0.0020223962178417157, Validation Loss: 0.12198644600625123\n",
      "Epoch 1075/3000, Training Loss: 0.0020167257246413673, Validation Loss: 0.12202528099606812\n",
      "Epoch 1076/3000, Training Loss: 0.002011081459761409, Validation Loss: 0.12206420999536541\n",
      "Epoch 1077/3000, Training Loss: 0.0020054647256794495, Validation Loss: 0.12210307629101821\n",
      "Epoch 1078/3000, Training Loss: 0.001999877133461928, Validation Loss: 0.1221415677264696\n",
      "Epoch 1079/3000, Training Loss: 0.0019943145581756, Validation Loss: 0.12218018569308148\n",
      "Epoch 1080/3000, Training Loss: 0.001988780752359995, Validation Loss: 0.12221855968202723\n",
      "Epoch 1081/3000, Training Loss: 0.001983272972214201, Validation Loss: 0.12225722866829145\n",
      "Epoch 1082/3000, Training Loss: 0.001977795054327492, Validation Loss: 0.1222958316796226\n",
      "Epoch 1083/3000, Training Loss: 0.0019723406035390178, Validation Loss: 0.1223342196463211\n",
      "Epoch 1084/3000, Training Loss: 0.001966914519210937, Validation Loss: 0.12237237874846477\n",
      "Epoch 1085/3000, Training Loss: 0.0019615135282804504, Validation Loss: 0.12241080796570761\n",
      "Epoch 1086/3000, Training Loss: 0.0019561403334310075, Validation Loss: 0.12244885305103066\n",
      "Epoch 1087/3000, Training Loss: 0.0019507909403812347, Validation Loss: 0.12248717392635075\n",
      "Epoch 1088/3000, Training Loss: 0.0019454699921558852, Validation Loss: 0.12252542879000014\n",
      "Epoch 1089/3000, Training Loss: 0.0019401727025626838, Validation Loss: 0.12256330397846714\n",
      "Epoch 1090/3000, Training Loss: 0.0019349019023472511, Validation Loss: 0.1226013033363959\n",
      "Epoch 1091/3000, Training Loss: 0.0019296544843593736, Validation Loss: 0.12263906742038493\n",
      "Epoch 1092/3000, Training Loss: 0.001924434540182614, Validation Loss: 0.12267743227783381\n",
      "Epoch 1093/3000, Training Loss: 0.001919238893402918, Validation Loss: 0.12271507951937245\n",
      "Epoch 1094/3000, Training Loss: 0.001914067880961096, Validation Loss: 0.12275285674252277\n",
      "Epoch 1095/3000, Training Loss: 0.0019089201801334704, Validation Loss: 0.12279040721372565\n",
      "Epoch 1096/3000, Training Loss: 0.0019037994650780905, Validation Loss: 0.12282825768950661\n",
      "Epoch 1097/3000, Training Loss: 0.001898701953817455, Validation Loss: 0.1228657362963936\n",
      "Epoch 1098/3000, Training Loss: 0.0018936300743556222, Validation Loss: 0.12290338286346933\n",
      "Epoch 1099/3000, Training Loss: 0.0018885814254844592, Validation Loss: 0.12294022836737764\n",
      "Epoch 1100/3000, Training Loss: 0.0018835579923164353, Validation Loss: 0.12297705980956662\n",
      "Epoch 1101/3000, Training Loss: 0.001878556745201143, Validation Loss: 0.12301367562404593\n",
      "Epoch 1102/3000, Training Loss: 0.0018735791914351026, Validation Loss: 0.12305043395825462\n",
      "Epoch 1103/3000, Training Loss: 0.0018686257902974536, Validation Loss: 0.12308751760311035\n",
      "Epoch 1104/3000, Training Loss: 0.0018636958475323196, Validation Loss: 0.12312425073933156\n",
      "Epoch 1105/3000, Training Loss: 0.0018587879077512429, Validation Loss: 0.12316078050317106\n",
      "Epoch 1106/3000, Training Loss: 0.001853902454605995, Validation Loss: 0.12319743694070824\n",
      "Epoch 1107/3000, Training Loss: 0.0018490409997580677, Validation Loss: 0.12323450340699621\n",
      "Epoch 1108/3000, Training Loss: 0.0018442020407987722, Validation Loss: 0.1232718576082974\n",
      "Epoch 1109/3000, Training Loss: 0.0018393861841525922, Validation Loss: 0.1233087152450473\n",
      "Epoch 1110/3000, Training Loss: 0.0018345910030602005, Validation Loss: 0.12334569875510742\n",
      "Epoch 1111/3000, Training Loss: 0.001829819844091931, Validation Loss: 0.12338260423122312\n",
      "Epoch 1112/3000, Training Loss: 0.00182506871654565, Validation Loss: 0.12341947738750039\n",
      "Epoch 1113/3000, Training Loss: 0.001820340743497875, Validation Loss: 0.12345645109221252\n",
      "Epoch 1114/3000, Training Loss: 0.0018156336816006141, Validation Loss: 0.12349336683866105\n",
      "Epoch 1115/3000, Training Loss: 0.0018109498098773684, Validation Loss: 0.12352991298144517\n",
      "Epoch 1116/3000, Training Loss: 0.0018062845826138684, Validation Loss: 0.12356658566211294\n",
      "Epoch 1117/3000, Training Loss: 0.0018016425497584467, Validation Loss: 0.12360303574458144\n",
      "Epoch 1118/3000, Training Loss: 0.0017970198791614832, Validation Loss: 0.12363958502649673\n",
      "Epoch 1119/3000, Training Loss: 0.0017924215685310588, Validation Loss: 0.12367641406921484\n",
      "Epoch 1120/3000, Training Loss: 0.0017878405301057967, Validation Loss: 0.12371251270603116\n",
      "Epoch 1121/3000, Training Loss: 0.0017832820072828397, Validation Loss: 0.12374873863898563\n",
      "Epoch 1122/3000, Training Loss: 0.001778742876735734, Validation Loss: 0.1237848939783592\n",
      "Epoch 1123/3000, Training Loss: 0.0017742256885911612, Validation Loss: 0.12382102156760638\n",
      "Epoch 1124/3000, Training Loss: 0.0017697279824941238, Validation Loss: 0.12385747420685289\n",
      "Epoch 1125/3000, Training Loss: 0.0017652511686050642, Validation Loss: 0.12389370360268778\n",
      "Epoch 1126/3000, Training Loss: 0.0017607943666383928, Validation Loss: 0.12392985171961475\n",
      "Epoch 1127/3000, Training Loss: 0.0017563580132050744, Validation Loss: 0.12396598106063783\n",
      "Epoch 1128/3000, Training Loss: 0.0017519404111702875, Validation Loss: 0.12400190006066031\n",
      "Epoch 1129/3000, Training Loss: 0.0017475438091971726, Validation Loss: 0.12403823396579432\n",
      "Epoch 1130/3000, Training Loss: 0.0017431665586071163, Validation Loss: 0.12407418945904115\n",
      "Epoch 1131/3000, Training Loss: 0.0017388080886975156, Validation Loss: 0.12411011561320318\n",
      "Epoch 1132/3000, Training Loss: 0.0017344686716914879, Validation Loss: 0.1241458388609508\n",
      "Epoch 1133/3000, Training Loss: 0.0017301485098097952, Validation Loss: 0.12418180981063194\n",
      "Epoch 1134/3000, Training Loss: 0.0017258489857691556, Validation Loss: 0.12421778095707181\n",
      "Epoch 1135/3000, Training Loss: 0.001721566906242441, Validation Loss: 0.12425356539790212\n",
      "Epoch 1136/3000, Training Loss: 0.001717303841740186, Validation Loss: 0.12428914243428264\n",
      "Epoch 1137/3000, Training Loss: 0.0017130591280286745, Validation Loss: 0.12432512407152627\n",
      "Epoch 1138/3000, Training Loss: 0.0017088342509780012, Validation Loss: 0.12436076275156029\n",
      "Epoch 1139/3000, Training Loss: 0.0017046269659413303, Validation Loss: 0.1243967878981834\n",
      "Epoch 1140/3000, Training Loss: 0.0017004373117766875, Validation Loss: 0.1244322955471172\n",
      "Epoch 1141/3000, Training Loss: 0.0016962650581659005, Validation Loss: 0.12446850848422363\n",
      "Epoch 1142/3000, Training Loss: 0.0016921130085909254, Validation Loss: 0.12450447804622558\n",
      "Epoch 1143/3000, Training Loss: 0.0016879775095435961, Validation Loss: 0.12454055321322267\n",
      "Epoch 1144/3000, Training Loss: 0.001683862202682726, Validation Loss: 0.12457672204732652\n",
      "Epoch 1145/3000, Training Loss: 0.001679763156193885, Validation Loss: 0.12461265790660866\n",
      "Epoch 1146/3000, Training Loss: 0.0016756831690711539, Validation Loss: 0.12464856239920605\n",
      "Epoch 1147/3000, Training Loss: 0.0016716186405710432, Validation Loss: 0.12468425302700555\n",
      "Epoch 1148/3000, Training Loss: 0.0016675732013955408, Validation Loss: 0.12472003684094957\n",
      "Epoch 1149/3000, Training Loss: 0.001663545012874307, Validation Loss: 0.12475605725950453\n",
      "Epoch 1150/3000, Training Loss: 0.0016595344715705528, Validation Loss: 0.12479195257617964\n",
      "Epoch 1151/3000, Training Loss: 0.0016555399171429078, Validation Loss: 0.12482763743075477\n",
      "Epoch 1152/3000, Training Loss: 0.0016515634143971266, Validation Loss: 0.12486341752886264\n",
      "Epoch 1153/3000, Training Loss: 0.0016476037444668272, Validation Loss: 0.1248991045679468\n",
      "Epoch 1154/3000, Training Loss: 0.0016436616506774434, Validation Loss: 0.12493505278843668\n",
      "Epoch 1155/3000, Training Loss: 0.0016397350488258485, Validation Loss: 0.12497049013672898\n",
      "Epoch 1156/3000, Training Loss: 0.0016358254040673191, Validation Loss: 0.12500614635804208\n",
      "Epoch 1157/3000, Training Loss: 0.001631932822684345, Validation Loss: 0.12504143291219905\n",
      "Epoch 1158/3000, Training Loss: 0.0016280564015982233, Validation Loss: 0.12507681150595903\n",
      "Epoch 1159/3000, Training Loss: 0.001624197227230957, Validation Loss: 0.12511227451135773\n",
      "Epoch 1160/3000, Training Loss: 0.0016203533606245978, Validation Loss: 0.12514766765783414\n",
      "Epoch 1161/3000, Training Loss: 0.0016165264600163098, Validation Loss: 0.12518271462750308\n",
      "Epoch 1162/3000, Training Loss: 0.0016127144507920206, Validation Loss: 0.12521786468651902\n",
      "Epoch 1163/3000, Training Loss: 0.0016089191347921183, Validation Loss: 0.1252530958836724\n",
      "Epoch 1164/3000, Training Loss: 0.0016051397225269515, Validation Loss: 0.12528825173127717\n",
      "Epoch 1165/3000, Training Loss: 0.001601376593958609, Validation Loss: 0.12532307669897622\n",
      "Epoch 1166/3000, Training Loss: 0.0015976275021481984, Validation Loss: 0.12535806532412205\n",
      "Epoch 1167/3000, Training Loss: 0.0015938953194575022, Validation Loss: 0.12539304515261543\n",
      "Epoch 1168/3000, Training Loss: 0.0015901779417097559, Validation Loss: 0.12542852918476208\n",
      "Epoch 1169/3000, Training Loss: 0.0015864781022291875, Validation Loss: 0.12546337876592023\n",
      "Epoch 1170/3000, Training Loss: 0.0015827908395341526, Validation Loss: 0.12549817852261122\n",
      "Epoch 1171/3000, Training Loss: 0.0015791203542097577, Validation Loss: 0.12553307089697052\n",
      "Epoch 1172/3000, Training Loss: 0.0015754637741441672, Validation Loss: 0.12556787566360103\n",
      "Epoch 1173/3000, Training Loss: 0.0015718241681457364, Validation Loss: 0.12560338758754053\n",
      "Epoch 1174/3000, Training Loss: 0.0015681985266906577, Validation Loss: 0.1256383915072817\n",
      "Epoch 1175/3000, Training Loss: 0.0015645883056430794, Validation Loss: 0.12567348546173518\n",
      "Epoch 1176/3000, Training Loss: 0.0015609922611330676, Validation Loss: 0.12570849113969487\n",
      "Epoch 1177/3000, Training Loss: 0.0015574116965295078, Validation Loss: 0.12574345053513877\n",
      "Epoch 1178/3000, Training Loss: 0.0015538453926499998, Validation Loss: 0.12577873104714118\n",
      "Epoch 1179/3000, Training Loss: 0.0015502939059795686, Validation Loss: 0.12581380749333027\n",
      "Epoch 1180/3000, Training Loss: 0.0015467565797105134, Validation Loss: 0.12584878596762145\n",
      "Epoch 1181/3000, Training Loss: 0.001543233774232109, Validation Loss: 0.12588369019064924\n",
      "Epoch 1182/3000, Training Loss: 0.0015397249564327406, Validation Loss: 0.12591841407179097\n",
      "Epoch 1183/3000, Training Loss: 0.0015362305956598714, Validation Loss: 0.1259535068485545\n",
      "Epoch 1184/3000, Training Loss: 0.001532750666083484, Validation Loss: 0.12598822580275532\n",
      "Epoch 1185/3000, Training Loss: 0.0015292841073719253, Validation Loss: 0.1260228820524891\n",
      "Epoch 1186/3000, Training Loss: 0.0015258317289369011, Validation Loss: 0.12605737191038976\n",
      "Epoch 1187/3000, Training Loss: 0.001522392574290149, Validation Loss: 0.1260921174752751\n",
      "Epoch 1188/3000, Training Loss: 0.0015189692017675994, Validation Loss: 0.12612709127854801\n",
      "Epoch 1189/3000, Training Loss: 0.0015155581279837171, Validation Loss: 0.12616174055827475\n",
      "Epoch 1190/3000, Training Loss: 0.0015121612628250032, Validation Loss: 0.12619617494663596\n",
      "Epoch 1191/3000, Training Loss: 0.0015087766611474006, Validation Loss: 0.12623059788553384\n",
      "Epoch 1192/3000, Training Loss: 0.0015054058967847418, Validation Loss: 0.1262652262464755\n",
      "Epoch 1193/3000, Training Loss: 0.0015020480377527141, Validation Loss: 0.12629975977691005\n",
      "Epoch 1194/3000, Training Loss: 0.0014987041653038145, Validation Loss: 0.1263341256798016\n",
      "Epoch 1195/3000, Training Loss: 0.0014953724264564663, Validation Loss: 0.12636828182020723\n",
      "Epoch 1196/3000, Training Loss: 0.001492055152024878, Validation Loss: 0.12640262030702018\n",
      "Epoch 1197/3000, Training Loss: 0.0014887505140427138, Validation Loss: 0.12643659837016555\n",
      "Epoch 1198/3000, Training Loss: 0.0014854596355305743, Validation Loss: 0.1264709644543683\n",
      "Epoch 1199/3000, Training Loss: 0.001482180948596448, Validation Loss: 0.12650484327331146\n",
      "Epoch 1200/3000, Training Loss: 0.0014789155964133544, Validation Loss: 0.12653886243095575\n",
      "Epoch 1201/3000, Training Loss: 0.0014756629984090288, Validation Loss: 0.12657256647052126\n",
      "Epoch 1202/3000, Training Loss: 0.0014724226876067696, Validation Loss: 0.12660636187086613\n",
      "Epoch 1203/3000, Training Loss: 0.0014691959752249693, Validation Loss: 0.12664018997320622\n",
      "Epoch 1204/3000, Training Loss: 0.0014659810777292003, Validation Loss: 0.12667365849707077\n",
      "Epoch 1205/3000, Training Loss: 0.0014627792181303044, Validation Loss: 0.12670681592028643\n",
      "Epoch 1206/3000, Training Loss: 0.0014595888671202928, Validation Loss: 0.12674002576624396\n",
      "Epoch 1207/3000, Training Loss: 0.0014564116429981774, Validation Loss: 0.1267730862512835\n",
      "Epoch 1208/3000, Training Loss: 0.001453246596385651, Validation Loss: 0.12680658970791292\n",
      "Epoch 1209/3000, Training Loss: 0.0014500946280167823, Validation Loss: 0.1268395419277543\n",
      "Epoch 1210/3000, Training Loss: 0.001446952959737747, Validation Loss: 0.1268725917875926\n",
      "Epoch 1211/3000, Training Loss: 0.0014438247708600783, Validation Loss: 0.12690549635889115\n",
      "Epoch 1212/3000, Training Loss: 0.0014407075279201176, Validation Loss: 0.12693873340337933\n",
      "Epoch 1213/3000, Training Loss: 0.0014376044876538339, Validation Loss: 0.12697171130405155\n",
      "Epoch 1214/3000, Training Loss: 0.0014345110171430408, Validation Loss: 0.12700449686932377\n",
      "Epoch 1215/3000, Training Loss: 0.0014314303555325912, Validation Loss: 0.12703734415483722\n",
      "Epoch 1216/3000, Training Loss: 0.0014283608224510112, Validation Loss: 0.1270701539836013\n",
      "Epoch 1217/3000, Training Loss: 0.0014253040606194538, Validation Loss: 0.12710319922852414\n",
      "Epoch 1218/3000, Training Loss: 0.0014222585060287396, Validation Loss: 0.12713579441945322\n",
      "Epoch 1219/3000, Training Loss: 0.0014192246677063904, Validation Loss: 0.12716843101755707\n",
      "Epoch 1220/3000, Training Loss: 0.001416202396396184, Validation Loss: 0.12720102594075403\n",
      "Epoch 1221/3000, Training Loss: 0.0014131921274415794, Validation Loss: 0.1272335661047221\n",
      "Epoch 1222/3000, Training Loss: 0.0014101930153451873, Validation Loss: 0.12726621790289944\n",
      "Epoch 1223/3000, Training Loss: 0.0014072053124600057, Validation Loss: 0.12729869735810173\n",
      "Epoch 1224/3000, Training Loss: 0.0014042287639954838, Validation Loss: 0.12733110548015877\n",
      "Epoch 1225/3000, Training Loss: 0.0014012633956874841, Validation Loss: 0.1273634504722871\n",
      "Epoch 1226/3000, Training Loss: 0.0013983092969040213, Validation Loss: 0.12739565310199547\n",
      "Epoch 1227/3000, Training Loss: 0.0013953657755158692, Validation Loss: 0.1274281717772524\n",
      "Epoch 1228/3000, Training Loss: 0.0013924344980408726, Validation Loss: 0.12746046659619492\n",
      "Epoch 1229/3000, Training Loss: 0.0013895130932901141, Validation Loss: 0.1274930213881575\n",
      "Epoch 1230/3000, Training Loss: 0.0013866038302936758, Validation Loss: 0.12752566295495027\n",
      "Epoch 1231/3000, Training Loss: 0.0013837042564030324, Validation Loss: 0.12755807033631483\n",
      "Epoch 1232/3000, Training Loss: 0.001380817097238981, Validation Loss: 0.12759096633950065\n",
      "Epoch 1233/3000, Training Loss: 0.0013779399543387193, Validation Loss: 0.1276232928712422\n",
      "Epoch 1234/3000, Training Loss: 0.0013750735825059643, Validation Loss: 0.12765567437640574\n",
      "Epoch 1235/3000, Training Loss: 0.001372217179525426, Validation Loss: 0.12768789839180406\n",
      "Epoch 1236/3000, Training Loss: 0.0013693718443641924, Validation Loss: 0.12772028709510685\n",
      "Epoch 1237/3000, Training Loss: 0.0013665377668175256, Validation Loss: 0.12775267313089544\n",
      "Epoch 1238/3000, Training Loss: 0.001363713277985287, Validation Loss: 0.1277848525775837\n",
      "Epoch 1239/3000, Training Loss: 0.0013608990589003738, Validation Loss: 0.12781685491729272\n",
      "Epoch 1240/3000, Training Loss: 0.0013580951796185053, Validation Loss: 0.12784921595431556\n",
      "Epoch 1241/3000, Training Loss: 0.0013553024536734942, Validation Loss: 0.1278815781127804\n",
      "Epoch 1242/3000, Training Loss: 0.001352519410706963, Validation Loss: 0.1279137597923539\n",
      "Epoch 1243/3000, Training Loss: 0.0013497466671075604, Validation Loss: 0.127945720504152\n",
      "Epoch 1244/3000, Training Loss: 0.0013469832634995218, Validation Loss: 0.12797790937729542\n",
      "Epoch 1245/3000, Training Loss: 0.0013442313302793624, Validation Loss: 0.12800991064182762\n",
      "Epoch 1246/3000, Training Loss: 0.0013414883391716138, Validation Loss: 0.12804212830434844\n",
      "Epoch 1247/3000, Training Loss: 0.0013387566692388752, Validation Loss: 0.1280741778016835\n",
      "Epoch 1248/3000, Training Loss: 0.0013360334058583779, Validation Loss: 0.1281062693584345\n",
      "Epoch 1249/3000, Training Loss: 0.0013333218154703446, Validation Loss: 0.12813851498490317\n",
      "Epoch 1250/3000, Training Loss: 0.0013306187155442504, Validation Loss: 0.12817049505651668\n",
      "Epoch 1251/3000, Training Loss: 0.0013279266339195743, Validation Loss: 0.1282027783190141\n",
      "Epoch 1252/3000, Training Loss: 0.00132524327959788, Validation Loss: 0.12823464860314104\n",
      "Epoch 1253/3000, Training Loss: 0.0013225701851899611, Validation Loss: 0.1282667207474706\n",
      "Epoch 1254/3000, Training Loss: 0.0013199062251016804, Validation Loss: 0.12829855153886788\n",
      "Epoch 1255/3000, Training Loss: 0.0013172521880023234, Validation Loss: 0.12833067509319607\n",
      "Epoch 1256/3000, Training Loss: 0.0013146079087235397, Validation Loss: 0.1283623967923029\n",
      "Epoch 1257/3000, Training Loss: 0.0013119726721250585, Validation Loss: 0.12839429565042054\n",
      "Epoch 1258/3000, Training Loss: 0.0013093471660736216, Validation Loss: 0.12842587703456615\n",
      "Epoch 1259/3000, Training Loss: 0.001306730311735179, Validation Loss: 0.12845749020078584\n",
      "Epoch 1260/3000, Training Loss: 0.001304124275561323, Validation Loss: 0.12848914852377796\n",
      "Epoch 1261/3000, Training Loss: 0.0013015266765661149, Validation Loss: 0.12852076852877944\n",
      "Epoch 1262/3000, Training Loss: 0.0012989389860366372, Validation Loss: 0.1285520988041393\n",
      "Epoch 1263/3000, Training Loss: 0.0012963591656304893, Validation Loss: 0.12858338630611132\n",
      "Epoch 1264/3000, Training Loss: 0.0012937897153266916, Validation Loss: 0.12861496729138872\n",
      "Epoch 1265/3000, Training Loss: 0.001291229004358419, Validation Loss: 0.12864625939494756\n",
      "Epoch 1266/3000, Training Loss: 0.0012886775531805662, Validation Loss: 0.12867752027549179\n",
      "Epoch 1267/3000, Training Loss: 0.0012861343149240601, Validation Loss: 0.12870857940129973\n",
      "Epoch 1268/3000, Training Loss: 0.0012836004322310402, Validation Loss: 0.12873975125860176\n",
      "Epoch 1269/3000, Training Loss: 0.001281075640350664, Validation Loss: 0.12877106485286685\n",
      "Epoch 1270/3000, Training Loss: 0.001278560051102708, Validation Loss: 0.12880213436512722\n",
      "Epoch 1271/3000, Training Loss: 0.001276052574335331, Validation Loss: 0.12883302661480023\n",
      "Epoch 1272/3000, Training Loss: 0.0012735536702979985, Validation Loss: 0.12886400421018418\n",
      "Epoch 1273/3000, Training Loss: 0.0012710640221668221, Validation Loss: 0.12889517750327398\n",
      "Epoch 1274/3000, Training Loss: 0.0012685834963236447, Validation Loss: 0.12892611135275767\n",
      "Epoch 1275/3000, Training Loss: 0.0012661112061745062, Validation Loss: 0.1289568741573268\n",
      "Epoch 1276/3000, Training Loss: 0.0012636467655185416, Validation Loss: 0.12898768142828526\n",
      "Epoch 1277/3000, Training Loss: 0.0012611918270497853, Validation Loss: 0.12901835427697014\n",
      "Epoch 1278/3000, Training Loss: 0.0012587450337277148, Validation Loss: 0.1290492737698073\n",
      "Epoch 1279/3000, Training Loss: 0.0012563074980016667, Validation Loss: 0.12907994098500256\n",
      "Epoch 1280/3000, Training Loss: 0.0012538769815714486, Validation Loss: 0.12911043541600484\n",
      "Epoch 1281/3000, Training Loss: 0.0012514555981127905, Validation Loss: 0.12914101464419034\n",
      "Epoch 1282/3000, Training Loss: 0.0012490423612705052, Validation Loss: 0.12917173477559654\n",
      "Epoch 1283/3000, Training Loss: 0.0012466383330808877, Validation Loss: 0.12920221135500654\n",
      "Epoch 1284/3000, Training Loss: 0.0012442414637826684, Validation Loss: 0.12923252464391197\n",
      "Epoch 1285/3000, Training Loss: 0.0012418530172180896, Validation Loss: 0.12926288363462007\n",
      "Epoch 1286/3000, Training Loss: 0.0012394729664186768, Validation Loss: 0.12929289077687633\n",
      "Epoch 1287/3000, Training Loss: 0.001237101226001258, Validation Loss: 0.1293231022023405\n",
      "Epoch 1288/3000, Training Loss: 0.0012347378358770521, Validation Loss: 0.12935294250045873\n",
      "Epoch 1289/3000, Training Loss: 0.0012323815976204911, Validation Loss: 0.12938286916249406\n",
      "Epoch 1290/3000, Training Loss: 0.0012300340655003856, Validation Loss: 0.12941274575878306\n",
      "Epoch 1291/3000, Training Loss: 0.0012276941174085626, Validation Loss: 0.1294428217587771\n",
      "Epoch 1292/3000, Training Loss: 0.001225363240983813, Validation Loss: 0.1294725200640044\n",
      "Epoch 1293/3000, Training Loss: 0.0012230386658791078, Validation Loss: 0.12950218857802212\n",
      "Epoch 1294/3000, Training Loss: 0.001220723021342353, Validation Loss: 0.12953181608669995\n",
      "Epoch 1295/3000, Training Loss: 0.001218415025147042, Validation Loss: 0.1295617053207908\n",
      "Epoch 1296/3000, Training Loss: 0.001216114835018455, Validation Loss: 0.12959178617675238\n",
      "Epoch 1297/3000, Training Loss: 0.0012138227369142033, Validation Loss: 0.12962148959984357\n",
      "Epoch 1298/3000, Training Loss: 0.0012115375658953438, Validation Loss: 0.12965127985445735\n",
      "Epoch 1299/3000, Training Loss: 0.001209260863674209, Validation Loss: 0.12968100370188856\n",
      "Epoch 1300/3000, Training Loss: 0.0012069912205748808, Validation Loss: 0.12971085087203882\n",
      "Epoch 1301/3000, Training Loss: 0.0012047305125654812, Validation Loss: 0.1297403244418902\n",
      "Epoch 1302/3000, Training Loss: 0.0012024758686958225, Validation Loss: 0.12976976661883385\n",
      "Epoch 1303/3000, Training Loss: 0.0012002300243272252, Validation Loss: 0.12979926920785365\n",
      "Epoch 1304/3000, Training Loss: 0.0011979910064970473, Validation Loss: 0.12982836219990987\n",
      "Epoch 1305/3000, Training Loss: 0.0011957596880887687, Validation Loss: 0.12985775430715826\n",
      "Epoch 1306/3000, Training Loss: 0.0011935360293478079, Validation Loss: 0.12988646981610552\n",
      "Epoch 1307/3000, Training Loss: 0.0011913192612169104, Validation Loss: 0.1299153777814104\n",
      "Epoch 1308/3000, Training Loss: 0.0011891104056113218, Validation Loss: 0.12994403563958917\n",
      "Epoch 1309/3000, Training Loss: 0.0011869082176721737, Validation Loss: 0.12997299590969513\n",
      "Epoch 1310/3000, Training Loss: 0.001184714383827359, Validation Loss: 0.13000159612406267\n",
      "Epoch 1311/3000, Training Loss: 0.001182526445829219, Validation Loss: 0.13003038756624957\n",
      "Epoch 1312/3000, Training Loss: 0.001180347117468245, Validation Loss: 0.1300588331726793\n",
      "Epoch 1313/3000, Training Loss: 0.0011781739664672732, Validation Loss: 0.13008720183088462\n",
      "Epoch 1314/3000, Training Loss: 0.0011760091670660964, Validation Loss: 0.13011596126148503\n",
      "Epoch 1315/3000, Training Loss: 0.001173851141439614, Validation Loss: 0.13014435817409975\n",
      "Epoch 1316/3000, Training Loss: 0.0011716999719829796, Validation Loss: 0.1301729458338123\n",
      "Epoch 1317/3000, Training Loss: 0.0011695559770378355, Validation Loss: 0.13020126004033175\n",
      "Epoch 1318/3000, Training Loss: 0.001167418601469774, Validation Loss: 0.1302299911512821\n",
      "Epoch 1319/3000, Training Loss: 0.0011652893043659755, Validation Loss: 0.13025834857749902\n",
      "Epoch 1320/3000, Training Loss: 0.0011631658678095149, Validation Loss: 0.1302868950670016\n",
      "Epoch 1321/3000, Training Loss: 0.0011610502744528533, Validation Loss: 0.130315170254097\n",
      "Epoch 1322/3000, Training Loss: 0.0011589405035422086, Validation Loss: 0.1303433579087705\n",
      "Epoch 1323/3000, Training Loss: 0.0011568388163236255, Validation Loss: 0.13037199403291577\n",
      "Epoch 1324/3000, Training Loss: 0.0011547437991881874, Validation Loss: 0.13040029853800147\n",
      "Epoch 1325/3000, Training Loss: 0.0011526555795740047, Validation Loss: 0.1304286496505124\n",
      "Epoch 1326/3000, Training Loss: 0.001150574152248147, Validation Loss: 0.1304568228440081\n",
      "Epoch 1327/3000, Training Loss: 0.0011484992989400503, Validation Loss: 0.13048547225530194\n",
      "Epoch 1328/3000, Training Loss: 0.001146431973354008, Validation Loss: 0.13051373756823775\n",
      "Epoch 1329/3000, Training Loss: 0.0011443704901758147, Validation Loss: 0.13054218676002358\n",
      "Epoch 1330/3000, Training Loss: 0.0011423162393877943, Validation Loss: 0.13057036651788098\n",
      "Epoch 1331/3000, Training Loss: 0.0011402677358687378, Validation Loss: 0.13059845703938455\n",
      "Epoch 1332/3000, Training Loss: 0.001138226934566561, Validation Loss: 0.1306269261278003\n",
      "Epoch 1333/3000, Training Loss: 0.0011361921868931356, Validation Loss: 0.13065513586019972\n",
      "Epoch 1334/3000, Training Loss: 0.0011341635802930084, Validation Loss: 0.13068334117296074\n",
      "Epoch 1335/3000, Training Loss: 0.0011321416651320618, Validation Loss: 0.13071136968666\n",
      "Epoch 1336/3000, Training Loss: 0.00113012606104667, Validation Loss: 0.13073974546600875\n",
      "Epoch 1337/3000, Training Loss: 0.0011281175479370047, Validation Loss: 0.13076783447159993\n",
      "Epoch 1338/3000, Training Loss: 0.0011261144943480622, Validation Loss: 0.13079591875936078\n",
      "Epoch 1339/3000, Training Loss: 0.0011241185298550614, Validation Loss: 0.1308238274662081\n",
      "Epoch 1340/3000, Training Loss: 0.0011221283017898477, Validation Loss: 0.1308518889162792\n",
      "Epoch 1341/3000, Training Loss: 0.0011201453075031838, Validation Loss: 0.13087998285480096\n",
      "Epoch 1342/3000, Training Loss: 0.0011181684009709276, Validation Loss: 0.13090780836412227\n",
      "Epoch 1343/3000, Training Loss: 0.0011161971560113796, Validation Loss: 0.1309357514625016\n",
      "Epoch 1344/3000, Training Loss: 0.0011142324874236836, Validation Loss: 0.13096352069325723\n",
      "Epoch 1345/3000, Training Loss: 0.0011122739380112952, Validation Loss: 0.13099163328796687\n",
      "Epoch 1346/3000, Training Loss: 0.001110322159751287, Validation Loss: 0.13101939360621073\n",
      "Epoch 1347/3000, Training Loss: 0.0011083756814955258, Validation Loss: 0.13104701095951024\n",
      "Epoch 1348/3000, Training Loss: 0.0011064363179156046, Validation Loss: 0.13107465366815593\n",
      "Epoch 1349/3000, Training Loss: 0.0011045022363722927, Validation Loss: 0.13110247232965894\n",
      "Epoch 1350/3000, Training Loss: 0.001102574640386664, Validation Loss: 0.13113023360121534\n",
      "Epoch 1351/3000, Training Loss: 0.001100653069520549, Validation Loss: 0.13115774015030182\n",
      "Epoch 1352/3000, Training Loss: 0.0010987369415847094, Validation Loss: 0.1311853596654696\n",
      "Epoch 1353/3000, Training Loss: 0.0010968272993233752, Validation Loss: 0.131213032624316\n",
      "Epoch 1354/3000, Training Loss: 0.0010949234430414959, Validation Loss: 0.1312405962892849\n",
      "Epoch 1355/3000, Training Loss: 0.0010930260655932127, Validation Loss: 0.1312681195945681\n",
      "Epoch 1356/3000, Training Loss: 0.001091133742041313, Validation Loss: 0.13129540967543651\n",
      "Epoch 1357/3000, Training Loss: 0.001089248091390452, Validation Loss: 0.13132289146353965\n",
      "Epoch 1358/3000, Training Loss: 0.001087368225058612, Validation Loss: 0.13135043506663194\n",
      "Epoch 1359/3000, Training Loss: 0.0010854941169857754, Validation Loss: 0.13137795007307387\n",
      "Epoch 1360/3000, Training Loss: 0.0010836257154154421, Validation Loss: 0.13140524126145672\n",
      "Epoch 1361/3000, Training Loss: 0.0010817626143118, Validation Loss: 0.13143261240088483\n",
      "Epoch 1362/3000, Training Loss: 0.001079906007037883, Validation Loss: 0.13146004452132834\n",
      "Epoch 1363/3000, Training Loss: 0.0010780545613727226, Validation Loss: 0.1314874527711758\n",
      "Epoch 1364/3000, Training Loss: 0.0010762094191249293, Validation Loss: 0.1315146297959923\n",
      "Epoch 1365/3000, Training Loss: 0.0010743691102665079, Validation Loss: 0.13154160809876717\n",
      "Epoch 1366/3000, Training Loss: 0.0010725351326233241, Validation Loss: 0.13156899743593398\n",
      "Epoch 1367/3000, Training Loss: 0.0010707069890395712, Validation Loss: 0.13159603305959447\n",
      "Epoch 1368/3000, Training Loss: 0.001068884031743994, Validation Loss: 0.13162320850807535\n",
      "Epoch 1369/3000, Training Loss: 0.0010670667917865523, Validation Loss: 0.13165019188384028\n",
      "Epoch 1370/3000, Training Loss: 0.0010652545611930144, Validation Loss: 0.13167722777589846\n",
      "Epoch 1371/3000, Training Loss: 0.0010634487694922382, Validation Loss: 0.1317043807445304\n",
      "Epoch 1372/3000, Training Loss: 0.0010616476630429977, Validation Loss: 0.13173147819304398\n",
      "Epoch 1373/3000, Training Loss: 0.0010598527370030638, Validation Loss: 0.1317583573866578\n",
      "Epoch 1374/3000, Training Loss: 0.0010580625256523602, Validation Loss: 0.13178510481811223\n",
      "Epoch 1375/3000, Training Loss: 0.0010562781410101156, Validation Loss: 0.13181225839100247\n",
      "Epoch 1376/3000, Training Loss: 0.0010544992153383346, Validation Loss: 0.13183907333959594\n",
      "Epoch 1377/3000, Training Loss: 0.001052725437495007, Validation Loss: 0.13186602637321568\n",
      "Epoch 1378/3000, Training Loss: 0.00105095720467158, Validation Loss: 0.13189272410696493\n",
      "Epoch 1379/3000, Training Loss: 0.0010491940072554983, Validation Loss: 0.13191956874565872\n",
      "Epoch 1380/3000, Training Loss: 0.0010474371009074166, Validation Loss: 0.1319460956354628\n",
      "Epoch 1381/3000, Training Loss: 0.0010456844232876726, Validation Loss: 0.13197260358575072\n",
      "Epoch 1382/3000, Training Loss: 0.0010439377624132083, Validation Loss: 0.1319991657638331\n",
      "Epoch 1383/3000, Training Loss: 0.0010421958542944211, Validation Loss: 0.1320256218940698\n",
      "Epoch 1384/3000, Training Loss: 0.0010404594517751946, Validation Loss: 0.13205238169798283\n",
      "Epoch 1385/3000, Training Loss: 0.001038728135090467, Validation Loss: 0.13207872366523593\n",
      "Epoch 1386/3000, Training Loss: 0.0010370020326171095, Validation Loss: 0.1321052135321682\n",
      "Epoch 1387/3000, Training Loss: 0.0010352812432213785, Validation Loss: 0.13213151143591062\n",
      "Epoch 1388/3000, Training Loss: 0.0010335653107317365, Validation Loss: 0.13215808609726798\n",
      "Epoch 1389/3000, Training Loss: 0.001031855051955225, Validation Loss: 0.13218434279250077\n",
      "Epoch 1390/3000, Training Loss: 0.0010301492131830888, Validation Loss: 0.1322105364882684\n",
      "Epoch 1391/3000, Training Loss: 0.001028449002617461, Validation Loss: 0.13223678445099504\n",
      "Epoch 1392/3000, Training Loss: 0.0010267536685936495, Validation Loss: 0.13226312834916654\n",
      "Epoch 1393/3000, Training Loss: 0.0010250634451730086, Validation Loss: 0.13228935251850898\n",
      "Epoch 1394/3000, Training Loss: 0.0010233773381080004, Validation Loss: 0.13231542012773415\n",
      "Epoch 1395/3000, Training Loss: 0.0010216961598507454, Validation Loss: 0.1323416211680129\n",
      "Epoch 1396/3000, Training Loss: 0.001020020319880872, Validation Loss: 0.13236782002399283\n",
      "Epoch 1397/3000, Training Loss: 0.0010183492420413318, Validation Loss: 0.13239378908568178\n",
      "Epoch 1398/3000, Training Loss: 0.001016683627060617, Validation Loss: 0.13241973460551973\n",
      "Epoch 1399/3000, Training Loss: 0.0010150219455113834, Validation Loss: 0.13244574992976169\n",
      "Epoch 1400/3000, Training Loss: 0.001013365571047972, Validation Loss: 0.1324717334441174\n",
      "Epoch 1401/3000, Training Loss: 0.0010117145170037427, Validation Loss: 0.1324977352805569\n",
      "Epoch 1402/3000, Training Loss: 0.001010067936421316, Validation Loss: 0.1325236037598876\n",
      "Epoch 1403/3000, Training Loss: 0.001008426364704845, Validation Loss: 0.13254918514259928\n",
      "Epoch 1404/3000, Training Loss: 0.0010067891585609306, Validation Loss: 0.13257487501371315\n",
      "Epoch 1405/3000, Training Loss: 0.0010051575471369787, Validation Loss: 0.13260071463485898\n",
      "Epoch 1406/3000, Training Loss: 0.0010035303017349075, Validation Loss: 0.13262618022273917\n",
      "Epoch 1407/3000, Training Loss: 0.0010019077244007046, Validation Loss: 0.1326518016759747\n",
      "Epoch 1408/3000, Training Loss: 0.001000289673159943, Validation Loss: 0.13267730549877116\n",
      "Epoch 1409/3000, Training Loss: 0.0009986764355167752, Validation Loss: 0.13270307397123365\n",
      "Epoch 1410/3000, Training Loss: 0.0009970682592464124, Validation Loss: 0.1327284745915188\n",
      "Epoch 1411/3000, Training Loss: 0.0009954641708224215, Validation Loss: 0.13275395615220076\n",
      "Epoch 1412/3000, Training Loss: 0.0009938651967337376, Validation Loss: 0.13277929573591107\n",
      "Epoch 1413/3000, Training Loss: 0.00099227039683116, Validation Loss: 0.13280474343687237\n",
      "Epoch 1414/3000, Training Loss: 0.0009906814784132665, Validation Loss: 0.13283025060822062\n",
      "Epoch 1415/3000, Training Loss: 0.0009890957476950913, Validation Loss: 0.13285555217755088\n",
      "Epoch 1416/3000, Training Loss: 0.000987514816725515, Validation Loss: 0.13288089751229648\n",
      "Epoch 1417/3000, Training Loss: 0.00098593865088526, Validation Loss: 0.13290609408974674\n",
      "Epoch 1418/3000, Training Loss: 0.000984367146006635, Validation Loss: 0.1329316123672385\n",
      "Epoch 1419/3000, Training Loss: 0.0009828003253538283, Validation Loss: 0.13295675386640227\n",
      "Epoch 1420/3000, Training Loss: 0.0009812375166747015, Validation Loss: 0.13298203944574533\n",
      "Epoch 1421/3000, Training Loss: 0.000979679838000418, Validation Loss: 0.13300708527421598\n",
      "Epoch 1422/3000, Training Loss: 0.00097812633872486, Validation Loss: 0.1330322442291411\n",
      "Epoch 1423/3000, Training Loss: 0.0009765775821283984, Validation Loss: 0.13305735864221083\n",
      "Epoch 1424/3000, Training Loss: 0.0009750328897043092, Validation Loss: 0.13308232681918006\n",
      "Epoch 1425/3000, Training Loss: 0.0009734924615085057, Validation Loss: 0.1331074089009126\n",
      "Epoch 1426/3000, Training Loss: 0.000971956772105143, Validation Loss: 0.1331325418858189\n",
      "Epoch 1427/3000, Training Loss: 0.0009704253354333364, Validation Loss: 0.13315757939918896\n",
      "Epoch 1428/3000, Training Loss: 0.0009688983906458399, Validation Loss: 0.13318257334158964\n",
      "Epoch 1429/3000, Training Loss: 0.0009673752581220621, Validation Loss: 0.1332074743566002\n",
      "Epoch 1430/3000, Training Loss: 0.0009658571667351981, Validation Loss: 0.13323235510348228\n",
      "Epoch 1431/3000, Training Loss: 0.0009643429648285922, Validation Loss: 0.13325735934067198\n",
      "Epoch 1432/3000, Training Loss: 0.0009628330140171474, Validation Loss: 0.13328226235324311\n",
      "Epoch 1433/3000, Training Loss: 0.0009613273189957308, Validation Loss: 0.13330705331491643\n",
      "Epoch 1434/3000, Training Loss: 0.0009598256634270016, Validation Loss: 0.13333193480410802\n",
      "Epoch 1435/3000, Training Loss: 0.0009583289928428308, Validation Loss: 0.13335688257227588\n",
      "Epoch 1436/3000, Training Loss: 0.000956835849475082, Validation Loss: 0.13338180889293294\n",
      "Epoch 1437/3000, Training Loss: 0.0009553472040119115, Validation Loss: 0.13340654218353318\n",
      "Epoch 1438/3000, Training Loss: 0.000953862412538805, Validation Loss: 0.13343115778236914\n",
      "Epoch 1439/3000, Training Loss: 0.0009523820972747462, Validation Loss: 0.1334561620212744\n",
      "Epoch 1440/3000, Training Loss: 0.0009509057946351967, Validation Loss: 0.13348084203449517\n",
      "Epoch 1441/3000, Training Loss: 0.0009494333520607107, Validation Loss: 0.1335055830037588\n",
      "Epoch 1442/3000, Training Loss: 0.00094796518647325, Validation Loss: 0.13353023207580011\n",
      "Epoch 1443/3000, Training Loss: 0.0009465009852053157, Validation Loss: 0.13355521445182514\n",
      "Epoch 1444/3000, Training Loss: 0.0009450414549287942, Validation Loss: 0.13357982662342197\n",
      "Epoch 1445/3000, Training Loss: 0.0009435851727047384, Validation Loss: 0.13360434668587304\n",
      "Epoch 1446/3000, Training Loss: 0.000942133249239712, Validation Loss: 0.13362900029796826\n",
      "Epoch 1447/3000, Training Loss: 0.0009406852201712582, Validation Loss: 0.13365376174293045\n",
      "Epoch 1448/3000, Training Loss: 0.0009392414676733194, Validation Loss: 0.13367840421797136\n",
      "Epoch 1449/3000, Training Loss: 0.000937801413221847, Validation Loss: 0.13370292867581574\n",
      "Epoch 1450/3000, Training Loss: 0.0009363651697232088, Validation Loss: 0.13372751237930913\n",
      "Epoch 1451/3000, Training Loss: 0.0009349331678577144, Validation Loss: 0.13375208097525187\n",
      "Epoch 1452/3000, Training Loss: 0.0009335053052598573, Validation Loss: 0.13377689763304146\n",
      "Epoch 1453/3000, Training Loss: 0.0009320815710363933, Validation Loss: 0.1338014902747056\n",
      "Epoch 1454/3000, Training Loss: 0.0009306609893836545, Validation Loss: 0.13382600556797714\n",
      "Epoch 1455/3000, Training Loss: 0.0009292447452569079, Validation Loss: 0.13385057074674617\n",
      "Epoch 1456/3000, Training Loss: 0.0009278326956397919, Validation Loss: 0.13387524368005282\n",
      "Epoch 1457/3000, Training Loss: 0.0009264240972422314, Validation Loss: 0.1338997974701337\n",
      "Epoch 1458/3000, Training Loss: 0.0009250194928245897, Validation Loss: 0.1339242086915443\n",
      "Epoch 1459/3000, Training Loss: 0.0009236184592322593, Validation Loss: 0.13394871822294116\n",
      "Epoch 1460/3000, Training Loss: 0.0009222224348752864, Validation Loss: 0.13397357119228526\n",
      "Epoch 1461/3000, Training Loss: 0.0009208294950303443, Validation Loss: 0.13399823162655772\n",
      "Epoch 1462/3000, Training Loss: 0.0009194404272495286, Validation Loss: 0.1340229614290881\n",
      "Epoch 1463/3000, Training Loss: 0.000918055078469858, Validation Loss: 0.13404754566111385\n",
      "Epoch 1464/3000, Training Loss: 0.0009166736287818738, Validation Loss: 0.13407242961830962\n",
      "Epoch 1465/3000, Training Loss: 0.0009152961432569469, Validation Loss: 0.13409699296085031\n",
      "Epoch 1466/3000, Training Loss: 0.0009139218200887714, Validation Loss: 0.13412163048847903\n",
      "Epoch 1467/3000, Training Loss: 0.0009125516465456653, Validation Loss: 0.1341461303792737\n",
      "Epoch 1468/3000, Training Loss: 0.0009111849868205785, Validation Loss: 0.1341708334603735\n",
      "Epoch 1469/3000, Training Loss: 0.0009098223988695846, Validation Loss: 0.1341953618900523\n",
      "Epoch 1470/3000, Training Loss: 0.0009084631150240686, Validation Loss: 0.1342197539132078\n",
      "Epoch 1471/3000, Training Loss: 0.0009071074583355405, Validation Loss: 0.1342442406356209\n",
      "Epoch 1472/3000, Training Loss: 0.0009057556218353648, Validation Loss: 0.1342687833481277\n",
      "Epoch 1473/3000, Training Loss: 0.0009044075230511904, Validation Loss: 0.1342931872903487\n",
      "Epoch 1474/3000, Training Loss: 0.000903063036965748, Validation Loss: 0.13431747269214553\n",
      "Epoch 1475/3000, Training Loss: 0.0009017218932746445, Validation Loss: 0.1343417390296557\n",
      "Epoch 1476/3000, Training Loss: 0.0009003847986642179, Validation Loss: 0.1343659891853459\n",
      "Epoch 1477/3000, Training Loss: 0.0008990510838821618, Validation Loss: 0.13439040006267652\n",
      "Epoch 1478/3000, Training Loss: 0.0008977209966838501, Validation Loss: 0.1344145749424757\n",
      "Epoch 1479/3000, Training Loss: 0.0008963944929938383, Validation Loss: 0.13443856878057417\n",
      "Epoch 1480/3000, Training Loss: 0.0008950715207114253, Validation Loss: 0.1344625728077108\n",
      "Epoch 1481/3000, Training Loss: 0.0008937527854564087, Validation Loss: 0.1344866362568158\n",
      "Epoch 1482/3000, Training Loss: 0.0008924369772916172, Validation Loss: 0.13451043521603148\n",
      "Epoch 1483/3000, Training Loss: 0.0008911252271557653, Validation Loss: 0.1345341907835051\n",
      "Epoch 1484/3000, Training Loss: 0.0008898163669516097, Validation Loss: 0.13455790693381986\n",
      "Epoch 1485/3000, Training Loss: 0.0008885114831360579, Validation Loss: 0.13458192038767428\n",
      "Epoch 1486/3000, Training Loss: 0.0008872099521097903, Validation Loss: 0.13460569312842152\n",
      "Epoch 1487/3000, Training Loss: 0.0008859116951051676, Validation Loss: 0.13462940086187364\n",
      "Epoch 1488/3000, Training Loss: 0.0008846170677591106, Validation Loss: 0.1346530279599931\n",
      "Epoch 1489/3000, Training Loss: 0.0008833258620691649, Validation Loss: 0.13467672451386403\n",
      "Epoch 1490/3000, Training Loss: 0.0008820384616903425, Validation Loss: 0.13470021213988123\n",
      "Epoch 1491/3000, Training Loss: 0.0008807536199056136, Validation Loss: 0.13472373482917252\n",
      "Epoch 1492/3000, Training Loss: 0.000879472489538987, Validation Loss: 0.13474730715101796\n",
      "Epoch 1493/3000, Training Loss: 0.0008781948326670224, Validation Loss: 0.13477098761086767\n",
      "Epoch 1494/3000, Training Loss: 0.000876920516099376, Validation Loss: 0.13479453995336355\n",
      "Epoch 1495/3000, Training Loss: 0.0008756495108233874, Validation Loss: 0.13481797037912768\n",
      "Epoch 1496/3000, Training Loss: 0.0008743817259980988, Validation Loss: 0.13484137052658918\n",
      "Epoch 1497/3000, Training Loss: 0.0008731178205629082, Validation Loss: 0.13486490977711696\n",
      "Epoch 1498/3000, Training Loss: 0.0008718567772571348, Validation Loss: 0.1348882392765062\n",
      "Epoch 1499/3000, Training Loss: 0.0008705991205322797, Validation Loss: 0.13491162232593618\n",
      "Epoch 1500/3000, Training Loss: 0.0008693445686347727, Validation Loss: 0.1349349263835538\n",
      "Epoch 1501/3000, Training Loss: 0.0008680934248252344, Validation Loss: 0.13495846613681883\n",
      "Epoch 1502/3000, Training Loss: 0.0008668457764392296, Validation Loss: 0.13498173519501197\n",
      "Epoch 1503/3000, Training Loss: 0.0008656008736264396, Validation Loss: 0.1350050111074352\n",
      "Epoch 1504/3000, Training Loss: 0.0008643597251875164, Validation Loss: 0.13502829824637658\n",
      "Epoch 1505/3000, Training Loss: 0.0008631212921545101, Validation Loss: 0.13505166779177552\n",
      "Epoch 1506/3000, Training Loss: 0.0008618866491410294, Validation Loss: 0.1350749228125725\n",
      "Epoch 1507/3000, Training Loss: 0.000860654715752641, Validation Loss: 0.13509804503695588\n",
      "Epoch 1508/3000, Training Loss: 0.0008594259993091307, Validation Loss: 0.13512126631340016\n",
      "Epoch 1509/3000, Training Loss: 0.000858200503269028, Validation Loss: 0.13514455224916774\n",
      "Epoch 1510/3000, Training Loss: 0.0008569783555404371, Validation Loss: 0.1351677430976081\n",
      "Epoch 1511/3000, Training Loss: 0.0008557591999764289, Validation Loss: 0.13519076392438492\n",
      "Epoch 1512/3000, Training Loss: 0.0008545429747161145, Validation Loss: 0.13521381832598472\n",
      "Epoch 1513/3000, Training Loss: 0.0008533300495599807, Validation Loss: 0.1352369672295885\n",
      "Epoch 1514/3000, Training Loss: 0.0008521204157224464, Validation Loss: 0.13526027098093632\n",
      "Epoch 1515/3000, Training Loss: 0.0008509135695220088, Validation Loss: 0.13528339110784207\n",
      "Epoch 1516/3000, Training Loss: 0.0008497098262400765, Validation Loss: 0.13530638089267694\n",
      "Epoch 1517/3000, Training Loss: 0.0008485089689348269, Validation Loss: 0.13532946848382746\n",
      "Epoch 1518/3000, Training Loss: 0.0008473117591490145, Validation Loss: 0.13535261904184165\n",
      "Epoch 1519/3000, Training Loss: 0.0008461171570633806, Validation Loss: 0.13537561047588206\n",
      "Epoch 1520/3000, Training Loss: 0.0008449257480361298, Validation Loss: 0.13539856377054216\n",
      "Epoch 1521/3000, Training Loss: 0.0008437369221948034, Validation Loss: 0.13542148400070142\n",
      "Epoch 1522/3000, Training Loss: 0.0008425516081435865, Validation Loss: 0.1354446833655287\n",
      "Epoch 1523/3000, Training Loss: 0.0008413691785091879, Validation Loss: 0.13546766134919833\n",
      "Epoch 1524/3000, Training Loss: 0.0008401896541725853, Validation Loss: 0.13549062231531606\n",
      "Epoch 1525/3000, Training Loss: 0.0008390132537101766, Validation Loss: 0.13551349865966558\n",
      "Epoch 1526/3000, Training Loss: 0.0008378399469983836, Validation Loss: 0.13553654701260046\n",
      "Epoch 1527/3000, Training Loss: 0.0008366697010449252, Validation Loss: 0.13555939688240773\n",
      "Epoch 1528/3000, Training Loss: 0.0008355017806356813, Validation Loss: 0.13558222857431967\n",
      "Epoch 1529/3000, Training Loss: 0.0008343370523294808, Validation Loss: 0.13560511114758975\n",
      "Epoch 1530/3000, Training Loss: 0.0008331754433838138, Validation Loss: 0.13562811318693777\n",
      "Epoch 1531/3000, Training Loss: 0.0008320164916658711, Validation Loss: 0.13565099881633097\n",
      "Epoch 1532/3000, Training Loss: 0.000830860443910571, Validation Loss: 0.13567378148502068\n",
      "Epoch 1533/3000, Training Loss: 0.0008297073005952558, Validation Loss: 0.13569662260645945\n",
      "Epoch 1534/3000, Training Loss: 0.0008285573510025616, Validation Loss: 0.13571955759033152\n",
      "Epoch 1535/3000, Training Loss: 0.0008274097923040429, Validation Loss: 0.13574229247819822\n",
      "Epoch 1536/3000, Training Loss: 0.0008262652809106179, Validation Loss: 0.13576508567724863\n",
      "Epoch 1537/3000, Training Loss: 0.0008251236178635037, Validation Loss: 0.13578779238668254\n",
      "Epoch 1538/3000, Training Loss: 0.000823984933878184, Validation Loss: 0.1358107301525301\n",
      "Epoch 1539/3000, Training Loss: 0.0008228492285143444, Validation Loss: 0.1358334071299084\n",
      "Epoch 1540/3000, Training Loss: 0.0008217160667260638, Validation Loss: 0.1358560888600461\n",
      "Epoch 1541/3000, Training Loss: 0.0008205859179134955, Validation Loss: 0.13587871076949162\n",
      "Epoch 1542/3000, Training Loss: 0.0008194583279272331, Validation Loss: 0.13590159160653675\n",
      "Epoch 1543/3000, Training Loss: 0.000818333833232103, Validation Loss: 0.13592429276474874\n",
      "Epoch 1544/3000, Training Loss: 0.0008172118392438366, Validation Loss: 0.13594686672592945\n",
      "Epoch 1545/3000, Training Loss: 0.0008160925714593836, Validation Loss: 0.13596949332561797\n",
      "Epoch 1546/3000, Training Loss: 0.0008149764035428432, Validation Loss: 0.13599211857559543\n",
      "Epoch 1547/3000, Training Loss: 0.0008138629985258507, Validation Loss: 0.13601462908077327\n",
      "Epoch 1548/3000, Training Loss: 0.0008127522161596364, Validation Loss: 0.1360372104250979\n",
      "Epoch 1549/3000, Training Loss: 0.0008116439389971821, Validation Loss: 0.1360596724106285\n",
      "Epoch 1550/3000, Training Loss: 0.0008105386842593411, Validation Loss: 0.13608262101974442\n",
      "Epoch 1551/3000, Training Loss: 0.0008094363928444, Validation Loss: 0.13610521478221857\n",
      "Epoch 1552/3000, Training Loss: 0.0008083363969672282, Validation Loss: 0.13612783672985176\n",
      "Epoch 1553/3000, Training Loss: 0.0008072394920963098, Validation Loss: 0.136150340543388\n",
      "Epoch 1554/3000, Training Loss: 0.0008061448820993491, Validation Loss: 0.13617316476100844\n",
      "Epoch 1555/3000, Training Loss: 0.0008050535569540854, Validation Loss: 0.1361961596514077\n",
      "Epoch 1556/3000, Training Loss: 0.0008039646048379297, Validation Loss: 0.13621899870016022\n",
      "Epoch 1557/3000, Training Loss: 0.0008028784876744747, Validation Loss: 0.13624184829104496\n",
      "Epoch 1558/3000, Training Loss: 0.0008017951742536498, Validation Loss: 0.13626478642627\n",
      "Epoch 1559/3000, Training Loss: 0.0008007146098986059, Validation Loss: 0.13628763691855528\n",
      "Epoch 1560/3000, Training Loss: 0.000799636740797281, Validation Loss: 0.13631037614311617\n",
      "Epoch 1561/3000, Training Loss: 0.0007985610194310335, Validation Loss: 0.13633308313124348\n",
      "Epoch 1562/3000, Training Loss: 0.0007974883279923585, Validation Loss: 0.13635601341100784\n",
      "Epoch 1563/3000, Training Loss: 0.0007964182024922415, Validation Loss: 0.136378698237724\n",
      "Epoch 1564/3000, Training Loss: 0.0007953505346778411, Validation Loss: 0.13640143895212686\n",
      "Epoch 1565/3000, Training Loss: 0.0007942854434130796, Validation Loss: 0.13642409718405116\n",
      "Epoch 1566/3000, Training Loss: 0.0007932229883258796, Validation Loss: 0.13644686232157108\n",
      "Epoch 1567/3000, Training Loss: 0.0007921635946097942, Validation Loss: 0.1364694251568052\n",
      "Epoch 1568/3000, Training Loss: 0.000791106031480343, Validation Loss: 0.1364919974602656\n",
      "Epoch 1569/3000, Training Loss: 0.0007900513784456694, Validation Loss: 0.1365145844451593\n",
      "Epoch 1570/3000, Training Loss: 0.0007889991457948444, Validation Loss: 0.13653726750574588\n",
      "Epoch 1571/3000, Training Loss: 0.0007879496341538368, Validation Loss: 0.136559835502676\n",
      "Epoch 1572/3000, Training Loss: 0.0007869024266301585, Validation Loss: 0.13658227627135033\n",
      "Epoch 1573/3000, Training Loss: 0.0007858576957263221, Validation Loss: 0.1366047425524935\n",
      "Epoch 1574/3000, Training Loss: 0.0007848158496352133, Validation Loss: 0.13662728505304395\n",
      "Epoch 1575/3000, Training Loss: 0.000783776160774399, Validation Loss: 0.1366496914851231\n",
      "Epoch 1576/3000, Training Loss: 0.0007827389835428841, Validation Loss: 0.1366721656135801\n",
      "Epoch 1577/3000, Training Loss: 0.0007817041537300473, Validation Loss: 0.13669454472228842\n",
      "Epoch 1578/3000, Training Loss: 0.0007806722142679059, Validation Loss: 0.1367169274538201\n",
      "Epoch 1579/3000, Training Loss: 0.0007796427287819892, Validation Loss: 0.13673905466302982\n",
      "Epoch 1580/3000, Training Loss: 0.0007786154628235291, Validation Loss: 0.13676114848123563\n",
      "Epoch 1581/3000, Training Loss: 0.000777590998922814, Validation Loss: 0.1367831861352247\n",
      "Epoch 1582/3000, Training Loss: 0.0007765686656410473, Validation Loss: 0.13680537282267505\n",
      "Epoch 1583/3000, Training Loss: 0.0007755490369919601, Validation Loss: 0.13682748763417574\n",
      "Epoch 1584/3000, Training Loss: 0.0007745315693887953, Validation Loss: 0.13684948219578794\n",
      "Epoch 1585/3000, Training Loss: 0.0007735165028140184, Validation Loss: 0.13687156458205013\n",
      "Epoch 1586/3000, Training Loss: 0.0007725041242672224, Validation Loss: 0.13689370481640428\n",
      "Epoch 1587/3000, Training Loss: 0.0007714940529643221, Validation Loss: 0.13691570317435955\n",
      "Epoch 1588/3000, Training Loss: 0.0007704864683277155, Validation Loss: 0.13693759740526257\n",
      "Epoch 1589/3000, Training Loss: 0.0007694808410311993, Validation Loss: 0.13695945525548345\n",
      "Epoch 1590/3000, Training Loss: 0.0007684780272151684, Validation Loss: 0.13698157481401596\n",
      "Epoch 1591/3000, Training Loss: 0.0007674774031407734, Validation Loss: 0.1370034215844894\n",
      "Epoch 1592/3000, Training Loss: 0.0007664790106051702, Validation Loss: 0.13702531059580142\n",
      "Epoch 1593/3000, Training Loss: 0.0007654830640593229, Validation Loss: 0.13704713677671196\n",
      "Epoch 1594/3000, Training Loss: 0.000764489655592347, Validation Loss: 0.13706912067775018\n",
      "Epoch 1595/3000, Training Loss: 0.000763498709057085, Validation Loss: 0.13709091386172373\n",
      "Epoch 1596/3000, Training Loss: 0.0007625095746773399, Validation Loss: 0.13711275970364367\n",
      "Epoch 1597/3000, Training Loss: 0.0007615230056963785, Validation Loss: 0.13713458145203347\n",
      "Epoch 1598/3000, Training Loss: 0.0007605389051516229, Validation Loss: 0.13715649978616878\n",
      "Epoch 1599/3000, Training Loss: 0.0007595568995725936, Validation Loss: 0.1371783957234638\n",
      "Epoch 1600/3000, Training Loss: 0.0007585772448064685, Validation Loss: 0.13720016003555377\n",
      "Epoch 1601/3000, Training Loss: 0.0007575998708742374, Validation Loss: 0.13722189986288288\n",
      "Epoch 1602/3000, Training Loss: 0.0007566252494819874, Validation Loss: 0.13724375004314104\n",
      "Epoch 1603/3000, Training Loss: 0.0007556523232321615, Validation Loss: 0.1372654325948873\n",
      "Epoch 1604/3000, Training Loss: 0.0007546817752933454, Validation Loss: 0.13728717978831123\n",
      "Epoch 1605/3000, Training Loss: 0.0007537134471654956, Validation Loss: 0.13730887321151175\n",
      "Epoch 1606/3000, Training Loss: 0.0007527477566281616, Validation Loss: 0.1373308488187191\n",
      "Epoch 1607/3000, Training Loss: 0.0007517840362277669, Validation Loss: 0.13735246840783216\n",
      "Epoch 1608/3000, Training Loss: 0.0007508225125354072, Validation Loss: 0.13737409463329445\n",
      "Epoch 1609/3000, Training Loss: 0.000749863441993757, Validation Loss: 0.13739583383599802\n",
      "Epoch 1610/3000, Training Loss: 0.0007489064221069495, Validation Loss: 0.13741736929141302\n",
      "Epoch 1611/3000, Training Loss: 0.0007479515258490141, Validation Loss: 0.137439047194353\n",
      "Epoch 1612/3000, Training Loss: 0.0007469988641753547, Validation Loss: 0.1374606047094211\n",
      "Epoch 1613/3000, Training Loss: 0.0007460483477867451, Validation Loss: 0.13748235945504558\n",
      "Epoch 1614/3000, Training Loss: 0.0007451005223209186, Validation Loss: 0.13750390500329088\n",
      "Epoch 1615/3000, Training Loss: 0.0007441542272308397, Validation Loss: 0.1375254775347262\n",
      "Epoch 1616/3000, Training Loss: 0.0007432103676539533, Validation Loss: 0.13754706995986116\n",
      "Epoch 1617/3000, Training Loss: 0.0007422685930071308, Validation Loss: 0.13756871897078635\n",
      "Epoch 1618/3000, Training Loss: 0.0007413291401049306, Validation Loss: 0.13759027787201394\n",
      "Epoch 1619/3000, Training Loss: 0.0007403916197841629, Validation Loss: 0.13761174121020928\n",
      "Epoch 1620/3000, Training Loss: 0.0007394562028899162, Validation Loss: 0.13763318994890109\n",
      "Epoch 1621/3000, Training Loss: 0.0007385232774451171, Validation Loss: 0.13765479121041907\n",
      "Epoch 1622/3000, Training Loss: 0.0007375921538971946, Validation Loss: 0.13767620949654943\n",
      "Epoch 1623/3000, Training Loss: 0.0007366632373651537, Validation Loss: 0.13769765945401913\n",
      "Epoch 1624/3000, Training Loss: 0.0007357363827563486, Validation Loss: 0.13771903485834752\n",
      "Epoch 1625/3000, Training Loss: 0.0007348118473264524, Validation Loss: 0.13774068376489887\n",
      "Epoch 1626/3000, Training Loss: 0.0007338894689139667, Validation Loss: 0.1377620250410763\n",
      "Epoch 1627/3000, Training Loss: 0.0007329688996549832, Validation Loss: 0.13778337230699145\n",
      "Epoch 1628/3000, Training Loss: 0.0007320506635108376, Validation Loss: 0.13780466547171488\n",
      "Epoch 1629/3000, Training Loss: 0.0007311342994427218, Validation Loss: 0.13782612827446436\n",
      "Epoch 1630/3000, Training Loss: 0.0007302201642136447, Validation Loss: 0.1378474843237457\n",
      "Epoch 1631/3000, Training Loss: 0.0007293079078239117, Validation Loss: 0.13786872622641516\n",
      "Epoch 1632/3000, Training Loss: 0.0007283976929373927, Validation Loss: 0.1378900508707683\n",
      "Epoch 1633/3000, Training Loss: 0.0007274898464158878, Validation Loss: 0.1379114313870762\n",
      "Epoch 1634/3000, Training Loss: 0.0007265838460989645, Validation Loss: 0.13793267686003868\n",
      "Epoch 1635/3000, Training Loss: 0.0007256800270490613, Validation Loss: 0.13795394721938953\n",
      "Epoch 1636/3000, Training Loss: 0.0007247778383064519, Validation Loss: 0.13797511946958987\n",
      "Epoch 1637/3000, Training Loss: 0.0007238781112603457, Validation Loss: 0.1379965403483519\n",
      "Epoch 1638/3000, Training Loss: 0.0007229801977869135, Validation Loss: 0.13801769946823908\n",
      "Epoch 1639/3000, Training Loss: 0.0007220841618199801, Validation Loss: 0.13803912983530983\n",
      "Epoch 1640/3000, Training Loss: 0.0007211903690737407, Validation Loss: 0.13806044335307951\n",
      "Epoch 1641/3000, Training Loss: 0.0007202987604709789, Validation Loss: 0.13808192653781534\n",
      "Epoch 1642/3000, Training Loss: 0.0007194090698817675, Validation Loss: 0.13810318461158266\n",
      "Epoch 1643/3000, Training Loss: 0.0007185210266117337, Validation Loss: 0.13812444540996943\n",
      "Epoch 1644/3000, Training Loss: 0.000717635102276121, Validation Loss: 0.1381459350501176\n",
      "Epoch 1645/3000, Training Loss: 0.0007167515022002518, Validation Loss: 0.13816720097716115\n",
      "Epoch 1646/3000, Training Loss: 0.0007158694518234599, Validation Loss: 0.1381884286763489\n",
      "Epoch 1647/3000, Training Loss: 0.0007149897077057794, Validation Loss: 0.13820956578241397\n",
      "Epoch 1648/3000, Training Loss: 0.0007141116326141545, Validation Loss: 0.13823083168042175\n",
      "Epoch 1649/3000, Training Loss: 0.0007132357522425575, Validation Loss: 0.13825201104352863\n",
      "Epoch 1650/3000, Training Loss: 0.0007123616073446884, Validation Loss: 0.13827310009529903\n",
      "Epoch 1651/3000, Training Loss: 0.0007114893402009128, Validation Loss: 0.13829422609801859\n",
      "Epoch 1652/3000, Training Loss: 0.000710619267648276, Validation Loss: 0.13831544303131502\n",
      "Epoch 1653/3000, Training Loss: 0.0007097509629151253, Validation Loss: 0.13833642284237402\n",
      "Epoch 1654/3000, Training Loss: 0.0007088846831079631, Validation Loss: 0.13835748568089623\n",
      "Epoch 1655/3000, Training Loss: 0.00070802016381864, Validation Loss: 0.13837853925217278\n",
      "Epoch 1656/3000, Training Loss: 0.0007071576967598927, Validation Loss: 0.13839976867175838\n",
      "Epoch 1657/3000, Training Loss: 0.0007062971631483886, Validation Loss: 0.13842075990643243\n",
      "Epoch 1658/3000, Training Loss: 0.0007054381978828794, Validation Loss: 0.13844181067879074\n",
      "Epoch 1659/3000, Training Loss: 0.0007045814179135976, Validation Loss: 0.1384627487019615\n",
      "Epoch 1660/3000, Training Loss: 0.000703726477632629, Validation Loss: 0.1384837986367617\n",
      "Epoch 1661/3000, Training Loss: 0.0007028734349350687, Validation Loss: 0.13850480215492963\n",
      "Epoch 1662/3000, Training Loss: 0.0007020221453224387, Validation Loss: 0.13852569026760567\n",
      "Epoch 1663/3000, Training Loss: 0.0007011726018126571, Validation Loss: 0.13854665742224256\n",
      "Epoch 1664/3000, Training Loss: 0.000700325359259604, Validation Loss: 0.1385676750046501\n",
      "Epoch 1665/3000, Training Loss: 0.0006994795303931089, Validation Loss: 0.13858859236356735\n",
      "Epoch 1666/3000, Training Loss: 0.0006986359285327204, Validation Loss: 0.13860928488649568\n",
      "Epoch 1667/3000, Training Loss: 0.0006977937911640781, Validation Loss: 0.13863008747613761\n",
      "Epoch 1668/3000, Training Loss: 0.0006969538468846075, Validation Loss: 0.13865061113888077\n",
      "Epoch 1669/3000, Training Loss: 0.0006961155727465792, Validation Loss: 0.13867104626994683\n",
      "Epoch 1670/3000, Training Loss: 0.000695278938600821, Validation Loss: 0.13869152411149271\n",
      "Epoch 1671/3000, Training Loss: 0.0006944444487817448, Validation Loss: 0.13871209468267975\n",
      "Epoch 1672/3000, Training Loss: 0.000693611653874305, Validation Loss: 0.13873243519052292\n",
      "Epoch 1673/3000, Training Loss: 0.0006927807032648969, Validation Loss: 0.13875287311266596\n",
      "Epoch 1674/3000, Training Loss: 0.0006919514618890588, Validation Loss: 0.1387732452332818\n",
      "Epoch 1675/3000, Training Loss: 0.0006911240059956442, Validation Loss: 0.13879387667246532\n",
      "Epoch 1676/3000, Training Loss: 0.000690298647498398, Validation Loss: 0.13881421702142185\n",
      "Epoch 1677/3000, Training Loss: 0.0006894745556849444, Validation Loss: 0.13883454447910984\n",
      "Epoch 1678/3000, Training Loss: 0.0006886526108975154, Validation Loss: 0.13885477536378704\n",
      "Epoch 1679/3000, Training Loss: 0.0006878323782541146, Validation Loss: 0.13887511756240734\n",
      "Epoch 1680/3000, Training Loss: 0.0006870139482676164, Validation Loss: 0.13889550264939382\n",
      "Epoch 1681/3000, Training Loss: 0.0006861972967170842, Validation Loss: 0.1389157777544035\n",
      "Epoch 1682/3000, Training Loss: 0.0006853822229594817, Validation Loss: 0.1389361329262875\n",
      "Epoch 1683/3000, Training Loss: 0.0006845693479350892, Validation Loss: 0.13895654074299083\n",
      "Epoch 1684/3000, Training Loss: 0.0006837578594374304, Validation Loss: 0.13897675728147119\n",
      "Epoch 1685/3000, Training Loss: 0.0006829483546121016, Validation Loss: 0.13899703986150377\n",
      "Epoch 1686/3000, Training Loss: 0.0006821404042544526, Validation Loss: 0.1390172296152221\n",
      "Epoch 1687/3000, Training Loss: 0.0006813343782984398, Validation Loss: 0.13903762055165356\n",
      "Epoch 1688/3000, Training Loss: 0.0006805299879287253, Validation Loss: 0.13905773457005524\n",
      "Epoch 1689/3000, Training Loss: 0.0006797270930734765, Validation Loss: 0.13907779729007042\n",
      "Epoch 1690/3000, Training Loss: 0.0006789263378808458, Validation Loss: 0.1390980611167831\n",
      "Epoch 1691/3000, Training Loss: 0.0006781271233176762, Validation Loss: 0.139118097703714\n",
      "Epoch 1692/3000, Training Loss: 0.0006773297094888752, Validation Loss: 0.1391381425633191\n",
      "Epoch 1693/3000, Training Loss: 0.0006765338261632863, Validation Loss: 0.1391581003396198\n",
      "Epoch 1694/3000, Training Loss: 0.0006757396261405848, Validation Loss: 0.1391782463337231\n",
      "Epoch 1695/3000, Training Loss: 0.0006749475025361307, Validation Loss: 0.13919814781750559\n",
      "Epoch 1696/3000, Training Loss: 0.0006741566213601791, Validation Loss: 0.13921807989333598\n",
      "Epoch 1697/3000, Training Loss: 0.0006733676718980096, Validation Loss: 0.13923797802204294\n",
      "Epoch 1698/3000, Training Loss: 0.0006725804841400722, Validation Loss: 0.1392579836703282\n",
      "Epoch 1699/3000, Training Loss: 0.0006717947236014333, Validation Loss: 0.1392779103977897\n",
      "Epoch 1700/3000, Training Loss: 0.0006710107283688439, Validation Loss: 0.1392977525458762\n",
      "Epoch 1701/3000, Training Loss: 0.0006702281469868155, Validation Loss: 0.1393175645620915\n",
      "Epoch 1702/3000, Training Loss: 0.0006694477037193156, Validation Loss: 0.13933761331936517\n",
      "Epoch 1703/3000, Training Loss: 0.0006686684743474862, Validation Loss: 0.13935741958212777\n",
      "Epoch 1704/3000, Training Loss: 0.0006678910857556257, Validation Loss: 0.13937721848460863\n",
      "Epoch 1705/3000, Training Loss: 0.0006671152608875238, Validation Loss: 0.13939715894304833\n",
      "Epoch 1706/3000, Training Loss: 0.000666341111388499, Validation Loss: 0.13941700202369403\n",
      "Epoch 1707/3000, Training Loss: 0.0006655686753756084, Validation Loss: 0.139436799070457\n",
      "Epoch 1708/3000, Training Loss: 0.0006647975431855525, Validation Loss: 0.13945654552249598\n",
      "Epoch 1709/3000, Training Loss: 0.0006640283411104081, Validation Loss: 0.13947649003932283\n",
      "Epoch 1710/3000, Training Loss: 0.0006632606629023789, Validation Loss: 0.13949621158435052\n",
      "Epoch 1711/3000, Training Loss: 0.0006624945787787344, Validation Loss: 0.1395159447057361\n",
      "Epoch 1712/3000, Training Loss: 0.0006617300443562109, Validation Loss: 0.13953562490425675\n",
      "Epoch 1713/3000, Training Loss: 0.0006609670617205811, Validation Loss: 0.1395554320557934\n",
      "Epoch 1714/3000, Training Loss: 0.000660205954783865, Validation Loss: 0.13957514268737775\n",
      "Epoch 1715/3000, Training Loss: 0.0006594460782856274, Validation Loss: 0.13959475127994644\n",
      "Epoch 1716/3000, Training Loss: 0.0006586879387886945, Validation Loss: 0.1396144388028632\n",
      "Epoch 1717/3000, Training Loss: 0.0006579315075543359, Validation Loss: 0.1396341283328837\n",
      "Epoch 1718/3000, Training Loss: 0.0006571765287572391, Validation Loss: 0.13965377744157056\n",
      "Epoch 1719/3000, Training Loss: 0.0006564232203187007, Validation Loss: 0.13967342906231636\n",
      "Epoch 1720/3000, Training Loss: 0.0006556711751175844, Validation Loss: 0.13969313721472054\n",
      "Epoch 1721/3000, Training Loss: 0.0006549211563528167, Validation Loss: 0.13971276430317958\n",
      "Epoch 1722/3000, Training Loss: 0.000654172270992602, Validation Loss: 0.13973230869456393\n",
      "Epoch 1723/3000, Training Loss: 0.0006534250548344589, Validation Loss: 0.13975184293541965\n",
      "Epoch 1724/3000, Training Loss: 0.0006526795040590739, Validation Loss: 0.13977151928293813\n",
      "Epoch 1725/3000, Training Loss: 0.0006519353909586069, Validation Loss: 0.13979098007452476\n",
      "Epoch 1726/3000, Training Loss: 0.0006511928830348171, Validation Loss: 0.1398104820160446\n",
      "Epoch 1727/3000, Training Loss: 0.0006504517147400013, Validation Loss: 0.1398299204294779\n",
      "Epoch 1728/3000, Training Loss: 0.0006497123096738364, Validation Loss: 0.13984955690600093\n",
      "Epoch 1729/3000, Training Loss: 0.000648974436791388, Validation Loss: 0.13986895692654433\n",
      "Epoch 1730/3000, Training Loss: 0.0006482380478775987, Validation Loss: 0.1398883096090576\n",
      "Epoch 1731/3000, Training Loss: 0.0006475031897931786, Validation Loss: 0.1399076141279016\n",
      "Epoch 1732/3000, Training Loss: 0.0006467698198245411, Validation Loss: 0.1399271893341809\n",
      "Epoch 1733/3000, Training Loss: 0.0006460380725841808, Validation Loss: 0.13994666917980156\n",
      "Epoch 1734/3000, Training Loss: 0.0006453076498429807, Validation Loss: 0.13996582681737293\n",
      "Epoch 1735/3000, Training Loss: 0.0006445788118539458, Validation Loss: 0.13998506346296616\n",
      "Epoch 1736/3000, Training Loss: 0.000643851745819132, Validation Loss: 0.1400043039620817\n",
      "Epoch 1737/3000, Training Loss: 0.0006431258783524161, Validation Loss: 0.14002345295984386\n",
      "Epoch 1738/3000, Training Loss: 0.0006424015689600575, Validation Loss: 0.14004265938306318\n",
      "Epoch 1739/3000, Training Loss: 0.0006416786499224818, Validation Loss: 0.1400619201319509\n",
      "Epoch 1740/3000, Training Loss: 0.0006409574248866835, Validation Loss: 0.14008110539677227\n",
      "Epoch 1741/3000, Training Loss: 0.0006402374636149217, Validation Loss: 0.14010026616791202\n",
      "Epoch 1742/3000, Training Loss: 0.000639518947536502, Validation Loss: 0.14011935752529014\n",
      "Epoch 1743/3000, Training Loss: 0.0006388022136630739, Validation Loss: 0.14013859000931345\n",
      "Epoch 1744/3000, Training Loss: 0.0006380866004765386, Validation Loss: 0.14015765991756549\n",
      "Epoch 1745/3000, Training Loss: 0.000637372498726476, Validation Loss: 0.14017677102221762\n",
      "Epoch 1746/3000, Training Loss: 0.0006366597993925795, Validation Loss: 0.14019582076805825\n",
      "Epoch 1747/3000, Training Loss: 0.0006359487291279905, Validation Loss: 0.14021508371723207\n",
      "Epoch 1748/3000, Training Loss: 0.0006352389865613774, Validation Loss: 0.14023409363905553\n",
      "Epoch 1749/3000, Training Loss: 0.0006345306248475353, Validation Loss: 0.14025313087333946\n",
      "Epoch 1750/3000, Training Loss: 0.0006338238576939825, Validation Loss: 0.14027228985653914\n",
      "Epoch 1751/3000, Training Loss: 0.0006331184004220682, Validation Loss: 0.14029122138788738\n",
      "Epoch 1752/3000, Training Loss: 0.0006324143388707874, Validation Loss: 0.14031023037295423\n",
      "Epoch 1753/3000, Training Loss: 0.0006317117132650741, Validation Loss: 0.14032921482192942\n",
      "Epoch 1754/3000, Training Loss: 0.0006310105574233502, Validation Loss: 0.14034828656086862\n",
      "Epoch 1755/3000, Training Loss: 0.0006303110280424984, Validation Loss: 0.14036710657914378\n",
      "Epoch 1756/3000, Training Loss: 0.000629612457115439, Validation Loss: 0.14038591767345696\n",
      "Epoch 1757/3000, Training Loss: 0.000628915479079446, Validation Loss: 0.14040477377457564\n",
      "Epoch 1758/3000, Training Loss: 0.000628220019240044, Validation Loss: 0.14042371686035618\n",
      "Epoch 1759/3000, Training Loss: 0.0006275257863586748, Validation Loss: 0.14044256836458463\n",
      "Epoch 1760/3000, Training Loss: 0.0006268330197321777, Validation Loss: 0.14046132438263328\n",
      "Epoch 1761/3000, Training Loss: 0.0006261415148069405, Validation Loss: 0.14048024645016427\n",
      "Epoch 1762/3000, Training Loss: 0.0006254518360302175, Validation Loss: 0.14049898763388508\n",
      "Epoch 1763/3000, Training Loss: 0.0006247630157286546, Validation Loss: 0.14051770221888824\n",
      "Epoch 1764/3000, Training Loss: 0.0006240756504076933, Validation Loss: 0.140536493551479\n",
      "Epoch 1765/3000, Training Loss: 0.0006233898828945792, Validation Loss: 0.14055539279395918\n",
      "Epoch 1766/3000, Training Loss: 0.0006227053192273212, Validation Loss: 0.1405739902516372\n",
      "Epoch 1767/3000, Training Loss: 0.0006220221874148149, Validation Loss: 0.14059251124205388\n",
      "Epoch 1768/3000, Training Loss: 0.0006213403165174352, Validation Loss: 0.14061096116306268\n",
      "Epoch 1769/3000, Training Loss: 0.0006206599701803476, Validation Loss: 0.1406296839517054\n",
      "Epoch 1770/3000, Training Loss: 0.0006199808793162978, Validation Loss: 0.14064818083288516\n",
      "Epoch 1771/3000, Training Loss: 0.0006193030069331601, Validation Loss: 0.14066672023789448\n",
      "Epoch 1772/3000, Training Loss: 0.0006186266049666954, Validation Loss: 0.14068534937905483\n",
      "Epoch 1773/3000, Training Loss: 0.0006179516388660862, Validation Loss: 0.14070378810593476\n",
      "Epoch 1774/3000, Training Loss: 0.0006172780588245412, Validation Loss: 0.1407223179409517\n",
      "Epoch 1775/3000, Training Loss: 0.0006166056521280606, Validation Loss: 0.14074078924104796\n",
      "Epoch 1776/3000, Training Loss: 0.0006159346620511323, Validation Loss: 0.140759450363039\n",
      "Epoch 1777/3000, Training Loss: 0.0006152651613570261, Validation Loss: 0.14077795737361629\n",
      "Epoch 1778/3000, Training Loss: 0.0006145966241582205, Validation Loss: 0.14079646934629805\n",
      "Epoch 1779/3000, Training Loss: 0.0006139296571204131, Validation Loss: 0.14081489098760114\n",
      "Epoch 1780/3000, Training Loss: 0.0006132639680625002, Validation Loss: 0.1408334181451817\n",
      "Epoch 1781/3000, Training Loss: 0.0006125995836832891, Validation Loss: 0.14085190316118146\n",
      "Epoch 1782/3000, Training Loss: 0.000611936447399229, Validation Loss: 0.14087029506109788\n",
      "Epoch 1783/3000, Training Loss: 0.0006112744829800781, Validation Loss: 0.14088876299968325\n",
      "Epoch 1784/3000, Training Loss: 0.0006106142437235755, Validation Loss: 0.14090728478336878\n",
      "Epoch 1785/3000, Training Loss: 0.0006099548429006412, Validation Loss: 0.1409256201746767\n",
      "Epoch 1786/3000, Training Loss: 0.0006092969472424485, Validation Loss: 0.14094406231658177\n",
      "Epoch 1787/3000, Training Loss: 0.0006086403260146956, Validation Loss: 0.14096255830515708\n",
      "Epoch 1788/3000, Training Loss: 0.0006079849901768237, Validation Loss: 0.14098098251275426\n",
      "Epoch 1789/3000, Training Loss: 0.0006073309208622, Validation Loss: 0.14099938372565046\n",
      "Epoch 1790/3000, Training Loss: 0.0006066779347760142, Validation Loss: 0.14101767233953313\n",
      "Epoch 1791/3000, Training Loss: 0.0006060266763672612, Validation Loss: 0.1410361696055061\n",
      "Epoch 1792/3000, Training Loss: 0.0006053764105065681, Validation Loss: 0.14105449039933624\n",
      "Epoch 1793/3000, Training Loss: 0.0006047273479919522, Validation Loss: 0.14107285178819248\n",
      "Epoch 1794/3000, Training Loss: 0.0006040796218454397, Validation Loss: 0.14109115504176675\n",
      "Epoch 1795/3000, Training Loss: 0.0006034332419875178, Validation Loss: 0.14110977797374438\n",
      "Epoch 1796/3000, Training Loss: 0.0006027881974640737, Validation Loss: 0.1411281954408613\n",
      "Epoch 1797/3000, Training Loss: 0.0006021441332717544, Validation Loss: 0.14114660313486796\n",
      "Epoch 1798/3000, Training Loss: 0.0006015017108464009, Validation Loss: 0.14116514786238576\n",
      "Epoch 1799/3000, Training Loss: 0.0006008603740068908, Validation Loss: 0.1411835356256204\n",
      "Epoch 1800/3000, Training Loss: 0.0006002200595678112, Validation Loss: 0.14120197940515264\n",
      "Epoch 1801/3000, Training Loss: 0.0005995812153176813, Validation Loss: 0.1412203822694099\n",
      "Epoch 1802/3000, Training Loss: 0.0005989435228440728, Validation Loss: 0.14123888436989115\n",
      "Epoch 1803/3000, Training Loss: 0.0005983072076053809, Validation Loss: 0.1412572497642368\n",
      "Epoch 1804/3000, Training Loss: 0.0005976719429325458, Validation Loss: 0.14127557002880542\n",
      "Epoch 1805/3000, Training Loss: 0.0005970377874673962, Validation Loss: 0.14129396588716\n",
      "Epoch 1806/3000, Training Loss: 0.000596405282852385, Validation Loss: 0.14131241044285844\n",
      "Epoch 1807/3000, Training Loss: 0.0005957735384930753, Validation Loss: 0.14133071733959804\n",
      "Epoch 1808/3000, Training Loss: 0.0005951431929209818, Validation Loss: 0.1413490886026473\n",
      "Epoch 1809/3000, Training Loss: 0.000594514009609854, Validation Loss: 0.14136751139426182\n",
      "Epoch 1810/3000, Training Loss: 0.0005938862436522205, Validation Loss: 0.14138581491730223\n",
      "Epoch 1811/3000, Training Loss: 0.0005932594621521563, Validation Loss: 0.14140409011915453\n",
      "Epoch 1812/3000, Training Loss: 0.0005926337797425492, Validation Loss: 0.1414225953568751\n",
      "Epoch 1813/3000, Training Loss: 0.0005920096848097924, Validation Loss: 0.14144131407513946\n",
      "Epoch 1814/3000, Training Loss: 0.0005913865468149435, Validation Loss: 0.1414598629451348\n",
      "Epoch 1815/3000, Training Loss: 0.0005907646427771162, Validation Loss: 0.14147834946049953\n",
      "Epoch 1816/3000, Training Loss: 0.0005901439685828673, Validation Loss: 0.14149696285330457\n",
      "Epoch 1817/3000, Training Loss: 0.0005895244610359785, Validation Loss: 0.14151542349959664\n",
      "Epoch 1818/3000, Training Loss: 0.000588906168773197, Validation Loss: 0.1415339174486834\n",
      "Epoch 1819/3000, Training Loss: 0.0005882888301413109, Validation Loss: 0.14155235416211756\n",
      "Epoch 1820/3000, Training Loss: 0.0005876729245601561, Validation Loss: 0.141570967078803\n",
      "Epoch 1821/3000, Training Loss: 0.0005870581152383938, Validation Loss: 0.14158934696144498\n",
      "Epoch 1822/3000, Training Loss: 0.0005864444353061632, Validation Loss: 0.14160773607493604\n",
      "Epoch 1823/3000, Training Loss: 0.0005858320001858234, Validation Loss: 0.14162611532341104\n",
      "Epoch 1824/3000, Training Loss: 0.0005852206647060827, Validation Loss: 0.1416446055870315\n",
      "Epoch 1825/3000, Training Loss: 0.0005846105739473257, Validation Loss: 0.14166302530128963\n",
      "Epoch 1826/3000, Training Loss: 0.0005840014480181933, Validation Loss: 0.14168136604405307\n",
      "Epoch 1827/3000, Training Loss: 0.000583393575415611, Validation Loss: 0.14169993805509473\n",
      "Epoch 1828/3000, Training Loss: 0.0005827870395409227, Validation Loss: 0.14171820788143583\n",
      "Epoch 1829/3000, Training Loss: 0.0005821813928885999, Validation Loss: 0.14173647549074125\n",
      "Epoch 1830/3000, Training Loss: 0.0005815770120650852, Validation Loss: 0.14175478176109693\n",
      "Epoch 1831/3000, Training Loss: 0.0005809737469613108, Validation Loss: 0.14177317061175618\n",
      "Epoch 1832/3000, Training Loss: 0.0005803717570617617, Validation Loss: 0.14179147255588004\n",
      "Epoch 1833/3000, Training Loss: 0.000579770788074019, Validation Loss: 0.14180967881152454\n",
      "Epoch 1834/3000, Training Loss: 0.0005791709346495235, Validation Loss: 0.14182790698695819\n",
      "Epoch 1835/3000, Training Loss: 0.0005785725411681912, Validation Loss: 0.14184623013571732\n",
      "Epoch 1836/3000, Training Loss: 0.0005779748656823154, Validation Loss: 0.14186441455339624\n",
      "Epoch 1837/3000, Training Loss: 0.0005773784039385638, Validation Loss: 0.1418826541780411\n",
      "Epoch 1838/3000, Training Loss: 0.000576783107546869, Validation Loss: 0.14190095858503246\n",
      "Epoch 1839/3000, Training Loss: 0.0005761889892662381, Validation Loss: 0.1419191932326438\n",
      "Epoch 1840/3000, Training Loss: 0.0005755959275733065, Validation Loss: 0.14193740010933553\n",
      "Epoch 1841/3000, Training Loss: 0.0005750039311347647, Validation Loss: 0.14195554892198936\n",
      "Epoch 1842/3000, Training Loss: 0.0005744132500220537, Validation Loss: 0.14197382559853064\n",
      "Epoch 1843/3000, Training Loss: 0.0005738234059778306, Validation Loss: 0.14199193082170203\n",
      "Epoch 1844/3000, Training Loss: 0.0005732346847155817, Validation Loss: 0.14201011598332183\n",
      "Epoch 1845/3000, Training Loss: 0.0005726470988200378, Validation Loss: 0.1420282072162227\n",
      "Epoch 1846/3000, Training Loss: 0.0005720607437396112, Validation Loss: 0.14204643212286766\n",
      "Epoch 1847/3000, Training Loss: 0.0005714755092730854, Validation Loss: 0.1420644028291291\n",
      "Epoch 1848/3000, Training Loss: 0.0005708910876947595, Validation Loss: 0.14208220947206532\n",
      "Epoch 1849/3000, Training Loss: 0.000570307941110461, Validation Loss: 0.14210022549214735\n",
      "Epoch 1850/3000, Training Loss: 0.0005697259664995309, Validation Loss: 0.14211802583004696\n",
      "Epoch 1851/3000, Training Loss: 0.0005691448392850055, Validation Loss: 0.14213586597626796\n",
      "Epoch 1852/3000, Training Loss: 0.0005685649433535055, Validation Loss: 0.14215364698154442\n",
      "Epoch 1853/3000, Training Loss: 0.0005679861939932295, Validation Loss: 0.14217154417788833\n",
      "Epoch 1854/3000, Training Loss: 0.0005674084533677989, Validation Loss: 0.14218936645362384\n",
      "Epoch 1855/3000, Training Loss: 0.0005668316767317964, Validation Loss: 0.14220711339794534\n",
      "Epoch 1856/3000, Training Loss: 0.0005662559621141401, Validation Loss: 0.14222503888541288\n",
      "Epoch 1857/3000, Training Loss: 0.0005656815600005722, Validation Loss: 0.14224276601375418\n",
      "Epoch 1858/3000, Training Loss: 0.0005651078869913708, Validation Loss: 0.14226044163571863\n",
      "Epoch 1859/3000, Training Loss: 0.0005645354563371949, Validation Loss: 0.14227819665418373\n",
      "Epoch 1860/3000, Training Loss: 0.0005639640581139442, Validation Loss: 0.14229603662827264\n",
      "Epoch 1861/3000, Training Loss: 0.0005633936865624728, Validation Loss: 0.14231379049957502\n",
      "Epoch 1862/3000, Training Loss: 0.0005628243651607267, Validation Loss: 0.14233145334992336\n",
      "Epoch 1863/3000, Training Loss: 0.0005622559119096787, Validation Loss: 0.14234917164158203\n",
      "Epoch 1864/3000, Training Loss: 0.0005616889060460086, Validation Loss: 0.14236696947015975\n",
      "Epoch 1865/3000, Training Loss: 0.0005611226517191188, Validation Loss: 0.14238457748163524\n",
      "Epoch 1866/3000, Training Loss: 0.0005605574042152862, Validation Loss: 0.14240226400489\n",
      "Epoch 1867/3000, Training Loss: 0.0005599933271677078, Validation Loss: 0.14242008428807484\n",
      "Epoch 1868/3000, Training Loss: 0.0005594302244688536, Validation Loss: 0.14243776904799801\n",
      "Epoch 1869/3000, Training Loss: 0.0005588681754772405, Validation Loss: 0.1424553630491662\n",
      "Epoch 1870/3000, Training Loss: 0.0005583069903151484, Validation Loss: 0.14247292255871158\n",
      "Epoch 1871/3000, Training Loss: 0.0005577470959771036, Validation Loss: 0.14249069739082595\n",
      "Epoch 1872/3000, Training Loss: 0.0005571881116336611, Validation Loss: 0.14250812105054483\n",
      "Epoch 1873/3000, Training Loss: 0.0005566299752677276, Validation Loss: 0.14252559558945554\n",
      "Epoch 1874/3000, Training Loss: 0.0005560730396406804, Validation Loss: 0.14254312299539995\n",
      "Epoch 1875/3000, Training Loss: 0.0005555170851277359, Validation Loss: 0.14256052070054068\n",
      "Epoch 1876/3000, Training Loss: 0.0005549622188772557, Validation Loss: 0.1425779248065324\n",
      "Epoch 1877/3000, Training Loss: 0.0005544081484266954, Validation Loss: 0.14259529813859445\n",
      "Epoch 1878/3000, Training Loss: 0.0005538551680670108, Validation Loss: 0.14261287449411741\n",
      "Epoch 1879/3000, Training Loss: 0.0005533033243525469, Validation Loss: 0.14263024136629274\n",
      "Epoch 1880/3000, Training Loss: 0.0005527522384095603, Validation Loss: 0.14264765308055483\n",
      "Epoch 1881/3000, Training Loss: 0.0005522023914965744, Validation Loss: 0.14266496443494367\n",
      "Epoch 1882/3000, Training Loss: 0.0005516535951579461, Validation Loss: 0.14268237304746267\n",
      "Epoch 1883/3000, Training Loss: 0.0005511055969125817, Validation Loss: 0.1426997111117734\n",
      "Epoch 1884/3000, Training Loss: 0.0005505586456403762, Validation Loss: 0.14271696644909115\n",
      "Epoch 1885/3000, Training Loss: 0.0005500125944185804, Validation Loss: 0.14273442633869163\n",
      "Epoch 1886/3000, Training Loss: 0.0005494678321143804, Validation Loss: 0.1427516776209587\n",
      "Epoch 1887/3000, Training Loss: 0.0005489236823225716, Validation Loss: 0.1427687727276181\n",
      "Epoch 1888/3000, Training Loss: 0.0005483807725449596, Validation Loss: 0.14278588088857366\n",
      "Epoch 1889/3000, Training Loss: 0.0005478387995869914, Validation Loss: 0.14280311531709658\n",
      "Epoch 1890/3000, Training Loss: 0.0005472977050715934, Validation Loss: 0.14282024065239898\n",
      "Epoch 1891/3000, Training Loss: 0.00054675767913466, Validation Loss: 0.14283729491422995\n",
      "Epoch 1892/3000, Training Loss: 0.0005462184544613184, Validation Loss: 0.14285446413641792\n",
      "Epoch 1893/3000, Training Loss: 0.0005456805598289258, Validation Loss: 0.1428715208667778\n",
      "Epoch 1894/3000, Training Loss: 0.0005451433514749926, Validation Loss: 0.14288860348822827\n",
      "Epoch 1895/3000, Training Loss: 0.0005446070504946829, Validation Loss: 0.1429056728186745\n",
      "Epoch 1896/3000, Training Loss: 0.0005440719120196111, Validation Loss: 0.1429228271302363\n",
      "Epoch 1897/3000, Training Loss: 0.0005435375742304141, Validation Loss: 0.14293983868606633\n",
      "Epoch 1898/3000, Training Loss: 0.0005430042781307397, Validation Loss: 0.14295688502272957\n",
      "Epoch 1899/3000, Training Loss: 0.0005424717554136253, Validation Loss: 0.14297388176480474\n",
      "Epoch 1900/3000, Training Loss: 0.0005419405449969536, Validation Loss: 0.14299100435085751\n",
      "Epoch 1901/3000, Training Loss: 0.0005414100184154016, Validation Loss: 0.1430079860512724\n",
      "Epoch 1902/3000, Training Loss: 0.0005408803002584082, Validation Loss: 0.1430249439391936\n",
      "Epoch 1903/3000, Training Loss: 0.0005403517820476341, Validation Loss: 0.14304210171386061\n",
      "Epoch 1904/3000, Training Loss: 0.0005398241203736155, Validation Loss: 0.14305908511629986\n",
      "Epoch 1905/3000, Training Loss: 0.0005392973933732416, Validation Loss: 0.14307605965904482\n",
      "Epoch 1906/3000, Training Loss: 0.0005387715935112654, Validation Loss: 0.14309302811019045\n",
      "Epoch 1907/3000, Training Loss: 0.0005382466952280066, Validation Loss: 0.1431101068654073\n",
      "Epoch 1908/3000, Training Loss: 0.0005377229037413125, Validation Loss: 0.14312716930369784\n",
      "Epoch 1909/3000, Training Loss: 0.0005371997854230942, Validation Loss: 0.1431441145557244\n",
      "Epoch 1910/3000, Training Loss: 0.0005366777493376417, Validation Loss: 0.14316122688647404\n",
      "Epoch 1911/3000, Training Loss: 0.0005361567186387267, Validation Loss: 0.14317812528973195\n",
      "Epoch 1912/3000, Training Loss: 0.0005356365298833307, Validation Loss: 0.1431950334846662\n",
      "Epoch 1913/3000, Training Loss: 0.0005351172343163425, Validation Loss: 0.1432118991763671\n",
      "Epoch 1914/3000, Training Loss: 0.0005345988012528222, Validation Loss: 0.14322890824383858\n",
      "Epoch 1915/3000, Training Loss: 0.0005340814543844301, Validation Loss: 0.14324583316070097\n",
      "Epoch 1916/3000, Training Loss: 0.0005335648183451941, Validation Loss: 0.14326267773074086\n",
      "Epoch 1917/3000, Training Loss: 0.000533049157522666, Validation Loss: 0.14327954751375993\n",
      "Epoch 1918/3000, Training Loss: 0.0005325346184246278, Validation Loss: 0.14329650715782444\n",
      "Epoch 1919/3000, Training Loss: 0.0005320206560283735, Validation Loss: 0.1433133441887642\n",
      "Epoch 1920/3000, Training Loss: 0.000531507739189491, Validation Loss: 0.14333030098816005\n",
      "Epoch 1921/3000, Training Loss: 0.0005309957056722246, Validation Loss: 0.14334730947854463\n",
      "Epoch 1922/3000, Training Loss: 0.000530484676727275, Validation Loss: 0.1433642488128853\n",
      "Epoch 1923/3000, Training Loss: 0.0005299744155266705, Validation Loss: 0.14338115450916528\n",
      "Epoch 1924/3000, Training Loss: 0.000529465109213665, Validation Loss: 0.14339796838318528\n",
      "Epoch 1925/3000, Training Loss: 0.000528956810767261, Validation Loss: 0.14341487646925266\n",
      "Epoch 1926/3000, Training Loss: 0.0005284491990856596, Validation Loss: 0.14343166013930495\n",
      "Epoch 1927/3000, Training Loss: 0.000527942502555128, Validation Loss: 0.14344849236762147\n",
      "Epoch 1928/3000, Training Loss: 0.0005274366900663366, Validation Loss: 0.1434653762594021\n",
      "Epoch 1929/3000, Training Loss: 0.0005269319269194757, Validation Loss: 0.14348215013666388\n",
      "Epoch 1930/3000, Training Loss: 0.0005264279387764104, Validation Loss: 0.1434989020880253\n",
      "Epoch 1931/3000, Training Loss: 0.0005259246548018809, Validation Loss: 0.14351562833505346\n",
      "Epoch 1932/3000, Training Loss: 0.000525422456615715, Validation Loss: 0.14353255448452373\n",
      "Epoch 1933/3000, Training Loss: 0.0005249210785087673, Validation Loss: 0.14354927945597126\n",
      "Epoch 1934/3000, Training Loss: 0.0005244204816533432, Validation Loss: 0.14356600093112953\n",
      "Epoch 1935/3000, Training Loss: 0.0005239208893276012, Validation Loss: 0.1435827105145487\n",
      "Epoch 1936/3000, Training Loss: 0.0005234221304114068, Validation Loss: 0.14359953008286752\n",
      "Epoch 1937/3000, Training Loss: 0.0005229241933420438, Validation Loss: 0.1436162842939407\n",
      "Epoch 1938/3000, Training Loss: 0.0005224270344385527, Validation Loss: 0.1436330155090364\n",
      "Epoch 1939/3000, Training Loss: 0.0005219308494110043, Validation Loss: 0.1436498681599729\n",
      "Epoch 1940/3000, Training Loss: 0.0005214355698983305, Validation Loss: 0.14366653623790757\n",
      "Epoch 1941/3000, Training Loss: 0.0005209409881054793, Validation Loss: 0.14368320057643338\n",
      "Epoch 1942/3000, Training Loss: 0.0005204474182835002, Validation Loss: 0.1436996739386762\n",
      "Epoch 1943/3000, Training Loss: 0.0005199546233195769, Validation Loss: 0.14371627635911702\n",
      "Epoch 1944/3000, Training Loss: 0.0005194626976793988, Validation Loss: 0.14373281199117666\n",
      "Epoch 1945/3000, Training Loss: 0.0005189715710786887, Validation Loss: 0.14374928487252417\n",
      "Epoch 1946/3000, Training Loss: 0.000518481267027632, Validation Loss: 0.14376589047651864\n",
      "Epoch 1947/3000, Training Loss: 0.0005179921230754086, Validation Loss: 0.1437823547816607\n",
      "Epoch 1948/3000, Training Loss: 0.0005175034184662083, Validation Loss: 0.14379883126426868\n",
      "Epoch 1949/3000, Training Loss: 0.0005170156553214817, Validation Loss: 0.14381534099011686\n",
      "Epoch 1950/3000, Training Loss: 0.0005165288393407541, Validation Loss: 0.14383193249222948\n",
      "Epoch 1951/3000, Training Loss: 0.0005160427615313937, Validation Loss: 0.14384844219070195\n",
      "Epoch 1952/3000, Training Loss: 0.0005155575667758457, Validation Loss: 0.14386487424103167\n",
      "Epoch 1953/3000, Training Loss: 0.0005150731120170342, Validation Loss: 0.14388140242005792\n",
      "Epoch 1954/3000, Training Loss: 0.000514589706332988, Validation Loss: 0.14389786612051822\n",
      "Epoch 1955/3000, Training Loss: 0.0005141069547448206, Validation Loss: 0.1439142069748394\n",
      "Epoch 1956/3000, Training Loss: 0.0005136249510054432, Validation Loss: 0.14393061106147811\n",
      "Epoch 1957/3000, Training Loss: 0.0005131439776667856, Validation Loss: 0.14394706703512067\n",
      "Epoch 1958/3000, Training Loss: 0.0005126637008797571, Validation Loss: 0.1439633580555965\n",
      "Epoch 1959/3000, Training Loss: 0.0005121842602446585, Validation Loss: 0.14397972816882665\n",
      "Epoch 1960/3000, Training Loss: 0.0005117055937275474, Validation Loss: 0.14399597348921425\n",
      "Epoch 1961/3000, Training Loss: 0.0005112278339538448, Validation Loss: 0.14401237613750564\n",
      "Epoch 1962/3000, Training Loss: 0.0005107508899964104, Validation Loss: 0.14402860333465534\n",
      "Epoch 1963/3000, Training Loss: 0.0005102745579213775, Validation Loss: 0.14404482201610802\n",
      "Epoch 1964/3000, Training Loss: 0.0005097993506960512, Validation Loss: 0.14406116638880478\n",
      "Epoch 1965/3000, Training Loss: 0.0005093247880727309, Validation Loss: 0.14407737180298996\n",
      "Epoch 1966/3000, Training Loss: 0.0005088509227181012, Validation Loss: 0.14409363052962235\n",
      "Epoch 1967/3000, Training Loss: 0.0005083779493689638, Validation Loss: 0.14410981079218413\n",
      "Epoch 1968/3000, Training Loss: 0.0005079057635560093, Validation Loss: 0.1441261247819903\n",
      "Epoch 1969/3000, Training Loss: 0.000507434529596079, Validation Loss: 0.14414232296799492\n",
      "Epoch 1970/3000, Training Loss: 0.0005069638806570356, Validation Loss: 0.14415865002388661\n",
      "Epoch 1971/3000, Training Loss: 0.0005064940787344702, Validation Loss: 0.14417512649656847\n",
      "Epoch 1972/3000, Training Loss: 0.0005060252490411997, Validation Loss: 0.1441914081278555\n",
      "Epoch 1973/3000, Training Loss: 0.0005055569537022347, Validation Loss: 0.14420772669925133\n",
      "Epoch 1974/3000, Training Loss: 0.000505089602406727, Validation Loss: 0.1442240048662428\n",
      "Epoch 1975/3000, Training Loss: 0.0005046230057610832, Validation Loss: 0.14424034541738776\n",
      "Epoch 1976/3000, Training Loss: 0.0005041572559716678, Validation Loss: 0.1442566652210204\n",
      "Epoch 1977/3000, Training Loss: 0.0005036921969644798, Validation Loss: 0.1442729038801261\n",
      "Epoch 1978/3000, Training Loss: 0.000503227839212927, Validation Loss: 0.1442892090188132\n",
      "Epoch 1979/3000, Training Loss: 0.0005027645600961663, Validation Loss: 0.14430555935305267\n",
      "Epoch 1980/3000, Training Loss: 0.0005023017411727495, Validation Loss: 0.14432179260754527\n",
      "Epoch 1981/3000, Training Loss: 0.0005018398831347663, Validation Loss: 0.1443380347252212\n",
      "Epoch 1982/3000, Training Loss: 0.0005013787267271559, Validation Loss: 0.14435436252476352\n",
      "Epoch 1983/3000, Training Loss: 0.000500918339729269, Validation Loss: 0.14437062757803476\n",
      "Epoch 1984/3000, Training Loss: 0.0005004587363239165, Validation Loss: 0.1443868247029183\n",
      "Epoch 1985/3000, Training Loss: 0.0004999997680218737, Validation Loss: 0.14440299778131005\n",
      "Epoch 1986/3000, Training Loss: 0.00049954188787674, Validation Loss: 0.14441936231816546\n",
      "Epoch 1987/3000, Training Loss: 0.0004990845575149702, Validation Loss: 0.14443553463117909\n",
      "Epoch 1988/3000, Training Loss: 0.0004986279280503181, Validation Loss: 0.14445174064960395\n",
      "Epoch 1989/3000, Training Loss: 0.0004981721863227363, Validation Loss: 0.14446800672742366\n",
      "Epoch 1990/3000, Training Loss: 0.0004977171562696533, Validation Loss: 0.14448415618615934\n",
      "Epoch 1991/3000, Training Loss: 0.0004972629042900888, Validation Loss: 0.14450035138456827\n",
      "Epoch 1992/3000, Training Loss: 0.0004968092593630779, Validation Loss: 0.1445164316379751\n",
      "Epoch 1993/3000, Training Loss: 0.0004963566023876704, Validation Loss: 0.144532744520182\n",
      "Epoch 1994/3000, Training Loss: 0.0004959046616783548, Validation Loss: 0.14454887014002663\n",
      "Epoch 1995/3000, Training Loss: 0.0004954532924898853, Validation Loss: 0.14456497155521317\n",
      "Epoch 1996/3000, Training Loss: 0.0004950028656274852, Validation Loss: 0.14458126320804238\n",
      "Epoch 1997/3000, Training Loss: 0.0004945531238783824, Validation Loss: 0.14459736591278766\n",
      "Epoch 1998/3000, Training Loss: 0.0004941041737680038, Validation Loss: 0.14461346018173996\n",
      "Epoch 1999/3000, Training Loss: 0.0004936558475164376, Validation Loss: 0.14462954805071096\n",
      "Epoch 2000/3000, Training Loss: 0.0004932083169544338, Validation Loss: 0.1446457875704703\n",
      "Epoch 2001/3000, Training Loss: 0.0004927616294069619, Validation Loss: 0.14466185549035157\n",
      "Epoch 2002/3000, Training Loss: 0.000492315445846779, Validation Loss: 0.14467789898174013\n",
      "Epoch 2003/3000, Training Loss: 0.0004918701744780964, Validation Loss: 0.14469410066710212\n",
      "Epoch 2004/3000, Training Loss: 0.0004914257850245617, Validation Loss: 0.1447103444117618\n",
      "Epoch 2005/3000, Training Loss: 0.0004909818823240991, Validation Loss: 0.14472652216216475\n",
      "Epoch 2006/3000, Training Loss: 0.0004905388289239493, Validation Loss: 0.14474263897274509\n",
      "Epoch 2007/3000, Training Loss: 0.0004900964808556852, Validation Loss: 0.14475912296398427\n",
      "Epoch 2008/3000, Training Loss: 0.0004896550682055597, Validation Loss: 0.1447755439217349\n",
      "Epoch 2009/3000, Training Loss: 0.0004892141844554146, Validation Loss: 0.14479189754240102\n",
      "Epoch 2010/3000, Training Loss: 0.0004887741458707266, Validation Loss: 0.1448083692558442\n",
      "Epoch 2011/3000, Training Loss: 0.0004883349420768449, Validation Loss: 0.14482470497079789\n",
      "Epoch 2012/3000, Training Loss: 0.0004878961812111514, Validation Loss: 0.14484101429248758\n",
      "Epoch 2013/3000, Training Loss: 0.0004874582556242908, Validation Loss: 0.14485738549356014\n",
      "Epoch 2014/3000, Training Loss: 0.00048702107867931363, Validation Loss: 0.14487379853173754\n",
      "Epoch 2015/3000, Training Loss: 0.00048658463722335994, Validation Loss: 0.1448901087652031\n",
      "Epoch 2016/3000, Training Loss: 0.0004861489038526249, Validation Loss: 0.14490639329430424\n",
      "Epoch 2017/3000, Training Loss: 0.00048571373034243885, Validation Loss: 0.14492277500744727\n",
      "Epoch 2018/3000, Training Loss: 0.0004852794856450557, Validation Loss: 0.14493909428259635\n",
      "Epoch 2019/3000, Training Loss: 0.00048484580119139266, Validation Loss: 0.14495538913840245\n",
      "Epoch 2020/3000, Training Loss: 0.0004844127641997666, Validation Loss: 0.14497167367731748\n",
      "Epoch 2021/3000, Training Loss: 0.0004839806516895549, Validation Loss: 0.14498799014036995\n",
      "Epoch 2022/3000, Training Loss: 0.00048354912939388797, Validation Loss: 0.1450042145142215\n",
      "Epoch 2023/3000, Training Loss: 0.00048311822023160535, Validation Loss: 0.14502046949916578\n",
      "Epoch 2024/3000, Training Loss: 0.0004826880869287457, Validation Loss: 0.14503678071434217\n",
      "Epoch 2025/3000, Training Loss: 0.0004822586847492953, Validation Loss: 0.14505297539974432\n",
      "Epoch 2026/3000, Training Loss: 0.00048182997939466956, Validation Loss: 0.14506917394366095\n",
      "Epoch 2027/3000, Training Loss: 0.00048140185842560464, Validation Loss: 0.1450853354032493\n",
      "Epoch 2028/3000, Training Loss: 0.00048097447697541804, Validation Loss: 0.1451016757349157\n",
      "Epoch 2029/3000, Training Loss: 0.0004805478974002973, Validation Loss: 0.1451178301821714\n",
      "Epoch 2030/3000, Training Loss: 0.00048012179500990835, Validation Loss: 0.145133960885553\n",
      "Epoch 2031/3000, Training Loss: 0.0004796965050403682, Validation Loss: 0.14515023178133052\n",
      "Epoch 2032/3000, Training Loss: 0.0004792720291612114, Validation Loss: 0.14516635711730783\n",
      "Epoch 2033/3000, Training Loss: 0.0004788479888667292, Validation Loss: 0.14518244718496687\n",
      "Epoch 2034/3000, Training Loss: 0.0004784247564229308, Validation Loss: 0.14519857967104594\n",
      "Epoch 2035/3000, Training Loss: 0.00047800220053591805, Validation Loss: 0.14521478489734113\n",
      "Epoch 2036/3000, Training Loss: 0.0004775803494486736, Validation Loss: 0.14523095185545004\n",
      "Epoch 2037/3000, Training Loss: 0.0004771591110968011, Validation Loss: 0.1452469728456533\n",
      "Epoch 2038/3000, Training Loss: 0.0004767385768864655, Validation Loss: 0.14526313401593302\n",
      "Epoch 2039/3000, Training Loss: 0.00047631890580667976, Validation Loss: 0.14527915024737995\n",
      "Epoch 2040/3000, Training Loss: 0.0004758996192019258, Validation Loss: 0.14529514100649688\n",
      "Epoch 2041/3000, Training Loss: 0.00047548105117602255, Validation Loss: 0.14531119278495758\n",
      "Epoch 2042/3000, Training Loss: 0.00047506333258371964, Validation Loss: 0.14532728848093193\n",
      "Epoch 2043/3000, Training Loss: 0.0004746461151909888, Validation Loss: 0.14534322714762174\n",
      "Epoch 2044/3000, Training Loss: 0.00047422962564069046, Validation Loss: 0.14535925083059123\n",
      "Epoch 2045/3000, Training Loss: 0.0004738138090058351, Validation Loss: 0.1453753160908398\n",
      "Epoch 2046/3000, Training Loss: 0.0004733986503456065, Validation Loss: 0.1453913205728641\n",
      "Epoch 2047/3000, Training Loss: 0.0004729841557714942, Validation Loss: 0.1454072440719345\n",
      "Epoch 2048/3000, Training Loss: 0.00047257020193060197, Validation Loss: 0.14542317483829184\n",
      "Epoch 2049/3000, Training Loss: 0.00047215719738311323, Validation Loss: 0.14543921882938812\n",
      "Epoch 2050/3000, Training Loss: 0.000471744645635522, Validation Loss: 0.1454551287078255\n",
      "Epoch 2051/3000, Training Loss: 0.00047133264576734686, Validation Loss: 0.14547103175803652\n",
      "Epoch 2052/3000, Training Loss: 0.0004709215163091438, Validation Loss: 0.14548708318410683\n",
      "Epoch 2053/3000, Training Loss: 0.00047051095990822695, Validation Loss: 0.14550300912969036\n",
      "Epoch 2054/3000, Training Loss: 0.00047010103671246174, Validation Loss: 0.145518899128154\n",
      "Epoch 2055/3000, Training Loss: 0.0004696917715553599, Validation Loss: 0.14553474853073736\n",
      "Epoch 2056/3000, Training Loss: 0.00046928315794319994, Validation Loss: 0.14555072551965845\n",
      "Epoch 2057/3000, Training Loss: 0.0004688752169672152, Validation Loss: 0.14556660914319156\n",
      "Epoch 2058/3000, Training Loss: 0.00046846781685805177, Validation Loss: 0.1455824470613836\n",
      "Epoch 2059/3000, Training Loss: 0.0004680611343127656, Validation Loss: 0.14559843257966343\n",
      "Epoch 2060/3000, Training Loss: 0.0004676551860570274, Validation Loss: 0.14561421406390482\n",
      "Epoch 2061/3000, Training Loss: 0.0004672497232091412, Validation Loss: 0.14563002455881738\n",
      "Epoch 2062/3000, Training Loss: 0.0004668449410845077, Validation Loss: 0.14564586202258353\n",
      "Epoch 2063/3000, Training Loss: 0.00046644089711879113, Validation Loss: 0.14566177355459806\n",
      "Epoch 2064/3000, Training Loss: 0.00046603734433560835, Validation Loss: 0.1456776062724927\n",
      "Epoch 2065/3000, Training Loss: 0.00046563450495858637, Validation Loss: 0.14569336532079655\n",
      "Epoch 2066/3000, Training Loss: 0.00046523228244529506, Validation Loss: 0.14570920943266566\n",
      "Epoch 2067/3000, Training Loss: 0.00046483075062419687, Validation Loss: 0.14572500323253512\n",
      "Epoch 2068/3000, Training Loss: 0.00046442977812833025, Validation Loss: 0.14574074900651895\n",
      "Epoch 2069/3000, Training Loss: 0.0004640293370978187, Validation Loss: 0.14575652241737494\n",
      "Epoch 2070/3000, Training Loss: 0.0004636298270989765, Validation Loss: 0.14577241055918835\n",
      "Epoch 2071/3000, Training Loss: 0.000463230674323789, Validation Loss: 0.14578808898110096\n",
      "Epoch 2072/3000, Training Loss: 0.00046283220051620996, Validation Loss: 0.145803832827696\n",
      "Epoch 2073/3000, Training Loss: 0.00046243445720793293, Validation Loss: 0.14581964556408383\n",
      "Epoch 2074/3000, Training Loss: 0.00046203722539249427, Validation Loss: 0.14583529198700207\n",
      "Epoch 2075/3000, Training Loss: 0.0004616406592775153, Validation Loss: 0.14585096840402262\n",
      "Epoch 2076/3000, Training Loss: 0.00046124460448302416, Validation Loss: 0.14586671397574535\n",
      "Epoch 2077/3000, Training Loss: 0.00046084941388142137, Validation Loss: 0.1458823353060525\n",
      "Epoch 2078/3000, Training Loss: 0.00046045466765797953, Validation Loss: 0.14589794458261052\n",
      "Epoch 2079/3000, Training Loss: 0.0004600604312270589, Validation Loss: 0.1459135339328953\n",
      "Epoch 2080/3000, Training Loss: 0.0004596670024936651, Validation Loss: 0.1459292948197167\n",
      "Epoch 2081/3000, Training Loss: 0.00045927411758738825, Validation Loss: 0.14594487838778597\n",
      "Epoch 2082/3000, Training Loss: 0.000458881802131791, Validation Loss: 0.1459604549377453\n",
      "Epoch 2083/3000, Training Loss: 0.00045849017917288186, Validation Loss: 0.14597613725844674\n",
      "Epoch 2084/3000, Training Loss: 0.00045809913864105735, Validation Loss: 0.14599169483237254\n",
      "Epoch 2085/3000, Training Loss: 0.00045770864988741946, Validation Loss: 0.14600727978246558\n",
      "Epoch 2086/3000, Training Loss: 0.000457318763719476, Validation Loss: 0.1460228199879141\n",
      "Epoch 2087/3000, Training Loss: 0.0004569295412896958, Validation Loss: 0.1460385095483129\n",
      "Epoch 2088/3000, Training Loss: 0.0004565410404969799, Validation Loss: 0.14605403203239362\n",
      "Epoch 2089/3000, Training Loss: 0.0004561528821888374, Validation Loss: 0.14606953229995095\n",
      "Epoch 2090/3000, Training Loss: 0.0004557654071121236, Validation Loss: 0.14608520495791455\n",
      "Epoch 2091/3000, Training Loss: 0.00045537868006807807, Validation Loss: 0.14610070112002088\n",
      "Epoch 2092/3000, Training Loss: 0.00045499232483892153, Validation Loss: 0.14611617143766473\n",
      "Epoch 2093/3000, Training Loss: 0.0004546067560609839, Validation Loss: 0.14613166433697225\n",
      "Epoch 2094/3000, Training Loss: 0.0004542217357220705, Validation Loss: 0.14614723501617224\n",
      "Epoch 2095/3000, Training Loss: 0.0004538372321303388, Validation Loss: 0.1461627470842328\n",
      "Epoch 2096/3000, Training Loss: 0.00045345337565194064, Validation Loss: 0.1461781989398611\n",
      "Epoch 2097/3000, Training Loss: 0.0004530700436557738, Validation Loss: 0.14619374346930702\n",
      "Epoch 2098/3000, Training Loss: 0.0004526874812684671, Validation Loss: 0.1462091930753091\n",
      "Epoch 2099/3000, Training Loss: 0.00045230534934804595, Validation Loss: 0.14622460350230154\n",
      "Epoch 2100/3000, Training Loss: 0.00045192374519605847, Validation Loss: 0.14624001717721893\n",
      "Epoch 2101/3000, Training Loss: 0.00045154296177455695, Validation Loss: 0.14625557473350648\n",
      "Epoch 2102/3000, Training Loss: 0.00045116257384079483, Validation Loss: 0.14627096690888144\n",
      "Epoch 2103/3000, Training Loss: 0.0004507827904943221, Validation Loss: 0.14628640179945074\n",
      "Epoch 2104/3000, Training Loss: 0.0004504036445957117, Validation Loss: 0.14630184171283855\n",
      "Epoch 2105/3000, Training Loss: 0.0004500251133601216, Validation Loss: 0.14631720857640662\n",
      "Epoch 2106/3000, Training Loss: 0.0004496471120041169, Validation Loss: 0.14633265576367216\n",
      "Epoch 2107/3000, Training Loss: 0.0004492696205057703, Validation Loss: 0.14634799179678223\n",
      "Epoch 2108/3000, Training Loss: 0.00044889283550936834, Validation Loss: 0.14636349799097434\n",
      "Epoch 2109/3000, Training Loss: 0.00044851656290607005, Validation Loss: 0.14637883001268404\n",
      "Epoch 2110/3000, Training Loss: 0.00044814081769605076, Validation Loss: 0.14639410454961851\n",
      "Epoch 2111/3000, Training Loss: 0.00044776573332389107, Validation Loss: 0.14640957182039194\n",
      "Epoch 2112/3000, Training Loss: 0.00044739123015595696, Validation Loss: 0.1464248789849117\n",
      "Epoch 2113/3000, Training Loss: 0.00044701715523760854, Validation Loss: 0.14644022666989331\n",
      "Epoch 2114/3000, Training Loss: 0.0004466437460922665, Validation Loss: 0.1464555038641994\n",
      "Epoch 2115/3000, Training Loss: 0.00044627095225536177, Validation Loss: 0.1464708637468467\n",
      "Epoch 2116/3000, Training Loss: 0.0004458986667304538, Validation Loss: 0.14648618848690945\n",
      "Epoch 2117/3000, Training Loss: 0.00044552693372829, Validation Loss: 0.14650144065319268\n",
      "Epoch 2118/3000, Training Loss: 0.0004451557425621542, Validation Loss: 0.1465168070350659\n",
      "Epoch 2119/3000, Training Loss: 0.00044478526181371476, Validation Loss: 0.146532102149053\n",
      "Epoch 2120/3000, Training Loss: 0.00044441514910209756, Validation Loss: 0.14654728291870153\n",
      "Epoch 2121/3000, Training Loss: 0.0004440456767956873, Validation Loss: 0.14656265955805245\n",
      "Epoch 2122/3000, Training Loss: 0.0004436769360449209, Validation Loss: 0.14657787062338998\n",
      "Epoch 2123/3000, Training Loss: 0.00044330848648987763, Validation Loss: 0.14659308327083262\n",
      "Epoch 2124/3000, Training Loss: 0.00044294072095120576, Validation Loss: 0.14660836695592985\n",
      "Epoch 2125/3000, Training Loss: 0.00044257352002286015, Validation Loss: 0.14662368254602845\n",
      "Epoch 2126/3000, Training Loss: 0.00044220692161586023, Validation Loss: 0.14663888911795134\n",
      "Epoch 2127/3000, Training Loss: 0.0004418408111527156, Validation Loss: 0.1466540574123875\n",
      "Epoch 2128/3000, Training Loss: 0.00044147516885761456, Validation Loss: 0.14666934382562563\n",
      "Epoch 2129/3000, Training Loss: 0.0004411102722174667, Validation Loss: 0.14668455460038013\n",
      "Epoch 2130/3000, Training Loss: 0.0004407457787333921, Validation Loss: 0.14669969591928114\n",
      "Epoch 2131/3000, Training Loss: 0.00044038182474358966, Validation Loss: 0.14671485503272624\n",
      "Epoch 2132/3000, Training Loss: 0.0004400186201500041, Validation Loss: 0.1467300945396579\n",
      "Epoch 2133/3000, Training Loss: 0.0004396557451134584, Validation Loss: 0.14674522449231758\n",
      "Epoch 2134/3000, Training Loss: 0.00043929343328625106, Validation Loss: 0.14676038096311025\n",
      "Epoch 2135/3000, Training Loss: 0.00043893171593411054, Validation Loss: 0.14677560865636946\n",
      "Epoch 2136/3000, Training Loss: 0.0004385705557987848, Validation Loss: 0.14679072686439243\n",
      "Epoch 2137/3000, Training Loss: 0.00043821000670301886, Validation Loss: 0.14680581126954007\n",
      "Epoch 2138/3000, Training Loss: 0.00043784980781901115, Validation Loss: 0.1468208944548391\n",
      "Epoch 2139/3000, Training Loss: 0.00043749029582743645, Validation Loss: 0.14683612230114954\n",
      "Epoch 2140/3000, Training Loss: 0.0004371313133692739, Validation Loss: 0.14685118882312317\n",
      "Epoch 2141/3000, Training Loss: 0.0004367727236508148, Validation Loss: 0.14686624946271115\n",
      "Epoch 2142/3000, Training Loss: 0.00043641489582646546, Validation Loss: 0.14688141757753959\n",
      "Epoch 2143/3000, Training Loss: 0.000436057507059486, Validation Loss: 0.14689645970103288\n",
      "Epoch 2144/3000, Training Loss: 0.0004357005435306423, Validation Loss: 0.1469115762057645\n",
      "Epoch 2145/3000, Training Loss: 0.0004353442456398077, Validation Loss: 0.14692660676830427\n",
      "Epoch 2146/3000, Training Loss: 0.0004349884542794402, Validation Loss: 0.14694173019688822\n",
      "Epoch 2147/3000, Training Loss: 0.00043463322849118005, Validation Loss: 0.14695675726041085\n",
      "Epoch 2148/3000, Training Loss: 0.0004342784575695249, Validation Loss: 0.14697176399944584\n",
      "Epoch 2149/3000, Training Loss: 0.00043392420135625825, Validation Loss: 0.14698686069383157\n",
      "Epoch 2150/3000, Training Loss: 0.00043357060909735674, Validation Loss: 0.1470018999976984\n",
      "Epoch 2151/3000, Training Loss: 0.00043321735369730117, Validation Loss: 0.14701688207870514\n",
      "Epoch 2152/3000, Training Loss: 0.0004328646911915199, Validation Loss: 0.14703185568400667\n",
      "Epoch 2153/3000, Training Loss: 0.0004325127153528696, Validation Loss: 0.14704693469129929\n",
      "Epoch 2154/3000, Training Loss: 0.0004321610295928795, Validation Loss: 0.1470618772572531\n",
      "Epoch 2155/3000, Training Loss: 0.00043180995127040664, Validation Loss: 0.14707690730307174\n",
      "Epoch 2156/3000, Training Loss: 0.0004314594113454608, Validation Loss: 0.14709197729206797\n",
      "Epoch 2157/3000, Training Loss: 0.00043110939844056884, Validation Loss: 0.14710699031324986\n",
      "Epoch 2158/3000, Training Loss: 0.00043075987471036947, Validation Loss: 0.14712194380545698\n",
      "Epoch 2159/3000, Training Loss: 0.0004304108540603786, Validation Loss: 0.14713695660609427\n",
      "Epoch 2160/3000, Training Loss: 0.0004300624664682165, Validation Loss: 0.14715195187711347\n",
      "Epoch 2161/3000, Training Loss: 0.0004297144685061528, Validation Loss: 0.14716688772976738\n",
      "Epoch 2162/3000, Training Loss: 0.000429366939188979, Validation Loss: 0.1471818533498028\n",
      "Epoch 2163/3000, Training Loss: 0.0004290200984958557, Validation Loss: 0.14719688498517355\n",
      "Epoch 2164/3000, Training Loss: 0.000428673678934849, Validation Loss: 0.14721178932611437\n",
      "Epoch 2165/3000, Training Loss: 0.00042832773091923404, Validation Loss: 0.14722674059469407\n",
      "Epoch 2166/3000, Training Loss: 0.0004279823381651858, Validation Loss: 0.1472417362691448\n",
      "Epoch 2167/3000, Training Loss: 0.00042763746270409344, Validation Loss: 0.14725661804901435\n",
      "Epoch 2168/3000, Training Loss: 0.000427293094025335, Validation Loss: 0.14727150841035497\n",
      "Epoch 2169/3000, Training Loss: 0.0004269491777584584, Validation Loss: 0.14728636345482687\n",
      "Epoch 2170/3000, Training Loss: 0.0004266057981351174, Validation Loss: 0.14730138152380262\n",
      "Epoch 2171/3000, Training Loss: 0.0004262629852082858, Validation Loss: 0.14731623255821671\n",
      "Epoch 2172/3000, Training Loss: 0.0004259205198510239, Validation Loss: 0.1473310606980542\n",
      "Epoch 2173/3000, Training Loss: 0.00042557866312629755, Validation Loss: 0.14734607498140526\n",
      "Epoch 2174/3000, Training Loss: 0.000425237343483175, Validation Loss: 0.1473608714440314\n",
      "Epoch 2175/3000, Training Loss: 0.0004248964413224404, Validation Loss: 0.1473757092202861\n",
      "Epoch 2176/3000, Training Loss: 0.00042455607587174555, Validation Loss: 0.14739057213471046\n",
      "Epoch 2177/3000, Training Loss: 0.0004242162296733076, Validation Loss: 0.14740550462425717\n",
      "Epoch 2178/3000, Training Loss: 0.00042387683878812636, Validation Loss: 0.14742036567025182\n",
      "Epoch 2179/3000, Training Loss: 0.00042353792796753515, Validation Loss: 0.14743515516655484\n",
      "Epoch 2180/3000, Training Loss: 0.0004231995419044011, Validation Loss: 0.1474500771234738\n",
      "Epoch 2181/3000, Training Loss: 0.00042286176073383293, Validation Loss: 0.14746486294817154\n",
      "Epoch 2182/3000, Training Loss: 0.0004225242628995401, Validation Loss: 0.14747963014496096\n",
      "Epoch 2183/3000, Training Loss: 0.00042218727676964456, Validation Loss: 0.14749444796359404\n",
      "Epoch 2184/3000, Training Loss: 0.0004218509847675925, Validation Loss: 0.14750934747388736\n",
      "Epoch 2185/3000, Training Loss: 0.0004215149982864907, Validation Loss: 0.147523958416538\n",
      "Epoch 2186/3000, Training Loss: 0.0004211795340281382, Validation Loss: 0.14753863302302203\n",
      "Epoch 2187/3000, Training Loss: 0.00042084461755028074, Validation Loss: 0.14755337555656242\n",
      "Epoch 2188/3000, Training Loss: 0.00042051012267023955, Validation Loss: 0.1475680453809284\n",
      "Epoch 2189/3000, Training Loss: 0.0004201761616194427, Validation Loss: 0.14758264889357214\n",
      "Epoch 2190/3000, Training Loss: 0.0004198425839215315, Validation Loss: 0.14759733286700358\n",
      "Epoch 2191/3000, Training Loss: 0.00041950965938304614, Validation Loss: 0.14761198221453617\n",
      "Epoch 2192/3000, Training Loss: 0.00041917712598033767, Validation Loss: 0.1476265610883769\n",
      "Epoch 2193/3000, Training Loss: 0.00041884496944144555, Validation Loss: 0.14764114772737022\n",
      "Epoch 2194/3000, Training Loss: 0.00041851348137891033, Validation Loss: 0.1476558720273317\n",
      "Epoch 2195/3000, Training Loss: 0.0004181823547154089, Validation Loss: 0.14767044092520196\n",
      "Epoch 2196/3000, Training Loss: 0.00041785173692992985, Validation Loss: 0.14768500644097343\n",
      "Epoch 2197/3000, Training Loss: 0.00041752158924842275, Validation Loss: 0.14769967068881426\n",
      "Epoch 2198/3000, Training Loss: 0.00041719193175656266, Validation Loss: 0.14771421948434654\n",
      "Epoch 2199/3000, Training Loss: 0.000416862710121787, Validation Loss: 0.14772879365238828\n",
      "Epoch 2200/3000, Training Loss: 0.0004165339434328481, Validation Loss: 0.14774332745175806\n",
      "Epoch 2201/3000, Training Loss: 0.0004162057553676338, Validation Loss: 0.14775796139730793\n",
      "Epoch 2202/3000, Training Loss: 0.00041587803829833234, Validation Loss: 0.14777247419402587\n",
      "Epoch 2203/3000, Training Loss: 0.0004155506296198175, Validation Loss: 0.14778698174720223\n",
      "Epoch 2204/3000, Training Loss: 0.00041522378995540317, Validation Loss: 0.147801624206213\n",
      "Epoch 2205/3000, Training Loss: 0.0004148974908626207, Validation Loss: 0.14781615386455782\n",
      "Epoch 2206/3000, Training Loss: 0.0004145715234032481, Validation Loss: 0.1478305928634562\n",
      "Epoch 2207/3000, Training Loss: 0.0004142461049574793, Validation Loss: 0.14784511577262277\n",
      "Epoch 2208/3000, Training Loss: 0.0004139211891504415, Validation Loss: 0.14785967860674\n",
      "Epoch 2209/3000, Training Loss: 0.0004135966568508057, Validation Loss: 0.14787425543163657\n",
      "Epoch 2210/3000, Training Loss: 0.0004132726284307049, Validation Loss: 0.1478887794637022\n",
      "Epoch 2211/3000, Training Loss: 0.0004129490337072061, Validation Loss: 0.1479033573202224\n",
      "Epoch 2212/3000, Training Loss: 0.00041262600880928173, Validation Loss: 0.1479179130168555\n",
      "Epoch 2213/3000, Training Loss: 0.00041230334033521865, Validation Loss: 0.1479324139231495\n",
      "Epoch 2214/3000, Training Loss: 0.00041198107078264603, Validation Loss: 0.14794689261289307\n",
      "Epoch 2215/3000, Training Loss: 0.0004116594833681196, Validation Loss: 0.14796155752669257\n",
      "Epoch 2216/3000, Training Loss: 0.0004113381443064745, Validation Loss: 0.14797605911967943\n",
      "Epoch 2217/3000, Training Loss: 0.00041101738906820155, Validation Loss: 0.14799058179569838\n",
      "Epoch 2218/3000, Training Loss: 0.0004106970621462442, Validation Loss: 0.14800517631176713\n",
      "Epoch 2219/3000, Training Loss: 0.00041037715452927405, Validation Loss: 0.14801966892352236\n",
      "Epoch 2220/3000, Training Loss: 0.0004100577175099391, Validation Loss: 0.14803420060236477\n",
      "Epoch 2221/3000, Training Loss: 0.0004097386819773074, Validation Loss: 0.14804877106536593\n",
      "Epoch 2222/3000, Training Loss: 0.00040942026994160734, Validation Loss: 0.14806325389248817\n",
      "Epoch 2223/3000, Training Loss: 0.00040910217837744785, Validation Loss: 0.1480777127835274\n",
      "Epoch 2224/3000, Training Loss: 0.0004087844685202514, Validation Loss: 0.14809215350071658\n",
      "Epoch 2225/3000, Training Loss: 0.00040846734170757, Validation Loss: 0.14810673931180462\n",
      "Epoch 2226/3000, Training Loss: 0.0004081506302898565, Validation Loss: 0.14812117657904783\n",
      "Epoch 2227/3000, Training Loss: 0.0004078343106714005, Validation Loss: 0.14813565680834023\n",
      "Epoch 2228/3000, Training Loss: 0.00040751853141141785, Validation Loss: 0.14815017752780715\n",
      "Epoch 2229/3000, Training Loss: 0.0004072031612252904, Validation Loss: 0.14816459678470495\n",
      "Epoch 2230/3000, Training Loss: 0.0004068881727523334, Validation Loss: 0.14817905264624032\n",
      "Epoch 2231/3000, Training Loss: 0.0004065736558024797, Validation Loss: 0.14819344390735642\n",
      "Epoch 2232/3000, Training Loss: 0.00040625961482471546, Validation Loss: 0.14820795953692475\n",
      "Epoch 2233/3000, Training Loss: 0.00040594609778048, Validation Loss: 0.14822233491998998\n",
      "Epoch 2234/3000, Training Loss: 0.00040563284424333356, Validation Loss: 0.14823671584627282\n",
      "Epoch 2235/3000, Training Loss: 0.0004053200764544887, Validation Loss: 0.1482512284636442\n",
      "Epoch 2236/3000, Training Loss: 0.00040500787522440644, Validation Loss: 0.14826559326309485\n",
      "Epoch 2237/3000, Training Loss: 0.00040469592511274493, Validation Loss: 0.14827991624624273\n",
      "Epoch 2238/3000, Training Loss: 0.00040438451937220576, Validation Loss: 0.14829430019471493\n",
      "Epoch 2239/3000, Training Loss: 0.00040407360670736747, Validation Loss: 0.14830874728846458\n",
      "Epoch 2240/3000, Training Loss: 0.0004037629992927738, Validation Loss: 0.14832306668868636\n",
      "Epoch 2241/3000, Training Loss: 0.00040345291628798705, Validation Loss: 0.14833731788016\n",
      "Epoch 2242/3000, Training Loss: 0.0004031431948246741, Validation Loss: 0.14835168313975666\n",
      "Epoch 2243/3000, Training Loss: 0.0004028340637699123, Validation Loss: 0.14836593157341943\n",
      "Epoch 2244/3000, Training Loss: 0.0004025252228492987, Validation Loss: 0.14838016913720564\n",
      "Epoch 2245/3000, Training Loss: 0.00040221675829411993, Validation Loss: 0.14839438917939657\n",
      "Epoch 2246/3000, Training Loss: 0.00040190893903619524, Validation Loss: 0.14840876505092493\n",
      "Epoch 2247/3000, Training Loss: 0.00040160136146677987, Validation Loss: 0.14842298240674076\n",
      "Epoch 2248/3000, Training Loss: 0.0004012942841660677, Validation Loss: 0.14843717010945265\n",
      "Epoch 2249/3000, Training Loss: 0.0004009876756827352, Validation Loss: 0.14845145763158332\n",
      "Epoch 2250/3000, Training Loss: 0.00040068142245276093, Validation Loss: 0.1484656892972688\n",
      "Epoch 2251/3000, Training Loss: 0.00040037562111897425, Validation Loss: 0.14847990887872348\n",
      "Epoch 2252/3000, Training Loss: 0.0004000701991079456, Validation Loss: 0.14849414876208317\n",
      "Epoch 2253/3000, Training Loss: 0.00039976534080189154, Validation Loss: 0.14850831423286182\n",
      "Epoch 2254/3000, Training Loss: 0.0003994608286002992, Validation Loss: 0.14852243882487082\n",
      "Epoch 2255/3000, Training Loss: 0.0003991566375043434, Validation Loss: 0.1485365419896635\n",
      "Epoch 2256/3000, Training Loss: 0.0003988529991509358, Validation Loss: 0.14855082633551425\n",
      "Epoch 2257/3000, Training Loss: 0.0003985497667745573, Validation Loss: 0.1485649725465363\n",
      "Epoch 2258/3000, Training Loss: 0.0003982469077373058, Validation Loss: 0.14857913286480579\n",
      "Epoch 2259/3000, Training Loss: 0.00039794459412682755, Validation Loss: 0.1485933704393124\n",
      "Epoch 2260/3000, Training Loss: 0.00039764263229897097, Validation Loss: 0.14860750199947723\n",
      "Epoch 2261/3000, Training Loss: 0.000397341040659677, Validation Loss: 0.14862166409419125\n",
      "Epoch 2262/3000, Training Loss: 0.0003970398950706984, Validation Loss: 0.1486357808631731\n",
      "Epoch 2263/3000, Training Loss: 0.00039673918734365167, Validation Loss: 0.14865004192136802\n",
      "Epoch 2264/3000, Training Loss: 0.0003964389473778121, Validation Loss: 0.14866428833943596\n",
      "Epoch 2265/3000, Training Loss: 0.0003961390384704561, Validation Loss: 0.1486784454344609\n",
      "Epoch 2266/3000, Training Loss: 0.000395839540314301, Validation Loss: 0.14869278188612722\n",
      "Epoch 2267/3000, Training Loss: 0.00039554057548495445, Validation Loss: 0.14870693476749422\n",
      "Epoch 2268/3000, Training Loss: 0.00039524185866848056, Validation Loss: 0.14872109183385993\n",
      "Epoch 2269/3000, Training Loss: 0.0003949436750518945, Validation Loss: 0.1487352311838569\n",
      "Epoch 2270/3000, Training Loss: 0.0003946459039821755, Validation Loss: 0.14874949025771558\n",
      "Epoch 2271/3000, Training Loss: 0.0003943484282729962, Validation Loss: 0.1487635797848507\n",
      "Epoch 2272/3000, Training Loss: 0.0003940514643257643, Validation Loss: 0.14877771878936422\n",
      "Epoch 2273/3000, Training Loss: 0.0003937548480360412, Validation Loss: 0.14879193572404625\n",
      "Epoch 2274/3000, Training Loss: 0.0003934587622979872, Validation Loss: 0.14880603742170379\n",
      "Epoch 2275/3000, Training Loss: 0.0003931629720071216, Validation Loss: 0.14882008630009477\n",
      "Epoch 2276/3000, Training Loss: 0.0003928675212529253, Validation Loss: 0.14883425225683325\n",
      "Epoch 2277/3000, Training Loss: 0.00039257269427089625, Validation Loss: 0.14884834312340423\n",
      "Epoch 2278/3000, Training Loss: 0.0003922780639012086, Validation Loss: 0.14886240059920303\n",
      "Epoch 2279/3000, Training Loss: 0.00039198390754051506, Validation Loss: 0.14887643260517047\n",
      "Epoch 2280/3000, Training Loss: 0.00039169021866982453, Validation Loss: 0.14889058205082856\n",
      "Epoch 2281/3000, Training Loss: 0.0003913968369970277, Validation Loss: 0.14890454359166644\n",
      "Epoch 2282/3000, Training Loss: 0.000391103895271275, Validation Loss: 0.1489185771634826\n",
      "Epoch 2283/3000, Training Loss: 0.00039081131999862555, Validation Loss: 0.1489326393659957\n",
      "Epoch 2284/3000, Training Loss: 0.00039051926105532794, Validation Loss: 0.14894663882957254\n",
      "Epoch 2285/3000, Training Loss: 0.0003902275395003597, Validation Loss: 0.1489605936996655\n",
      "Epoch 2286/3000, Training Loss: 0.00038993611374495753, Validation Loss: 0.1489745531880164\n",
      "Epoch 2287/3000, Training Loss: 0.00038964521988901215, Validation Loss: 0.14898864499729064\n",
      "Epoch 2288/3000, Training Loss: 0.000389354673723518, Validation Loss: 0.1490025993285527\n",
      "Epoch 2289/3000, Training Loss: 0.000389064466545278, Validation Loss: 0.1490165273568615\n",
      "Epoch 2290/3000, Training Loss: 0.0003887747874258598, Validation Loss: 0.14903057008994394\n",
      "Epoch 2291/3000, Training Loss: 0.00038848541959351406, Validation Loss: 0.14904446964995252\n",
      "Epoch 2292/3000, Training Loss: 0.0003881964080741634, Validation Loss: 0.14905843831945514\n",
      "Epoch 2293/3000, Training Loss: 0.0003879078133914082, Validation Loss: 0.1490723236048109\n",
      "Epoch 2294/3000, Training Loss: 0.00038761963501091805, Validation Loss: 0.1490863058398554\n",
      "Epoch 2295/3000, Training Loss: 0.00038733188429356034, Validation Loss: 0.14910025084750686\n",
      "Epoch 2296/3000, Training Loss: 0.00038704444996606464, Validation Loss: 0.14911401535730587\n",
      "Epoch 2297/3000, Training Loss: 0.0003867574247611619, Validation Loss: 0.14912798757150073\n",
      "Epoch 2298/3000, Training Loss: 0.0003864708815567639, Validation Loss: 0.14914173797929015\n",
      "Epoch 2299/3000, Training Loss: 0.000386184575322316, Validation Loss: 0.1491555131353637\n",
      "Epoch 2300/3000, Training Loss: 0.0003858987765913563, Validation Loss: 0.14916938862736898\n",
      "Epoch 2301/3000, Training Loss: 0.0003856133833750565, Validation Loss: 0.14918312882440532\n",
      "Epoch 2302/3000, Training Loss: 0.00038532822179219073, Validation Loss: 0.149196930235847\n",
      "Epoch 2303/3000, Training Loss: 0.0003850435617798149, Validation Loss: 0.1492106450782659\n",
      "Epoch 2304/3000, Training Loss: 0.00038475923458698946, Validation Loss: 0.14922449197939763\n",
      "Epoch 2305/3000, Training Loss: 0.00038447537749162613, Validation Loss: 0.14923820898687126\n",
      "Epoch 2306/3000, Training Loss: 0.0003841918219636656, Validation Loss: 0.14925193702546102\n",
      "Epoch 2307/3000, Training Loss: 0.00038390859857857757, Validation Loss: 0.14926575185880298\n",
      "Epoch 2308/3000, Training Loss: 0.0003836259276894152, Validation Loss: 0.14927948938529106\n",
      "Epoch 2309/3000, Training Loss: 0.000383343460602935, Validation Loss: 0.14929319913550052\n",
      "Epoch 2310/3000, Training Loss: 0.0003830614372703225, Validation Loss: 0.14930688087086225\n",
      "Epoch 2311/3000, Training Loss: 0.0003827798756453064, Validation Loss: 0.1493206805043963\n",
      "Epoch 2312/3000, Training Loss: 0.00038249856414706006, Validation Loss: 0.14933433721575282\n",
      "Epoch 2313/3000, Training Loss: 0.00038221767631686727, Validation Loss: 0.1493480626039787\n",
      "Epoch 2314/3000, Training Loss: 0.0003819371543807063, Validation Loss: 0.14936183001897974\n",
      "Epoch 2315/3000, Training Loss: 0.00038165707117422307, Validation Loss: 0.14937548979427934\n",
      "Epoch 2316/3000, Training Loss: 0.0003813773358778577, Validation Loss: 0.1493891529644919\n",
      "Epoch 2317/3000, Training Loss: 0.0003810978753796101, Validation Loss: 0.14940277472807179\n",
      "Epoch 2318/3000, Training Loss: 0.00038081892589207975, Validation Loss: 0.14941657120619356\n",
      "Epoch 2319/3000, Training Loss: 0.0003805402772809077, Validation Loss: 0.14943020031644969\n",
      "Epoch 2320/3000, Training Loss: 0.0003802619604976166, Validation Loss: 0.1494438291992139\n",
      "Epoch 2321/3000, Training Loss: 0.0003799841666269817, Validation Loss: 0.14945752693660683\n",
      "Epoch 2322/3000, Training Loss: 0.00037970663363768437, Validation Loss: 0.14947116339709238\n",
      "Epoch 2323/3000, Training Loss: 0.00037942943188952753, Validation Loss: 0.14948481718008347\n",
      "Epoch 2324/3000, Training Loss: 0.0003791526545502001, Validation Loss: 0.14949850881445464\n",
      "Epoch 2325/3000, Training Loss: 0.00037887627105904364, Validation Loss: 0.1495120745518184\n",
      "Epoch 2326/3000, Training Loss: 0.0003786002419314361, Validation Loss: 0.1495257064382968\n",
      "Epoch 2327/3000, Training Loss: 0.0003783245446787913, Validation Loss: 0.1495392989909401\n",
      "Epoch 2328/3000, Training Loss: 0.0003780492293050382, Validation Loss: 0.1495529866373993\n",
      "Epoch 2329/3000, Training Loss: 0.0003777743491770769, Validation Loss: 0.14956655570438834\n",
      "Epoch 2330/3000, Training Loss: 0.00037749968785251876, Validation Loss: 0.1495801411764262\n",
      "Epoch 2331/3000, Training Loss: 0.000377225544031066, Validation Loss: 0.1495937605011538\n",
      "Epoch 2332/3000, Training Loss: 0.0003769517439326278, Validation Loss: 0.14960731008696893\n",
      "Epoch 2333/3000, Training Loss: 0.00037667818271109124, Validation Loss: 0.1496208303635174\n",
      "Epoch 2334/3000, Training Loss: 0.00037640509430218543, Validation Loss: 0.14963439880030477\n",
      "Epoch 2335/3000, Training Loss: 0.0003761323440273068, Validation Loss: 0.14964801672837774\n",
      "Epoch 2336/3000, Training Loss: 0.00037586000701234654, Validation Loss: 0.14966152809386604\n",
      "Epoch 2337/3000, Training Loss: 0.00037558795104285605, Validation Loss: 0.14967503865633938\n",
      "Epoch 2338/3000, Training Loss: 0.0003753162442830171, Validation Loss: 0.149688634571949\n",
      "Epoch 2339/3000, Training Loss: 0.0003750450073309721, Validation Loss: 0.1497021565453278\n",
      "Epoch 2340/3000, Training Loss: 0.00037477399518681144, Validation Loss: 0.1497156474058675\n",
      "Epoch 2341/3000, Training Loss: 0.0003745033998765708, Validation Loss: 0.14972913562605616\n",
      "Epoch 2342/3000, Training Loss: 0.00037423325495094005, Validation Loss: 0.14974268249928765\n",
      "Epoch 2343/3000, Training Loss: 0.000373963318538755, Validation Loss: 0.149756155970522\n",
      "Epoch 2344/3000, Training Loss: 0.00037369376788880655, Validation Loss: 0.14976966438628117\n",
      "Epoch 2345/3000, Training Loss: 0.00037342461370776177, Validation Loss: 0.1497831935279179\n",
      "Epoch 2346/3000, Training Loss: 0.00037315582074569923, Validation Loss: 0.14979665859632013\n",
      "Epoch 2347/3000, Training Loss: 0.000372887375930154, Validation Loss: 0.14981008550781935\n",
      "Epoch 2348/3000, Training Loss: 0.00037261920608978, Validation Loss: 0.14982361602200558\n",
      "Epoch 2349/3000, Training Loss: 0.00037235150572372677, Validation Loss: 0.14983712532816618\n",
      "Epoch 2350/3000, Training Loss: 0.00037208407736537703, Validation Loss: 0.1498505415631864\n",
      "Epoch 2351/3000, Training Loss: 0.0003718169686243982, Validation Loss: 0.14986395359086901\n",
      "Epoch 2352/3000, Training Loss: 0.00037155037784983275, Validation Loss: 0.1498774575949108\n",
      "Epoch 2353/3000, Training Loss: 0.0003712839839295895, Validation Loss: 0.1498908641646704\n",
      "Epoch 2354/3000, Training Loss: 0.00037101791349496046, Validation Loss: 0.14990428867749742\n",
      "Epoch 2355/3000, Training Loss: 0.0003707522844936733, Validation Loss: 0.14991776735981155\n",
      "Epoch 2356/3000, Training Loss: 0.0003704869850407658, Validation Loss: 0.1499311229856004\n",
      "Epoch 2357/3000, Training Loss: 0.00037022201924051204, Validation Loss: 0.1499445056090485\n",
      "Epoch 2358/3000, Training Loss: 0.0003699573904744856, Validation Loss: 0.1499578690474613\n",
      "Epoch 2359/3000, Training Loss: 0.0003696931334621501, Validation Loss: 0.1499713596779326\n",
      "Epoch 2360/3000, Training Loss: 0.00036942925372288266, Validation Loss: 0.14998468958958242\n",
      "Epoch 2361/3000, Training Loss: 0.0003691655999304483, Validation Loss: 0.14999800615719083\n",
      "Epoch 2362/3000, Training Loss: 0.0003689024011569776, Validation Loss: 0.150011475638814\n",
      "Epoch 2363/3000, Training Loss: 0.0003686395642472759, Validation Loss: 0.1500247886007982\n",
      "Epoch 2364/3000, Training Loss: 0.0003683769331677087, Validation Loss: 0.15003810453245164\n",
      "Epoch 2365/3000, Training Loss: 0.00036811473857854463, Validation Loss: 0.1500514662024519\n",
      "Epoch 2366/3000, Training Loss: 0.00036785290578175315, Validation Loss: 0.1500648396201761\n",
      "Epoch 2367/3000, Training Loss: 0.00036759136450463347, Validation Loss: 0.15007816816843197\n",
      "Epoch 2368/3000, Training Loss: 0.0003673301828188405, Validation Loss: 0.15009147009475263\n",
      "Epoch 2369/3000, Training Loss: 0.00036706931961269127, Validation Loss: 0.15010485207571925\n",
      "Epoch 2370/3000, Training Loss: 0.00036680884281044257, Validation Loss: 0.1501181633857302\n",
      "Epoch 2371/3000, Training Loss: 0.00036654861873028035, Validation Loss: 0.150131432877764\n",
      "Epoch 2372/3000, Training Loss: 0.00036628879314992604, Validation Loss: 0.15014482203718454\n",
      "Epoch 2373/3000, Training Loss: 0.00036602937747340575, Validation Loss: 0.15015807185118604\n",
      "Epoch 2374/3000, Training Loss: 0.0003657701379826958, Validation Loss: 0.15017132253911436\n",
      "Epoch 2375/3000, Training Loss: 0.0003655112818405594, Validation Loss: 0.15018462104381697\n",
      "Epoch 2376/3000, Training Loss: 0.0003652528353879997, Validation Loss: 0.1501979544309344\n",
      "Epoch 2377/3000, Training Loss: 0.00036499468297809335, Validation Loss: 0.15021122113673657\n",
      "Epoch 2378/3000, Training Loss: 0.0003647368662950276, Validation Loss: 0.15022444258466316\n",
      "Epoch 2379/3000, Training Loss: 0.0003644793249452976, Validation Loss: 0.15023777018131798\n",
      "Epoch 2380/3000, Training Loss: 0.0003642221910504434, Validation Loss: 0.15025103240052518\n",
      "Epoch 2381/3000, Training Loss: 0.0003639653489251922, Validation Loss: 0.15026421647752985\n",
      "Epoch 2382/3000, Training Loss: 0.0003637088089840099, Validation Loss: 0.15027739696483833\n",
      "Epoch 2383/3000, Training Loss: 0.0003634527141382931, Validation Loss: 0.1502907281982506\n",
      "Epoch 2384/3000, Training Loss: 0.0003631968558438986, Validation Loss: 0.15030390501295146\n",
      "Epoch 2385/3000, Training Loss: 0.0003629412751048219, Validation Loss: 0.1503171249252558\n",
      "Epoch 2386/3000, Training Loss: 0.00036268614246579185, Validation Loss: 0.15033040809610015\n",
      "Epoch 2387/3000, Training Loss: 0.0003624313006424945, Validation Loss: 0.15034355525363852\n",
      "Epoch 2388/3000, Training Loss: 0.0003621767424634381, Validation Loss: 0.15035673810033476\n",
      "Epoch 2389/3000, Training Loss: 0.00036192252781100934, Validation Loss: 0.15036990217318585\n",
      "Epoch 2390/3000, Training Loss: 0.00036166868159038515, Validation Loss: 0.15038314561220953\n",
      "Epoch 2391/3000, Training Loss: 0.00036141516480295207, Validation Loss: 0.15039630840606458\n",
      "Epoch 2392/3000, Training Loss: 0.0003611618898564923, Validation Loss: 0.15040941881153483\n",
      "Epoch 2393/3000, Training Loss: 0.0003609089899497331, Validation Loss: 0.15042269682664697\n",
      "Epoch 2394/3000, Training Loss: 0.00036065645911446275, Validation Loss: 0.1504358261476463\n",
      "Epoch 2395/3000, Training Loss: 0.00036040413257830184, Validation Loss: 0.15044883312388055\n",
      "Epoch 2396/3000, Training Loss: 0.0003601522410074451, Validation Loss: 0.15046193475555455\n",
      "Epoch 2397/3000, Training Loss: 0.00035990069766787553, Validation Loss: 0.15047494172771694\n",
      "Epoch 2398/3000, Training Loss: 0.0003596493501316226, Validation Loss: 0.1504879078039199\n",
      "Epoch 2399/3000, Training Loss: 0.0003593984105803948, Validation Loss: 0.15050092342809765\n",
      "Epoch 2400/3000, Training Loss: 0.0003591477930226345, Validation Loss: 0.15051395627933398\n",
      "Epoch 2401/3000, Training Loss: 0.00035889748713079485, Validation Loss: 0.1505269306253143\n",
      "Epoch 2402/3000, Training Loss: 0.000358647489389878, Validation Loss: 0.15053987542717862\n",
      "Epoch 2403/3000, Training Loss: 0.00035839777433026423, Validation Loss: 0.15055292190074465\n",
      "Epoch 2404/3000, Training Loss: 0.0003581484777656918, Validation Loss: 0.15056590836898848\n",
      "Epoch 2405/3000, Training Loss: 0.0003578993857691341, Validation Loss: 0.1505788143892272\n",
      "Epoch 2406/3000, Training Loss: 0.0003576506317357505, Validation Loss: 0.15059179366026224\n",
      "Epoch 2407/3000, Training Loss: 0.00035740234632947507, Validation Loss: 0.15060478058464696\n",
      "Epoch 2408/3000, Training Loss: 0.00035715418512371463, Validation Loss: 0.1506176774272422\n",
      "Epoch 2409/3000, Training Loss: 0.0003569063930593777, Validation Loss: 0.15063059748442756\n",
      "Epoch 2410/3000, Training Loss: 0.00035665894936076317, Validation Loss: 0.15064356097968892\n",
      "Epoch 2411/3000, Training Loss: 0.00035641180111749813, Validation Loss: 0.150656454203868\n",
      "Epoch 2412/3000, Training Loss: 0.0003561650103249403, Validation Loss: 0.15066931645989035\n",
      "Epoch 2413/3000, Training Loss: 0.000355918428256151, Validation Loss: 0.15068228251160243\n",
      "Epoch 2414/3000, Training Loss: 0.0003556722685082455, Validation Loss: 0.15069517391945939\n",
      "Epoch 2415/3000, Training Loss: 0.00035542635875448656, Validation Loss: 0.15070801177407037\n",
      "Epoch 2416/3000, Training Loss: 0.0003551807023213952, Validation Loss: 0.15072086408023816\n",
      "Epoch 2417/3000, Training Loss: 0.00035493555346757403, Validation Loss: 0.15073380806644082\n",
      "Epoch 2418/3000, Training Loss: 0.0003546905427514438, Validation Loss: 0.15074664761150264\n",
      "Epoch 2419/3000, Training Loss: 0.00035444583690743506, Validation Loss: 0.15075951391576586\n",
      "Epoch 2420/3000, Training Loss: 0.0003542015126852209, Validation Loss: 0.1507724194876025\n",
      "Epoch 2421/3000, Training Loss: 0.0003539574629615937, Validation Loss: 0.15078523354932216\n",
      "Epoch 2422/3000, Training Loss: 0.00035371378261682564, Validation Loss: 0.15079806604185475\n",
      "Epoch 2423/3000, Training Loss: 0.00035347029491456563, Validation Loss: 0.1508108675657004\n",
      "Epoch 2424/3000, Training Loss: 0.00035322719174608815, Validation Loss: 0.15082381453762084\n",
      "Epoch 2425/3000, Training Loss: 0.0003529844003952514, Validation Loss: 0.1508365963197308\n",
      "Epoch 2426/3000, Training Loss: 0.0003527418195093787, Validation Loss: 0.15084938345327603\n",
      "Epoch 2427/3000, Training Loss: 0.00035249968533679614, Validation Loss: 0.1508622835800212\n",
      "Epoch 2428/3000, Training Loss: 0.0003522577912508964, Validation Loss: 0.15087507097835334\n",
      "Epoch 2429/3000, Training Loss: 0.00035201612733535995, Validation Loss: 0.1508878401963004\n",
      "Epoch 2430/3000, Training Loss: 0.00035177484325133146, Validation Loss: 0.15090074309489798\n",
      "Epoch 2431/3000, Training Loss: 0.00035153388182182, Validation Loss: 0.15091349253117375\n",
      "Epoch 2432/3000, Training Loss: 0.0003512931910111138, Validation Loss: 0.15092627098469402\n",
      "Epoch 2433/3000, Training Loss: 0.00035105278226447664, Validation Loss: 0.15093902047323118\n",
      "Epoch 2434/3000, Training Loss: 0.000350812681954708, Validation Loss: 0.15095186996569457\n",
      "Epoch 2435/3000, Training Loss: 0.0003505729099240049, Validation Loss: 0.15096461420289845\n",
      "Epoch 2436/3000, Training Loss: 0.0003503333864711647, Validation Loss: 0.15097729939466634\n",
      "Epoch 2437/3000, Training Loss: 0.00035009421233578124, Validation Loss: 0.15099010225488135\n",
      "Epoch 2438/3000, Training Loss: 0.00034985540021317693, Validation Loss: 0.15100278795930316\n",
      "Epoch 2439/3000, Training Loss: 0.00034961673077311067, Validation Loss: 0.15101547039146346\n",
      "Epoch 2440/3000, Training Loss: 0.0003493784082987626, Validation Loss: 0.15102817573936425\n",
      "Epoch 2441/3000, Training Loss: 0.0003491404634376526, Validation Loss: 0.15104094364005796\n",
      "Epoch 2442/3000, Training Loss: 0.00034890274398762216, Validation Loss: 0.15105359162554466\n",
      "Epoch 2443/3000, Training Loss: 0.00034866536755263883, Validation Loss: 0.1510661936468001\n",
      "Epoch 2444/3000, Training Loss: 0.0003484282317578819, Validation Loss: 0.15107890598280013\n",
      "Epoch 2445/3000, Training Loss: 0.0003481914344407236, Validation Loss: 0.15109156704284674\n",
      "Epoch 2446/3000, Training Loss: 0.0003479549024945776, Validation Loss: 0.1511041844239734\n",
      "Epoch 2447/3000, Training Loss: 0.00034771865043771124, Validation Loss: 0.15111685146149276\n",
      "Epoch 2448/3000, Training Loss: 0.0003474827917125044, Validation Loss: 0.15112949924378644\n",
      "Epoch 2449/3000, Training Loss: 0.0003472471210269804, Validation Loss: 0.15114209967689415\n",
      "Epoch 2450/3000, Training Loss: 0.0003470117197019451, Validation Loss: 0.15115472613711226\n",
      "Epoch 2451/3000, Training Loss: 0.00034677673092270065, Validation Loss: 0.15116738067759328\n",
      "Epoch 2452/3000, Training Loss: 0.0003465419685541931, Validation Loss: 0.15117993938632784\n",
      "Epoch 2453/3000, Training Loss: 0.00034630745997242394, Validation Loss: 0.1511925598258363\n",
      "Epoch 2454/3000, Training Loss: 0.00034607329363054335, Validation Loss: 0.15120522077621965\n",
      "Epoch 2455/3000, Training Loss: 0.0003458394141205434, Validation Loss: 0.1512177611659468\n",
      "Epoch 2456/3000, Training Loss: 0.00034560594218083267, Validation Loss: 0.15123034021795395\n",
      "Epoch 2457/3000, Training Loss: 0.00034537266180413324, Validation Loss: 0.15124284247254693\n",
      "Epoch 2458/3000, Training Loss: 0.0003451396967425325, Validation Loss: 0.15125546304736576\n",
      "Epoch 2459/3000, Training Loss: 0.0003449070795200957, Validation Loss: 0.15126801627258113\n",
      "Epoch 2460/3000, Training Loss: 0.0003446746880410194, Validation Loss: 0.15128053202762987\n",
      "Epoch 2461/3000, Training Loss: 0.00034444255177587546, Validation Loss: 0.15129312849028764\n",
      "Epoch 2462/3000, Training Loss: 0.0003442108051018043, Validation Loss: 0.15130567563659705\n",
      "Epoch 2463/3000, Training Loss: 0.0003439792867474385, Validation Loss: 0.15131815885391026\n",
      "Epoch 2464/3000, Training Loss: 0.00034374797777436905, Validation Loss: 0.1513307383915191\n",
      "Epoch 2465/3000, Training Loss: 0.00034351710056325244, Validation Loss: 0.15134324338166671\n",
      "Epoch 2466/3000, Training Loss: 0.00034328642300459456, Validation Loss: 0.1513557257839732\n",
      "Epoch 2467/3000, Training Loss: 0.0003430559855433937, Validation Loss: 0.15136818688456002\n",
      "Epoch 2468/3000, Training Loss: 0.000342825931699675, Validation Loss: 0.15138073732029603\n",
      "Epoch 2469/3000, Training Loss: 0.0003425961364758691, Validation Loss: 0.15139319832870676\n",
      "Epoch 2470/3000, Training Loss: 0.0003423665519250475, Validation Loss: 0.15140567600772484\n",
      "Epoch 2471/3000, Training Loss: 0.0003421373092359445, Validation Loss: 0.1514182114108959\n",
      "Epoch 2472/3000, Training Loss: 0.00034190838541877706, Validation Loss: 0.1514306478659757\n",
      "Epoch 2473/3000, Training Loss: 0.000341679643079163, Validation Loss: 0.1514430957088631\n",
      "Epoch 2474/3000, Training Loss: 0.0003414512116727199, Validation Loss: 0.15145553487884467\n",
      "Epoch 2475/3000, Training Loss: 0.00034122314820300297, Validation Loss: 0.1514680189870554\n",
      "Epoch 2476/3000, Training Loss: 0.00034099524934314446, Validation Loss: 0.15148046483196975\n",
      "Epoch 2477/3000, Training Loss: 0.00034076765046250006, Validation Loss: 0.15149288728942048\n",
      "Epoch 2478/3000, Training Loss: 0.00034054040214336604, Validation Loss: 0.15150537770592815\n",
      "Epoch 2479/3000, Training Loss: 0.0003403133470287546, Validation Loss: 0.1515178069262015\n",
      "Epoch 2480/3000, Training Loss: 0.00034008658901509654, Validation Loss: 0.15153023319280845\n",
      "Epoch 2481/3000, Training Loss: 0.0003398601059514261, Validation Loss: 0.15154267271517763\n",
      "Epoch 2482/3000, Training Loss: 0.0003396339456767446, Validation Loss: 0.15155502992019626\n",
      "Epoch 2483/3000, Training Loss: 0.00033940802613143293, Validation Loss: 0.15156741451013203\n",
      "Epoch 2484/3000, Training Loss: 0.0003391824075488185, Validation Loss: 0.15157967417466867\n",
      "Epoch 2485/3000, Training Loss: 0.00033895702484864075, Validation Loss: 0.1515920141967049\n",
      "Epoch 2486/3000, Training Loss: 0.0003387319380442786, Validation Loss: 0.1516043058742355\n",
      "Epoch 2487/3000, Training Loss: 0.0003385071635992192, Validation Loss: 0.15161653508595288\n",
      "Epoch 2488/3000, Training Loss: 0.00033828257392381627, Validation Loss: 0.15162886004535137\n",
      "Epoch 2489/3000, Training Loss: 0.00033805830715721657, Validation Loss: 0.1516411393265638\n",
      "Epoch 2490/3000, Training Loss: 0.00033783438011945156, Validation Loss: 0.15165336096229481\n",
      "Epoch 2491/3000, Training Loss: 0.0003376105728648126, Validation Loss: 0.15166559088425882\n",
      "Epoch 2492/3000, Training Loss: 0.00033738712742569914, Validation Loss: 0.15167790925329105\n",
      "Epoch 2493/3000, Training Loss: 0.0003371640229170201, Validation Loss: 0.1516901588919863\n",
      "Epoch 2494/3000, Training Loss: 0.0003369410391733881, Validation Loss: 0.15170237375419726\n",
      "Epoch 2495/3000, Training Loss: 0.0003367183717967824, Validation Loss: 0.15171470777097795\n",
      "Epoch 2496/3000, Training Loss: 0.00033649607625989704, Validation Loss: 0.15172689775507536\n",
      "Epoch 2497/3000, Training Loss: 0.0003362739344890075, Validation Loss: 0.15173908884977746\n",
      "Epoch 2498/3000, Training Loss: 0.0003360520246033384, Validation Loss: 0.15175142029372446\n",
      "Epoch 2499/3000, Training Loss: 0.0003358305558456732, Validation Loss: 0.15176361418950302\n",
      "Epoch 2500/3000, Training Loss: 0.00033560924308598424, Validation Loss: 0.15177579042683925\n",
      "Epoch 2501/3000, Training Loss: 0.0003353881230685057, Validation Loss: 0.1517880104579863\n",
      "Epoch 2502/3000, Training Loss: 0.00033516742937375263, Validation Loss: 0.15180027138300678\n",
      "Epoch 2503/3000, Training Loss: 0.00033494695697534963, Validation Loss: 0.15181242254051208\n",
      "Epoch 2504/3000, Training Loss: 0.0003347266344050682, Validation Loss: 0.15182457974515576\n",
      "Epoch 2505/3000, Training Loss: 0.0003345067184935256, Validation Loss: 0.1518368424413087\n",
      "Epoch 2506/3000, Training Loss: 0.00033428702530995883, Validation Loss: 0.1518490287059826\n",
      "Epoch 2507/3000, Training Loss: 0.0003340675434945013, Validation Loss: 0.15186113917303176\n",
      "Epoch 2508/3000, Training Loss: 0.00033384838713805233, Validation Loss: 0.15187326986392552\n",
      "Epoch 2509/3000, Training Loss: 0.0003336295048157166, Validation Loss: 0.15188546617042026\n",
      "Epoch 2510/3000, Training Loss: 0.0003334108222124631, Validation Loss: 0.15189760281873915\n",
      "Epoch 2511/3000, Training Loss: 0.0003331924462948257, Validation Loss: 0.15190971013478555\n",
      "Epoch 2512/3000, Training Loss: 0.0003329743171620792, Validation Loss: 0.15192188338352738\n",
      "Epoch 2513/3000, Training Loss: 0.0003327564415245137, Validation Loss: 0.15193400907545326\n",
      "Epoch 2514/3000, Training Loss: 0.0003325388618311957, Validation Loss: 0.15194610181889412\n",
      "Epoch 2515/3000, Training Loss: 0.0003323214870213809, Validation Loss: 0.15195828286177498\n",
      "Epoch 2516/3000, Training Loss: 0.0003321044065137329, Validation Loss: 0.1519703938445732\n",
      "Epoch 2517/3000, Training Loss: 0.0003318876189110291, Validation Loss: 0.1519824756613591\n",
      "Epoch 2518/3000, Training Loss: 0.0003316710177927055, Validation Loss: 0.15199453940754615\n",
      "Epoch 2519/3000, Training Loss: 0.00033145468930469293, Validation Loss: 0.15200670323036825\n",
      "Epoch 2520/3000, Training Loss: 0.0003312387101135488, Validation Loss: 0.15201880911687266\n",
      "Epoch 2521/3000, Training Loss: 0.0003310228868780564, Validation Loss: 0.1520308435604811\n",
      "Epoch 2522/3000, Training Loss: 0.000330807297028302, Validation Loss: 0.15204299314944492\n",
      "Epoch 2523/3000, Training Loss: 0.00033059209793528065, Validation Loss: 0.15205504217447824\n",
      "Epoch 2524/3000, Training Loss: 0.0003303770752871155, Validation Loss: 0.152067036426317\n",
      "Epoch 2525/3000, Training Loss: 0.00033016222648516206, Validation Loss: 0.15207902509367602\n",
      "Epoch 2526/3000, Training Loss: 0.00032994779519872217, Validation Loss: 0.15209115356886524\n",
      "Epoch 2527/3000, Training Loss: 0.00032973356533265154, Validation Loss: 0.15210314075682568\n",
      "Epoch 2528/3000, Training Loss: 0.0003295194868893482, Validation Loss: 0.1521151155116312\n",
      "Epoch 2529/3000, Training Loss: 0.0003293057649125961, Validation Loss: 0.15212719823701817\n",
      "Epoch 2530/3000, Training Loss: 0.00032909233649682873, Validation Loss: 0.1521391844104644\n",
      "Epoch 2531/3000, Training Loss: 0.00032887903667232346, Validation Loss: 0.15215114514529843\n",
      "Epoch 2532/3000, Training Loss: 0.00032866603271144534, Validation Loss: 0.15216320690583585\n",
      "Epoch 2533/3000, Training Loss: 0.0003284533950070373, Validation Loss: 0.15217519769616172\n",
      "Epoch 2534/3000, Training Loss: 0.0003282408580403001, Validation Loss: 0.1521872259715005\n",
      "Epoch 2535/3000, Training Loss: 0.0003280286038460058, Validation Loss: 0.1521992515086539\n",
      "Epoch 2536/3000, Training Loss: 0.0003278167059952689, Validation Loss: 0.15221138832245487\n",
      "Epoch 2537/3000, Training Loss: 0.00032760497584840377, Validation Loss: 0.15222340314647417\n",
      "Epoch 2538/3000, Training Loss: 0.0003273934705857166, Validation Loss: 0.15223541355444678\n",
      "Epoch 2539/3000, Training Loss: 0.0003271822926446733, Validation Loss: 0.15224749219096326\n",
      "Epoch 2540/3000, Training Loss: 0.00032697133439594654, Validation Loss: 0.15225950621421452\n",
      "Epoch 2541/3000, Training Loss: 0.0003267605840278404, Validation Loss: 0.15227151218386425\n",
      "Epoch 2542/3000, Training Loss: 0.0003265501337134574, Validation Loss: 0.15228358051936858\n",
      "Epoch 2543/3000, Training Loss: 0.0003263399412086842, Validation Loss: 0.15229559339589216\n",
      "Epoch 2544/3000, Training Loss: 0.00032612993367696596, Validation Loss: 0.15230758425575916\n",
      "Epoch 2545/3000, Training Loss: 0.00032592023817503704, Validation Loss: 0.15231958021886535\n",
      "Epoch 2546/3000, Training Loss: 0.0003257107302333984, Validation Loss: 0.15233167037069206\n",
      "Epoch 2547/3000, Training Loss: 0.0003255015120603532, Validation Loss: 0.15234363735644899\n",
      "Epoch 2548/3000, Training Loss: 0.00032529256283852685, Validation Loss: 0.15235558180153314\n",
      "Epoch 2549/3000, Training Loss: 0.0003250837583441577, Validation Loss: 0.15236761869396676\n",
      "Epoch 2550/3000, Training Loss: 0.0003248753166360081, Validation Loss: 0.15237957425967752\n",
      "Epoch 2551/3000, Training Loss: 0.00032466706074253747, Validation Loss: 0.15239156044375382\n",
      "Epoch 2552/3000, Training Loss: 0.00032445900869697124, Validation Loss: 0.15240347510623903\n",
      "Epoch 2553/3000, Training Loss: 0.00032425127470263055, Validation Loss: 0.15241548561481635\n",
      "Epoch 2554/3000, Training Loss: 0.0003240437920795213, Validation Loss: 0.15242742964778902\n",
      "Epoch 2555/3000, Training Loss: 0.00032383647988697714, Validation Loss: 0.1524393213678223\n",
      "Epoch 2556/3000, Training Loss: 0.00032362942723819456, Validation Loss: 0.15245131032015707\n",
      "Epoch 2557/3000, Training Loss: 0.00032342272300899823, Validation Loss: 0.15246325472799435\n",
      "Epoch 2558/3000, Training Loss: 0.00032321613115150516, Validation Loss: 0.1524751585348178\n",
      "Epoch 2559/3000, Training Loss: 0.0003230097727754001, Validation Loss: 0.15248711153967034\n",
      "Epoch 2560/3000, Training Loss: 0.00032280377955358865, Validation Loss: 0.15249907047333924\n",
      "Epoch 2561/3000, Training Loss: 0.000322597955613604, Validation Loss: 0.15251095088669564\n",
      "Epoch 2562/3000, Training Loss: 0.0003223923176688192, Validation Loss: 0.1525227166828428\n",
      "Epoch 2563/3000, Training Loss: 0.0003221870046126203, Validation Loss: 0.1525346241170291\n",
      "Epoch 2564/3000, Training Loss: 0.0003219819768499222, Validation Loss: 0.15254641734886906\n",
      "Epoch 2565/3000, Training Loss: 0.0003217770383811507, Validation Loss: 0.1525581731306285\n",
      "Epoch 2566/3000, Training Loss: 0.0003215724167957435, Validation Loss: 0.15257006677697788\n",
      "Epoch 2567/3000, Training Loss: 0.0003213681171154769, Validation Loss: 0.15258186814874108\n",
      "Epoch 2568/3000, Training Loss: 0.0003211639052222978, Validation Loss: 0.15259360010374295\n",
      "Epoch 2569/3000, Training Loss: 0.0003209599790429692, Validation Loss: 0.15260540396365915\n",
      "Epoch 2570/3000, Training Loss: 0.0003207563819546751, Validation Loss: 0.15261727102764444\n",
      "Epoch 2571/3000, Training Loss: 0.0003205529187389057, Validation Loss: 0.15262900299882723\n",
      "Epoch 2572/3000, Training Loss: 0.0003203496895748262, Validation Loss: 0.15264079475183107\n",
      "Epoch 2573/3000, Training Loss: 0.00032014676586329286, Validation Loss: 0.15265263139466806\n",
      "Epoch 2574/3000, Training Loss: 0.00031994404561231846, Validation Loss: 0.15266435142420798\n",
      "Epoch 2575/3000, Training Loss: 0.00031974152499704043, Validation Loss: 0.15267609175160762\n",
      "Epoch 2576/3000, Training Loss: 0.00031953926761037237, Validation Loss: 0.15268794660033067\n",
      "Epoch 2577/3000, Training Loss: 0.0003193373020512939, Validation Loss: 0.15269968027523592\n",
      "Epoch 2578/3000, Training Loss: 0.00031913547476650244, Validation Loss: 0.15271140012751275\n",
      "Epoch 2579/3000, Training Loss: 0.0003189339019034173, Validation Loss: 0.152723162091527\n",
      "Epoch 2580/3000, Training Loss: 0.00031873260992557555, Validation Loss: 0.1527349645618405\n",
      "Epoch 2581/3000, Training Loss: 0.0003185315276437299, Validation Loss: 0.15274668070308164\n",
      "Epoch 2582/3000, Training Loss: 0.00031833064141063866, Validation Loss: 0.1527584322374555\n",
      "Epoch 2583/3000, Training Loss: 0.00031813000902283924, Validation Loss: 0.15277020259958818\n",
      "Epoch 2584/3000, Training Loss: 0.0003179296625663958, Validation Loss: 0.15278190856489177\n",
      "Epoch 2585/3000, Training Loss: 0.00031772947030916113, Validation Loss: 0.1527936359852626\n",
      "Epoch 2586/3000, Training Loss: 0.0003175294917337142, Validation Loss: 0.15280530828364697\n",
      "Epoch 2587/3000, Training Loss: 0.0003173298491725951, Validation Loss: 0.1528171082781999\n",
      "Epoch 2588/3000, Training Loss: 0.00031713037863344284, Validation Loss: 0.1528288336280088\n",
      "Epoch 2589/3000, Training Loss: 0.00031693108358021556, Validation Loss: 0.15284049381397613\n",
      "Epoch 2590/3000, Training Loss: 0.00031673209581766323, Validation Loss: 0.15285230883407308\n",
      "Epoch 2591/3000, Training Loss: 0.00031653334706787506, Validation Loss: 0.15286404301423132\n",
      "Epoch 2592/3000, Training Loss: 0.0003163347464639522, Validation Loss: 0.15287569845642515\n",
      "Epoch 2593/3000, Training Loss: 0.0003161364002521306, Validation Loss: 0.15288745788597122\n",
      "Epoch 2594/3000, Training Loss: 0.00031593836310768626, Validation Loss: 0.15289920511407504\n",
      "Epoch 2595/3000, Training Loss: 0.00031574046639419037, Validation Loss: 0.15291084998851093\n",
      "Epoch 2596/3000, Training Loss: 0.0003155427933238399, Validation Loss: 0.15292252493381545\n",
      "Epoch 2597/3000, Training Loss: 0.0003153454123755389, Validation Loss: 0.15293433981701648\n",
      "Epoch 2598/3000, Training Loss: 0.00031514822700829135, Validation Loss: 0.15294598061558787\n",
      "Epoch 2599/3000, Training Loss: 0.0003149512324475886, Validation Loss: 0.1529576348022964\n",
      "Epoch 2600/3000, Training Loss: 0.000314754473840177, Validation Loss: 0.15296944547506575\n",
      "Epoch 2601/3000, Training Loss: 0.00031455801486478913, Validation Loss: 0.15298107547702575\n",
      "Epoch 2602/3000, Training Loss: 0.00031436170083756727, Validation Loss: 0.15299269748722746\n",
      "Epoch 2603/3000, Training Loss: 0.00031416557327639916, Validation Loss: 0.15300438231496252\n",
      "Epoch 2604/3000, Training Loss: 0.00031396985379410816, Validation Loss: 0.15301610517772485\n",
      "Epoch 2605/3000, Training Loss: 0.0003137741869382137, Validation Loss: 0.15302774279025058\n",
      "Epoch 2606/3000, Training Loss: 0.000313578723019455, Validation Loss: 0.1530394050643626\n",
      "Epoch 2607/3000, Training Loss: 0.00031338362649966007, Validation Loss: 0.15305112365226178\n",
      "Epoch 2608/3000, Training Loss: 0.00031318867614208573, Validation Loss: 0.15306275051084456\n",
      "Epoch 2609/3000, Training Loss: 0.00031299387938869, Validation Loss: 0.15307439850016727\n",
      "Epoch 2610/3000, Training Loss: 0.00031279940687178, Validation Loss: 0.15308608834664522\n",
      "Epoch 2611/3000, Training Loss: 0.00031260516315256885, Validation Loss: 0.15309769980292157\n",
      "Epoch 2612/3000, Training Loss: 0.0003124110308691154, Validation Loss: 0.15310933582470226\n",
      "Epoch 2613/3000, Training Loss: 0.00031221719256253715, Validation Loss: 0.1531209305738883\n",
      "Epoch 2614/3000, Training Loss: 0.00031202363030256115, Validation Loss: 0.1531326190565289\n",
      "Epoch 2615/3000, Training Loss: 0.0003118301663054028, Validation Loss: 0.1531442138780765\n",
      "Epoch 2616/3000, Training Loss: 0.00031163701049977664, Validation Loss: 0.15315580786197644\n",
      "Epoch 2617/3000, Training Loss: 0.0003114440338260307, Validation Loss: 0.1531675086670597\n",
      "Epoch 2618/3000, Training Loss: 0.0003112512782505892, Validation Loss: 0.15317909272043795\n",
      "Epoch 2619/3000, Training Loss: 0.0003110587771538128, Validation Loss: 0.1531906724676798\n",
      "Epoch 2620/3000, Training Loss: 0.000310866417631404, Validation Loss: 0.15320233463616711\n",
      "Epoch 2621/3000, Training Loss: 0.0003106743658852348, Validation Loss: 0.15321390454661526\n",
      "Epoch 2622/3000, Training Loss: 0.0003104825031620961, Validation Loss: 0.1532254835391092\n",
      "Epoch 2623/3000, Training Loss: 0.0003102907922688807, Validation Loss: 0.1532370370707574\n",
      "Epoch 2624/3000, Training Loss: 0.000310099365694187, Validation Loss: 0.1532486834860736\n",
      "Epoch 2625/3000, Training Loss: 0.00030990816575307366, Validation Loss: 0.1532602752830961\n",
      "Epoch 2626/3000, Training Loss: 0.0003097171270653714, Validation Loss: 0.15327181458193484\n",
      "Epoch 2627/3000, Training Loss: 0.00030952630344147043, Validation Loss: 0.15328344022154206\n",
      "Epoch 2628/3000, Training Loss: 0.00030933579685111373, Validation Loss: 0.1532950177413343\n",
      "Epoch 2629/3000, Training Loss: 0.00030914539909652583, Validation Loss: 0.15330655617397473\n",
      "Epoch 2630/3000, Training Loss: 0.0003089551840558874, Validation Loss: 0.15331808056707122\n",
      "Epoch 2631/3000, Training Loss: 0.000308765339550358, Validation Loss: 0.15332973411928788\n",
      "Epoch 2632/3000, Training Loss: 0.00030857559938775254, Validation Loss: 0.15334124060635781\n",
      "Epoch 2633/3000, Training Loss: 0.00030838602210651287, Validation Loss: 0.15335272755771157\n",
      "Epoch 2634/3000, Training Loss: 0.000308196759809993, Validation Loss: 0.15336435711001067\n",
      "Epoch 2635/3000, Training Loss: 0.00030800772817079906, Validation Loss: 0.15337589718083838\n",
      "Epoch 2636/3000, Training Loss: 0.00030781878521638406, Validation Loss: 0.15338744176104888\n",
      "Epoch 2637/3000, Training Loss: 0.0003076301308797233, Validation Loss: 0.1533991121749547\n",
      "Epoch 2638/3000, Training Loss: 0.00030744178655904255, Validation Loss: 0.15341075704532947\n",
      "Epoch 2639/3000, Training Loss: 0.00030725348590877254, Validation Loss: 0.153422337667342\n",
      "Epoch 2640/3000, Training Loss: 0.00030706545836610156, Validation Loss: 0.15343390979216745\n",
      "Epoch 2641/3000, Training Loss: 0.00030687772560119935, Validation Loss: 0.1534456556143693\n",
      "Epoch 2642/3000, Training Loss: 0.0003066900986260693, Validation Loss: 0.1534572656414357\n",
      "Epoch 2643/3000, Training Loss: 0.0003065027173214426, Validation Loss: 0.1534688622546505\n",
      "Epoch 2644/3000, Training Loss: 0.0003063155837110225, Validation Loss: 0.15348054757452118\n",
      "Epoch 2645/3000, Training Loss: 0.0003061286097159671, Validation Loss: 0.1534921233423029\n",
      "Epoch 2646/3000, Training Loss: 0.0003059418407562265, Validation Loss: 0.15350370770164332\n",
      "Epoch 2647/3000, Training Loss: 0.00030575530548052754, Validation Loss: 0.15351535968918537\n",
      "Epoch 2648/3000, Training Loss: 0.0003055689904265558, Validation Loss: 0.1535269234091318\n",
      "Epoch 2649/3000, Training Loss: 0.0003053828424008835, Validation Loss: 0.15353848228414574\n",
      "Epoch 2650/3000, Training Loss: 0.0003051969139629128, Validation Loss: 0.1535500634015721\n",
      "Epoch 2651/3000, Training Loss: 0.00030501122339360824, Validation Loss: 0.15356168789545296\n",
      "Epoch 2652/3000, Training Loss: 0.00030482572088774365, Validation Loss: 0.15357320834323032\n",
      "Epoch 2653/3000, Training Loss: 0.0003046403800072778, Validation Loss: 0.1535847836117714\n",
      "Epoch 2654/3000, Training Loss: 0.0003044553212574602, Validation Loss: 0.15359637209950794\n",
      "Epoch 2655/3000, Training Loss: 0.00030427045927710326, Validation Loss: 0.15360787653598545\n",
      "Epoch 2656/3000, Training Loss: 0.0003040857339613967, Validation Loss: 0.15361940457738696\n",
      "Epoch 2657/3000, Training Loss: 0.00030390125770390507, Validation Loss: 0.1536308860663279\n",
      "Epoch 2658/3000, Training Loss: 0.00030371705318632823, Validation Loss: 0.15364245312993488\n",
      "Epoch 2659/3000, Training Loss: 0.0003035329471392143, Validation Loss: 0.15365397739042633\n",
      "Epoch 2660/3000, Training Loss: 0.00030334906555980385, Validation Loss: 0.15366546116836036\n",
      "Epoch 2661/3000, Training Loss: 0.00030316546744532354, Validation Loss: 0.1536770161800441\n",
      "Epoch 2662/3000, Training Loss: 0.00030298198887305003, Validation Loss: 0.1536885250334575\n",
      "Epoch 2663/3000, Training Loss: 0.00030279871898234134, Validation Loss: 0.153699960924615\n",
      "Epoch 2664/3000, Training Loss: 0.00030261567896890725, Validation Loss: 0.1537115366791169\n",
      "Epoch 2665/3000, Training Loss: 0.0003024328602577991, Validation Loss: 0.15372303364523768\n",
      "Epoch 2666/3000, Training Loss: 0.000302250206142738, Validation Loss: 0.15373443698350658\n",
      "Epoch 2667/3000, Training Loss: 0.0003020677377063837, Validation Loss: 0.15374591570869875\n",
      "Epoch 2668/3000, Training Loss: 0.00030188555375761523, Validation Loss: 0.1537574731279567\n",
      "Epoch 2669/3000, Training Loss: 0.00030170351302948116, Validation Loss: 0.15376889157545326\n",
      "Epoch 2670/3000, Training Loss: 0.0003015216422473755, Validation Loss: 0.15378032862760696\n",
      "Epoch 2671/3000, Training Loss: 0.0003013400261377403, Validation Loss: 0.15379183990081088\n",
      "Epoch 2672/3000, Training Loss: 0.0003011586415935675, Validation Loss: 0.1538033035970188\n",
      "Epoch 2673/3000, Training Loss: 0.0003009773580904212, Validation Loss: 0.15381471918769915\n",
      "Epoch 2674/3000, Training Loss: 0.00030079629381045604, Validation Loss: 0.15382622660450032\n",
      "Epoch 2675/3000, Training Loss: 0.00030061554846194096, Validation Loss: 0.15383767677373122\n",
      "Epoch 2676/3000, Training Loss: 0.0003004348713145, Validation Loss: 0.1538490789065669\n",
      "Epoch 2677/3000, Training Loss: 0.00030025437236091886, Validation Loss: 0.15386048623801857\n",
      "Epoch 2678/3000, Training Loss: 0.0003000742488174994, Validation Loss: 0.15387199998652198\n",
      "Epoch 2679/3000, Training Loss: 0.00029989417181015056, Validation Loss: 0.15388337939022956\n",
      "Epoch 2680/3000, Training Loss: 0.0002997142654796857, Validation Loss: 0.15389473938313375\n",
      "Epoch 2681/3000, Training Loss: 0.0002995346647208277, Validation Loss: 0.15390627370901128\n",
      "Epoch 2682/3000, Training Loss: 0.0002993552501620607, Validation Loss: 0.15391765870863186\n",
      "Epoch 2683/3000, Training Loss: 0.00029917593952074966, Validation Loss: 0.15392900376444654\n",
      "Epoch 2684/3000, Training Loss: 0.00029899688568430405, Validation Loss: 0.153940526066162\n",
      "Epoch 2685/3000, Training Loss: 0.00029881810180256915, Validation Loss: 0.15395188636393267\n",
      "Epoch 2686/3000, Training Loss: 0.00029863938125560636, Validation Loss: 0.15396322937873355\n",
      "Epoch 2687/3000, Training Loss: 0.0002984608992020813, Validation Loss: 0.15397461642759822\n",
      "Epoch 2688/3000, Training Loss: 0.00029828270242474145, Validation Loss: 0.15398609596320206\n",
      "Epoch 2689/3000, Training Loss: 0.00029810458540701734, Validation Loss: 0.15399743452355993\n",
      "Epoch 2690/3000, Training Loss: 0.0002979266913670931, Validation Loss: 0.15400882033434843\n",
      "Epoch 2691/3000, Training Loss: 0.0002977490457965792, Validation Loss: 0.1540202673024732\n",
      "Epoch 2692/3000, Training Loss: 0.0002975715485500056, Validation Loss: 0.15403158453772048\n",
      "Epoch 2693/3000, Training Loss: 0.00029739423406520014, Validation Loss: 0.15404296645949742\n",
      "Epoch 2694/3000, Training Loss: 0.0002972171390323738, Validation Loss: 0.1540544074653698\n",
      "Epoch 2695/3000, Training Loss: 0.0002970402631839017, Validation Loss: 0.15406572084000117\n",
      "Epoch 2696/3000, Training Loss: 0.00029686351938961256, Validation Loss: 0.15407705236144437\n",
      "Epoch 2697/3000, Training Loss: 0.0002966869828277135, Validation Loss: 0.15408840687028635\n",
      "Epoch 2698/3000, Training Loss: 0.0002965106983320188, Validation Loss: 0.1540998152458717\n",
      "Epoch 2699/3000, Training Loss: 0.00029633453839893995, Validation Loss: 0.15411112542953292\n",
      "Epoch 2700/3000, Training Loss: 0.00029615857566902165, Validation Loss: 0.15412247433169518\n",
      "Epoch 2701/3000, Training Loss: 0.00029598283540631815, Validation Loss: 0.15413386130296808\n",
      "Epoch 2702/3000, Training Loss: 0.00029580729022530015, Validation Loss: 0.15414516604380063\n",
      "Epoch 2703/3000, Training Loss: 0.0002956318875677275, Validation Loss: 0.15415650300887598\n",
      "Epoch 2704/3000, Training Loss: 0.0002954566930678357, Validation Loss: 0.15416787437044904\n",
      "Epoch 2705/3000, Training Loss: 0.0002952817498294416, Validation Loss: 0.15417916692002656\n",
      "Epoch 2706/3000, Training Loss: 0.0002951069142289596, Validation Loss: 0.154190478809761\n",
      "Epoch 2707/3000, Training Loss: 0.0002949322785614736, Validation Loss: 0.15420174719852464\n",
      "Epoch 2708/3000, Training Loss: 0.0002947578959288036, Validation Loss: 0.1542130990985598\n",
      "Epoch 2709/3000, Training Loss: 0.00029458365527488146, Validation Loss: 0.15422440879871818\n",
      "Epoch 2710/3000, Training Loss: 0.000294409581037712, Validation Loss: 0.15423565359169533\n",
      "Epoch 2711/3000, Training Loss: 0.0002942357182793702, Validation Loss: 0.15424701943177296\n",
      "Epoch 2712/3000, Training Loss: 0.0002940620852647285, Validation Loss: 0.15425830752303532\n",
      "Epoch 2713/3000, Training Loss: 0.0002938886092510625, Validation Loss: 0.1542695725446531\n",
      "Epoch 2714/3000, Training Loss: 0.00029371525744368334, Validation Loss: 0.1542808438137138\n",
      "Epoch 2715/3000, Training Loss: 0.00029354222957406987, Validation Loss: 0.15429221920615527\n",
      "Epoch 2716/3000, Training Loss: 0.0002933693307091423, Validation Loss: 0.15430348128952082\n",
      "Epoch 2717/3000, Training Loss: 0.0002931965379412546, Validation Loss: 0.1543147644130784\n",
      "Epoch 2718/3000, Training Loss: 0.00029302403116609354, Validation Loss: 0.15432612770170345\n",
      "Epoch 2719/3000, Training Loss: 0.0002928517295181464, Validation Loss: 0.15433737427818225\n",
      "Epoch 2720/3000, Training Loss: 0.00029267949400252173, Validation Loss: 0.1543486087989242\n",
      "Epoch 2721/3000, Training Loss: 0.0002925074977983373, Validation Loss: 0.15435996624778942\n",
      "Epoch 2722/3000, Training Loss: 0.0002923357986916379, Validation Loss: 0.1543711999091486\n",
      "Epoch 2723/3000, Training Loss: 0.0002921641231729625, Validation Loss: 0.15438241822923648\n",
      "Epoch 2724/3000, Training Loss: 0.00029199264449813187, Validation Loss: 0.15439367967229467\n",
      "Epoch 2725/3000, Training Loss: 0.0002918215307473809, Validation Loss: 0.154404971938469\n",
      "Epoch 2726/3000, Training Loss: 0.000291650415922033, Validation Loss: 0.15441618788068562\n",
      "Epoch 2727/3000, Training Loss: 0.0002914794732610208, Validation Loss: 0.15442738840571202\n",
      "Epoch 2728/3000, Training Loss: 0.0002913088855436391, Validation Loss: 0.1544387145495978\n",
      "Epoch 2729/3000, Training Loss: 0.0002911383561575391, Validation Loss: 0.15444991538336555\n",
      "Epoch 2730/3000, Training Loss: 0.00029096796680079084, Validation Loss: 0.15446110411715128\n",
      "Epoch 2731/3000, Training Loss: 0.0002907978821981945, Validation Loss: 0.15447241503919773\n",
      "Epoch 2732/3000, Training Loss: 0.00029062794187702603, Validation Loss: 0.15448360408685877\n",
      "Epoch 2733/3000, Training Loss: 0.0002904581032077622, Validation Loss: 0.15449477775932852\n",
      "Epoch 2734/3000, Training Loss: 0.0002902885301327444, Validation Loss: 0.15450599429117529\n",
      "Epoch 2735/3000, Training Loss: 0.00029011916407582024, Validation Loss: 0.15451724160187413\n",
      "Epoch 2736/3000, Training Loss: 0.00028994987321069306, Validation Loss: 0.15452841294948044\n",
      "Epoch 2737/3000, Training Loss: 0.00028978082589395794, Validation Loss: 0.15453957973747237\n",
      "Epoch 2738/3000, Training Loss: 0.0002896119801617653, Validation Loss: 0.15455085001383895\n",
      "Epoch 2739/3000, Training Loss: 0.0002894432688923404, Validation Loss: 0.15456200634228978\n",
      "Epoch 2740/3000, Training Loss: 0.0002892747575788535, Validation Loss: 0.15457316140664318\n",
      "Epoch 2741/3000, Training Loss: 0.00028910642456520734, Validation Loss: 0.15458441819767013\n",
      "Epoch 2742/3000, Training Loss: 0.00028893828635625704, Validation Loss: 0.15459555183206114\n",
      "Epoch 2743/3000, Training Loss: 0.0002887703053655117, Validation Loss: 0.15460670292205533\n",
      "Epoch 2744/3000, Training Loss: 0.00028860250088862756, Validation Loss: 0.15461785306463477\n",
      "Epoch 2745/3000, Training Loss: 0.00028843490204392877, Validation Loss: 0.15462906678552335\n",
      "Epoch 2746/3000, Training Loss: 0.00028826746130424305, Validation Loss: 0.15464020291158065\n",
      "Epoch 2747/3000, Training Loss: 0.00028810019626035206, Validation Loss: 0.15465131619551517\n",
      "Epoch 2748/3000, Training Loss: 0.0002879331118531923, Validation Loss: 0.1546625401960299\n",
      "Epoch 2749/3000, Training Loss: 0.0002877662214472394, Validation Loss: 0.1546736553876676\n",
      "Epoch 2750/3000, Training Loss: 0.0002875994985270419, Validation Loss: 0.15468476320631946\n",
      "Epoch 2751/3000, Training Loss: 0.00028743291506027913, Validation Loss: 0.15469596602871347\n",
      "Epoch 2752/3000, Training Loss: 0.00028726658365340696, Validation Loss: 0.1547070756438977\n",
      "Epoch 2753/3000, Training Loss: 0.0002871003913906583, Validation Loss: 0.15471817183365869\n",
      "Epoch 2754/3000, Training Loss: 0.00028693432609749805, Validation Loss: 0.15472925267612947\n",
      "Epoch 2755/3000, Training Loss: 0.00028676850705844305, Validation Loss: 0.15474040765442873\n",
      "Epoch 2756/3000, Training Loss: 0.00028660287787029035, Validation Loss: 0.15475145474670443\n",
      "Epoch 2757/3000, Training Loss: 0.0002864373319175033, Validation Loss: 0.15476253356909603\n",
      "Epoch 2758/3000, Training Loss: 0.00028627202186505026, Validation Loss: 0.15477363055344837\n",
      "Epoch 2759/3000, Training Loss: 0.0002861069260793762, Validation Loss: 0.15478470982474615\n",
      "Epoch 2760/3000, Training Loss: 0.00028594191847197383, Validation Loss: 0.15479574901067525\n",
      "Epoch 2761/3000, Training Loss: 0.0002857771231857925, Validation Loss: 0.15480685977658432\n",
      "Epoch 2762/3000, Training Loss: 0.00028561255716299605, Validation Loss: 0.15481792446946271\n",
      "Epoch 2763/3000, Training Loss: 0.00028544807788750677, Validation Loss: 0.15482894324387397\n",
      "Epoch 2764/3000, Training Loss: 0.0002852837887054012, Validation Loss: 0.15483996712427772\n",
      "Epoch 2765/3000, Training Loss: 0.0002851197299052021, Validation Loss: 0.15485109292486396\n",
      "Epoch 2766/3000, Training Loss: 0.0002849558057468603, Validation Loss: 0.15486210641937767\n",
      "Epoch 2767/3000, Training Loss: 0.00028479201403488905, Validation Loss: 0.1548731189789092\n",
      "Epoch 2768/3000, Training Loss: 0.00028462846442918054, Validation Loss: 0.15488423007056995\n",
      "Epoch 2769/3000, Training Loss: 0.00028446507985376793, Validation Loss: 0.1548952322920234\n",
      "Epoch 2770/3000, Training Loss: 0.00028430179346751206, Validation Loss: 0.1549061962357985\n",
      "Epoch 2771/3000, Training Loss: 0.0002841387238322128, Validation Loss: 0.1549172152237095\n",
      "Epoch 2772/3000, Training Loss: 0.00028397590375296526, Validation Loss: 0.15492830907584493\n",
      "Epoch 2773/3000, Training Loss: 0.000283813134246541, Validation Loss: 0.15493927109071193\n",
      "Epoch 2774/3000, Training Loss: 0.000283650562596662, Validation Loss: 0.1549502756154884\n",
      "Epoch 2775/3000, Training Loss: 0.0002834882434678987, Validation Loss: 0.15496137583855837\n",
      "Epoch 2776/3000, Training Loss: 0.0002833260049143081, Validation Loss: 0.1549723250243185\n",
      "Epoch 2777/3000, Training Loss: 0.0002831639386519896, Validation Loss: 0.15498330029275154\n",
      "Epoch 2778/3000, Training Loss: 0.00028300210038468133, Validation Loss: 0.15499442158913973\n",
      "Epoch 2779/3000, Training Loss: 0.0002828404068341261, Validation Loss: 0.15500535640168495\n",
      "Epoch 2780/3000, Training Loss: 0.00028267885576016777, Validation Loss: 0.1550163202613146\n",
      "Epoch 2781/3000, Training Loss: 0.0002825174959421538, Validation Loss: 0.15502734603466997\n",
      "Epoch 2782/3000, Training Loss: 0.0002823563285643787, Validation Loss: 0.15503835909108063\n",
      "Epoch 2783/3000, Training Loss: 0.0002821952957802118, Validation Loss: 0.15504929994618996\n",
      "Epoch 2784/3000, Training Loss: 0.00028203442255056975, Validation Loss: 0.15506030520858077\n",
      "Epoch 2785/3000, Training Loss: 0.00028187374744260686, Validation Loss: 0.15507133589363647\n",
      "Epoch 2786/3000, Training Loss: 0.0002817132441346027, Validation Loss: 0.15508224236770174\n",
      "Epoch 2787/3000, Training Loss: 0.00028155286453827275, Validation Loss: 0.15509323309001233\n",
      "Epoch 2788/3000, Training Loss: 0.0002813926822353507, Validation Loss: 0.15510425242940123\n",
      "Epoch 2789/3000, Training Loss: 0.00028123269901225674, Validation Loss: 0.15511514450871133\n",
      "Epoch 2790/3000, Training Loss: 0.0002810728158596044, Validation Loss: 0.15512609920944065\n",
      "Epoch 2791/3000, Training Loss: 0.0002809131470499239, Validation Loss: 0.1551370162790496\n",
      "Epoch 2792/3000, Training Loss: 0.0002807536488257178, Validation Loss: 0.15514801858509603\n",
      "Epoch 2793/3000, Training Loss: 0.0002805942702179944, Validation Loss: 0.1551589502605433\n",
      "Epoch 2794/3000, Training Loss: 0.0002804350934836727, Validation Loss: 0.15516985614366802\n",
      "Epoch 2795/3000, Training Loss: 0.00028027608087789394, Validation Loss: 0.15518082642483744\n",
      "Epoch 2796/3000, Training Loss: 0.0002801172144624538, Validation Loss: 0.15519174694481802\n",
      "Epoch 2797/3000, Training Loss: 0.00027995852976552487, Validation Loss: 0.155202638519745\n",
      "Epoch 2798/3000, Training Loss: 0.00027980000146469677, Validation Loss: 0.15521356802799846\n",
      "Epoch 2799/3000, Training Loss: 0.00027964165475353786, Validation Loss: 0.1552244363284618\n",
      "Epoch 2800/3000, Training Loss: 0.00027948344991919995, Validation Loss: 0.15523528817276042\n",
      "Epoch 2801/3000, Training Loss: 0.0002793254275270599, Validation Loss: 0.15524612356047415\n",
      "Epoch 2802/3000, Training Loss: 0.0002791675416456869, Validation Loss: 0.15525707885823242\n",
      "Epoch 2803/3000, Training Loss: 0.0002790098486165957, Validation Loss: 0.15526792204315074\n",
      "Epoch 2804/3000, Training Loss: 0.0002788523127957301, Validation Loss: 0.15527872381702668\n",
      "Epoch 2805/3000, Training Loss: 0.00027869490956585423, Validation Loss: 0.15528962275317154\n",
      "Epoch 2806/3000, Training Loss: 0.00027853774912498045, Validation Loss: 0.1553004413040592\n",
      "Epoch 2807/3000, Training Loss: 0.0002783806738421429, Validation Loss: 0.1553112858030907\n",
      "Epoch 2808/3000, Training Loss: 0.0002782237490258085, Validation Loss: 0.15532209423695484\n",
      "Epoch 2809/3000, Training Loss: 0.00027806707383007563, Validation Loss: 0.15533299624905783\n",
      "Epoch 2810/3000, Training Loss: 0.00027791049203790567, Validation Loss: 0.15534381258906468\n",
      "Epoch 2811/3000, Training Loss: 0.0002777540656620199, Validation Loss: 0.15535460716566105\n",
      "Epoch 2812/3000, Training Loss: 0.00027759784501899264, Validation Loss: 0.15536550582944403\n",
      "Epoch 2813/3000, Training Loss: 0.0002774417838637235, Validation Loss: 0.15537630026291674\n",
      "Epoch 2814/3000, Training Loss: 0.0002772858291213615, Validation Loss: 0.15538713408893037\n",
      "Epoch 2815/3000, Training Loss: 0.00027713007405067596, Validation Loss: 0.15539802020886695\n",
      "Epoch 2816/3000, Training Loss: 0.0002769745190033845, Validation Loss: 0.15540880222165585\n",
      "Epoch 2817/3000, Training Loss: 0.0002768190359975732, Validation Loss: 0.15541962358170547\n",
      "Epoch 2818/3000, Training Loss: 0.0002766637533632336, Validation Loss: 0.15543041610951924\n",
      "Epoch 2819/3000, Training Loss: 0.0002765086817974944, Validation Loss: 0.15544128428932316\n",
      "Epoch 2820/3000, Training Loss: 0.00027635368396777333, Validation Loss: 0.15545206525047453\n",
      "Epoch 2821/3000, Training Loss: 0.0002761988820531982, Validation Loss: 0.1554628560869015\n",
      "Epoch 2822/3000, Training Loss: 0.00027604427630383694, Validation Loss: 0.15547370108716863\n",
      "Epoch 2823/3000, Training Loss: 0.00027588977135428413, Validation Loss: 0.1554844559097516\n",
      "Epoch 2824/3000, Training Loss: 0.00027573544599615444, Validation Loss: 0.15549525716972573\n",
      "Epoch 2825/3000, Training Loss: 0.00027558129948327954, Validation Loss: 0.15550609129431522\n",
      "Epoch 2826/3000, Training Loss: 0.0002754272862599628, Validation Loss: 0.15551683218672677\n",
      "Epoch 2827/3000, Training Loss: 0.0002752734200240834, Validation Loss: 0.15552761365582468\n",
      "Epoch 2828/3000, Training Loss: 0.00027511975434862886, Validation Loss: 0.1555383636231782\n",
      "Epoch 2829/3000, Training Loss: 0.00027496620647236484, Validation Loss: 0.15554918134399562\n",
      "Epoch 2830/3000, Training Loss: 0.00027481281833463837, Validation Loss: 0.15555990773345554\n",
      "Epoch 2831/3000, Training Loss: 0.0002746596173650303, Validation Loss: 0.15557067987565268\n",
      "Epoch 2832/3000, Training Loss: 0.0002745065385894842, Validation Loss: 0.15558148360643387\n",
      "Epoch 2833/3000, Training Loss: 0.0002743536255213167, Validation Loss: 0.15559219928707477\n",
      "Epoch 2834/3000, Training Loss: 0.00027420090178779786, Validation Loss: 0.15560294865020086\n",
      "Epoch 2835/3000, Training Loss: 0.0002740482719978072, Validation Loss: 0.15561367155711076\n",
      "Epoch 2836/3000, Training Loss: 0.00027389584317949017, Validation Loss: 0.15562443646835603\n",
      "Epoch 2837/3000, Training Loss: 0.0002737435705700237, Validation Loss: 0.15563518811238533\n",
      "Epoch 2838/3000, Training Loss: 0.00027359143112363403, Validation Loss: 0.15564589418510208\n",
      "Epoch 2839/3000, Training Loss: 0.0002734394507816184, Validation Loss: 0.15565664538694035\n",
      "Epoch 2840/3000, Training Loss: 0.00027328764897517675, Validation Loss: 0.15566738317277887\n",
      "Epoch 2841/3000, Training Loss: 0.00027313598941846093, Validation Loss: 0.1556780950847302\n",
      "Epoch 2842/3000, Training Loss: 0.0002729844452308081, Validation Loss: 0.15568884125678698\n",
      "Epoch 2843/3000, Training Loss: 0.00027283312161640975, Validation Loss: 0.15569954408188527\n",
      "Epoch 2844/3000, Training Loss: 0.0002726819427735745, Validation Loss: 0.15571024085943333\n",
      "Epoch 2845/3000, Training Loss: 0.0002725308369370493, Validation Loss: 0.1557208978128694\n",
      "Epoch 2846/3000, Training Loss: 0.0002723799852543758, Validation Loss: 0.1557316737456307\n",
      "Epoch 2847/3000, Training Loss: 0.0002722292662958762, Validation Loss: 0.1557423517679967\n",
      "Epoch 2848/3000, Training Loss: 0.00027207862423084007, Validation Loss: 0.15575298601731521\n",
      "Epoch 2849/3000, Training Loss: 0.0002719282060262082, Validation Loss: 0.15576372760849402\n",
      "Epoch 2850/3000, Training Loss: 0.00027177795310279814, Validation Loss: 0.15577442451549414\n",
      "Epoch 2851/3000, Training Loss: 0.00027162779005925915, Validation Loss: 0.15578507036448486\n",
      "Epoch 2852/3000, Training Loss: 0.0002714778189171965, Validation Loss: 0.15579580977470772\n",
      "Epoch 2853/3000, Training Loss: 0.00027132802905530025, Validation Loss: 0.15580646186473196\n",
      "Epoch 2854/3000, Training Loss: 0.00027117832482430386, Validation Loss: 0.15581709557825157\n",
      "Epoch 2855/3000, Training Loss: 0.00027102879005331274, Validation Loss: 0.1558277341509996\n",
      "Epoch 2856/3000, Training Loss: 0.00027087945205829024, Validation Loss: 0.15583847106949955\n",
      "Epoch 2857/3000, Training Loss: 0.00027073021972447104, Validation Loss: 0.15584908303708236\n",
      "Epoch 2858/3000, Training Loss: 0.0002705811338289471, Validation Loss: 0.15585971095870918\n",
      "Epoch 2859/3000, Training Loss: 0.00027043223021751386, Validation Loss: 0.15587041013150174\n",
      "Epoch 2860/3000, Training Loss: 0.0002702834702005106, Validation Loss: 0.1558810135090332\n",
      "Epoch 2861/3000, Training Loss: 0.0002701348391187646, Validation Loss: 0.155891617689039\n",
      "Epoch 2862/3000, Training Loss: 0.0002699863424450624, Validation Loss: 0.15590232985915733\n",
      "Epoch 2863/3000, Training Loss: 0.0002698380691356728, Validation Loss: 0.15591291965977425\n",
      "Epoch 2864/3000, Training Loss: 0.0002696898903244172, Validation Loss: 0.15592352218732128\n",
      "Epoch 2865/3000, Training Loss: 0.00026954183418452277, Validation Loss: 0.1559341427691743\n",
      "Epoch 2866/3000, Training Loss: 0.0002693939842234339, Validation Loss: 0.15594481705444613\n",
      "Epoch 2867/3000, Training Loss: 0.00026924628350906275, Validation Loss: 0.1559553890043762\n",
      "Epoch 2868/3000, Training Loss: 0.00026909867238775366, Validation Loss: 0.15596599906733058\n",
      "Epoch 2869/3000, Training Loss: 0.0002689512413382426, Validation Loss: 0.1559766270309656\n",
      "Epoch 2870/3000, Training Loss: 0.00026880400690594997, Validation Loss: 0.15598721258117207\n",
      "Epoch 2871/3000, Training Loss: 0.00026865684331909276, Validation Loss: 0.15599781775571633\n",
      "Epoch 2872/3000, Training Loss: 0.0002685098421171658, Validation Loss: 0.15600843521959099\n",
      "Epoch 2873/3000, Training Loss: 0.00026836305931356467, Validation Loss: 0.15601902408491833\n",
      "Epoch 2874/3000, Training Loss: 0.0002682163425476818, Validation Loss: 0.15602961871545987\n",
      "Epoch 2875/3000, Training Loss: 0.0002680697889502732, Validation Loss: 0.15604016695650655\n",
      "Epoch 2876/3000, Training Loss: 0.0002679234260245741, Validation Loss: 0.15605079213656275\n",
      "Epoch 2877/3000, Training Loss: 0.00026777717357345847, Validation Loss: 0.1560613563266252\n",
      "Epoch 2878/3000, Training Loss: 0.0002676310571169513, Validation Loss: 0.15607190295091994\n",
      "Epoch 2879/3000, Training Loss: 0.00026748510612393384, Validation Loss: 0.15608251455839864\n",
      "Epoch 2880/3000, Training Loss: 0.0002673393165355221, Validation Loss: 0.15609303629114196\n",
      "Epoch 2881/3000, Training Loss: 0.0002671936536120627, Validation Loss: 0.15610357966731067\n",
      "Epoch 2882/3000, Training Loss: 0.00026704811039755984, Validation Loss: 0.1561141018290808\n",
      "Epoch 2883/3000, Training Loss: 0.00026690274747413273, Validation Loss: 0.15612471947662737\n",
      "Epoch 2884/3000, Training Loss: 0.0002667575486855554, Validation Loss: 0.15613523554495579\n",
      "Epoch 2885/3000, Training Loss: 0.0002666124398935532, Validation Loss: 0.15614575301781985\n",
      "Epoch 2886/3000, Training Loss: 0.0002664674873282056, Validation Loss: 0.15615633701275847\n",
      "Epoch 2887/3000, Training Loss: 0.0002663227432504353, Validation Loss: 0.15616681654827355\n",
      "Epoch 2888/3000, Training Loss: 0.000266178076486591, Validation Loss: 0.156177300647844\n",
      "Epoch 2889/3000, Training Loss: 0.00026603353582251607, Validation Loss: 0.15618784771087244\n",
      "Epoch 2890/3000, Training Loss: 0.0002658892450041938, Validation Loss: 0.15619834086960036\n",
      "Epoch 2891/3000, Training Loss: 0.0002657450129983492, Validation Loss: 0.15620880308815327\n",
      "Epoch 2892/3000, Training Loss: 0.00026560088991812926, Validation Loss: 0.15621927180170725\n",
      "Epoch 2893/3000, Training Loss: 0.00026545701730754475, Validation Loss: 0.15622983700064547\n",
      "Epoch 2894/3000, Training Loss: 0.00026531324480267264, Validation Loss: 0.15624029772616657\n",
      "Epoch 2895/3000, Training Loss: 0.000265169544991837, Validation Loss: 0.15625075324485355\n",
      "Epoch 2896/3000, Training Loss: 0.00026502607050965017, Validation Loss: 0.15626130822683787\n",
      "Epoch 2897/3000, Training Loss: 0.0002648827563605733, Validation Loss: 0.1562717390957577\n",
      "Epoch 2898/3000, Training Loss: 0.0002647394857608833, Validation Loss: 0.15628217588354748\n",
      "Epoch 2899/3000, Training Loss: 0.00026459639777396164, Validation Loss: 0.15629272636132596\n",
      "Epoch 2900/3000, Training Loss: 0.00026445354784245613, Validation Loss: 0.15630314721944222\n",
      "Epoch 2901/3000, Training Loss: 0.0002643107061014368, Validation Loss: 0.15631357091883452\n",
      "Epoch 2902/3000, Training Loss: 0.00026416802724260635, Validation Loss: 0.15632401113223623\n",
      "Epoch 2903/3000, Training Loss: 0.00026402559302075824, Validation Loss: 0.15633452723292154\n",
      "Epoch 2904/3000, Training Loss: 0.00026388319910954485, Validation Loss: 0.15634493293821541\n",
      "Epoch 2905/3000, Training Loss: 0.0002637410044566186, Validation Loss: 0.15635537815897224\n",
      "Epoch 2906/3000, Training Loss: 0.0002635990652924008, Validation Loss: 0.15636598256680698\n",
      "Epoch 2907/3000, Training Loss: 0.0002634572711510166, Validation Loss: 0.1563764327199953\n",
      "Epoch 2908/3000, Training Loss: 0.0002633155473768573, Validation Loss: 0.15638686690444115\n",
      "Epoch 2909/3000, Training Loss: 0.00026317403897832136, Validation Loss: 0.15639743377325296\n",
      "Epoch 2910/3000, Training Loss: 0.0002630326576976561, Validation Loss: 0.1564078965938559\n",
      "Epoch 2911/3000, Training Loss: 0.0002628913533340992, Validation Loss: 0.15641839321626108\n",
      "Epoch 2912/3000, Training Loss: 0.0002627502752730248, Validation Loss: 0.15642885608909798\n",
      "Epoch 2913/3000, Training Loss: 0.0002626092862697476, Validation Loss: 0.15643932165805585\n",
      "Epoch 2914/3000, Training Loss: 0.00026246842593624837, Validation Loss: 0.15644980741324774\n",
      "Epoch 2915/3000, Training Loss: 0.00026232774114856664, Validation Loss: 0.1564602564290944\n",
      "Epoch 2916/3000, Training Loss: 0.0002621871638698245, Validation Loss: 0.15647071129892004\n",
      "Epoch 2917/3000, Training Loss: 0.0002620467454391185, Validation Loss: 0.15648118306578293\n",
      "Epoch 2918/3000, Training Loss: 0.0002619064522722542, Validation Loss: 0.156491621304436\n",
      "Epoch 2919/3000, Training Loss: 0.0002617662849597202, Validation Loss: 0.1565020458493589\n",
      "Epoch 2920/3000, Training Loss: 0.0002616263069423844, Validation Loss: 0.15651250685965976\n",
      "Epoch 2921/3000, Training Loss: 0.0002614864034798784, Validation Loss: 0.15652289996165364\n",
      "Epoch 2922/3000, Training Loss: 0.00026134666275110435, Validation Loss: 0.15653339299028887\n",
      "Epoch 2923/3000, Training Loss: 0.0002612071040698374, Validation Loss: 0.15654380113101427\n",
      "Epoch 2924/3000, Training Loss: 0.0002610675885471241, Validation Loss: 0.15655419148450742\n",
      "Epoch 2925/3000, Training Loss: 0.00026092827722608655, Validation Loss: 0.15656466779654143\n",
      "Epoch 2926/3000, Training Loss: 0.0002607891073000076, Validation Loss: 0.1565750623891615\n",
      "Epoch 2927/3000, Training Loss: 0.000260650010646812, Validation Loss: 0.156585438518002\n",
      "Epoch 2928/3000, Training Loss: 0.00026051112301978755, Validation Loss: 0.15659590407074825\n",
      "Epoch 2929/3000, Training Loss: 0.00026037233942052617, Validation Loss: 0.15660628743161037\n",
      "Epoch 2930/3000, Training Loss: 0.0002602336670618915, Validation Loss: 0.15661672851556233\n",
      "Epoch 2931/3000, Training Loss: 0.00026009519013398593, Validation Loss: 0.15662709038776762\n",
      "Epoch 2932/3000, Training Loss: 0.00025995679865895255, Validation Loss: 0.1566374563839957\n",
      "Epoch 2933/3000, Training Loss: 0.00025981854695250394, Validation Loss: 0.15664790468521386\n",
      "Epoch 2934/3000, Training Loss: 0.0002596804568612717, Validation Loss: 0.15665825598618577\n",
      "Epoch 2935/3000, Training Loss: 0.00025954247564129277, Validation Loss: 0.15666860837949975\n",
      "Epoch 2936/3000, Training Loss: 0.0002594046387664428, Validation Loss: 0.15667904831354498\n",
      "Epoch 2937/3000, Training Loss: 0.0002592669355309144, Validation Loss: 0.15668938604411986\n",
      "Epoch 2938/3000, Training Loss: 0.0002591293629103134, Validation Loss: 0.15669972788611483\n",
      "Epoch 2939/3000, Training Loss: 0.0002589919391544107, Validation Loss: 0.1567101355597247\n",
      "Epoch 2940/3000, Training Loss: 0.00025885462110134916, Validation Loss: 0.15672048395413232\n",
      "Epoch 2941/3000, Training Loss: 0.0002587174522177018, Validation Loss: 0.15673086626612456\n",
      "Epoch 2942/3000, Training Loss: 0.0002585804266164477, Validation Loss: 0.15674117564144088\n",
      "Epoch 2943/3000, Training Loss: 0.0002584435095189455, Validation Loss: 0.15675152030272557\n",
      "Epoch 2944/3000, Training Loss: 0.0002583067835109464, Validation Loss: 0.15676191103741827\n",
      "Epoch 2945/3000, Training Loss: 0.00025817011828566116, Validation Loss: 0.15677220693576427\n",
      "Epoch 2946/3000, Training Loss: 0.000258033586685352, Validation Loss: 0.15678251008080882\n",
      "Epoch 2947/3000, Training Loss: 0.00025789727596252186, Validation Loss: 0.15679292064957906\n",
      "Epoch 2948/3000, Training Loss: 0.0002577610032914032, Validation Loss: 0.15680320610950155\n",
      "Epoch 2949/3000, Training Loss: 0.00025762487024515927, Validation Loss: 0.1568135686563315\n",
      "Epoch 2950/3000, Training Loss: 0.0002574889633119155, Validation Loss: 0.15682387705083525\n",
      "Epoch 2951/3000, Training Loss: 0.0002573530767631034, Validation Loss: 0.15683416541166414\n",
      "Epoch 2952/3000, Training Loss: 0.00025721734841865505, Validation Loss: 0.15684451975498273\n",
      "Epoch 2953/3000, Training Loss: 0.0002570818214171234, Validation Loss: 0.15685481471136306\n",
      "Epoch 2954/3000, Training Loss: 0.0002569463336691474, Validation Loss: 0.15686508749080097\n",
      "Epoch 2955/3000, Training Loss: 0.0002568110123573094, Validation Loss: 0.1568754024971654\n",
      "Epoch 2956/3000, Training Loss: 0.00025667584027173516, Validation Loss: 0.15688570725497913\n",
      "Epoch 2957/3000, Training Loss: 0.0002565407722093203, Validation Loss: 0.15689596899228841\n",
      "Epoch 2958/3000, Training Loss: 0.000256405860486802, Validation Loss: 0.156906273589139\n",
      "Epoch 2959/3000, Training Loss: 0.0002562710488299375, Validation Loss: 0.15691656267737558\n",
      "Epoch 2960/3000, Training Loss: 0.0002561364031768199, Validation Loss: 0.1569268887909435\n",
      "Epoch 2961/3000, Training Loss: 0.0002560018608877191, Validation Loss: 0.15693709742095072\n",
      "Epoch 2962/3000, Training Loss: 0.000255867425471972, Validation Loss: 0.1569473326366487\n",
      "Epoch 2963/3000, Training Loss: 0.0002557331995507179, Validation Loss: 0.15695767808657599\n",
      "Epoch 2964/3000, Training Loss: 0.00025559906037718867, Validation Loss: 0.15696789528358068\n",
      "Epoch 2965/3000, Training Loss: 0.00025546498241793786, Validation Loss: 0.15697811724221175\n",
      "Epoch 2966/3000, Training Loss: 0.0002553311494805315, Validation Loss: 0.1569884732443254\n",
      "Epoch 2967/3000, Training Loss: 0.00025519735921387027, Validation Loss: 0.15699865844493888\n",
      "Epoch 2968/3000, Training Loss: 0.00025506371363927857, Validation Loss: 0.15700894460912712\n",
      "Epoch 2969/3000, Training Loss: 0.00025493025069359907, Validation Loss: 0.1570191941831749\n",
      "Epoch 2970/3000, Training Loss: 0.00025479684673817134, Validation Loss: 0.15702938537858838\n",
      "Epoch 2971/3000, Training Loss: 0.000254663605210599, Validation Loss: 0.15703965823565522\n",
      "Epoch 2972/3000, Training Loss: 0.0002545304950320828, Validation Loss: 0.15704989994488305\n",
      "Epoch 2973/3000, Training Loss: 0.0002543974865335907, Validation Loss: 0.15706007788143822\n",
      "Epoch 2974/3000, Training Loss: 0.0002542646474590696, Validation Loss: 0.15707033815058596\n",
      "Epoch 2975/3000, Training Loss: 0.00025413189255715156, Validation Loss: 0.1570805438557795\n",
      "Epoch 2976/3000, Training Loss: 0.0002539992916254384, Validation Loss: 0.15709080867569383\n",
      "Epoch 2977/3000, Training Loss: 0.00025386683178296396, Validation Loss: 0.1571009759432473\n",
      "Epoch 2978/3000, Training Loss: 0.00025373443327298697, Validation Loss: 0.15711117917890968\n",
      "Epoch 2979/3000, Training Loss: 0.0002536022327411213, Validation Loss: 0.1571214256666612\n",
      "Epoch 2980/3000, Training Loss: 0.00025347014410983675, Validation Loss: 0.15713158000189004\n",
      "Epoch 2981/3000, Training Loss: 0.00025333812123902113, Validation Loss: 0.15714176730150867\n",
      "Epoch 2982/3000, Training Loss: 0.0002532063143827795, Validation Loss: 0.15715200597014284\n",
      "Epoch 2983/3000, Training Loss: 0.0002530745941601244, Validation Loss: 0.1571621471651736\n",
      "Epoch 2984/3000, Training Loss: 0.00025294294456054434, Validation Loss: 0.15717232207124593\n",
      "Epoch 2985/3000, Training Loss: 0.0002528115293132651, Validation Loss: 0.15718254759795186\n",
      "Epoch 2986/3000, Training Loss: 0.0002526801761307911, Validation Loss: 0.1571926787478553\n",
      "Epoch 2987/3000, Training Loss: 0.0002525489148908697, Validation Loss: 0.1572029167350928\n",
      "Epoch 2988/3000, Training Loss: 0.0002524178623292614, Validation Loss: 0.15721303543625847\n",
      "Epoch 2989/3000, Training Loss: 0.0002522868723829107, Validation Loss: 0.15722317405354372\n",
      "Epoch 2990/3000, Training Loss: 0.000252156014827514, Validation Loss: 0.15723337149417835\n",
      "Epoch 2991/3000, Training Loss: 0.0002520253354805883, Validation Loss: 0.15724349170636087\n",
      "Epoch 2992/3000, Training Loss: 0.0002518946959430776, Validation Loss: 0.15725363869289954\n",
      "Epoch 2993/3000, Training Loss: 0.0002517642375755451, Validation Loss: 0.15726382296956354\n",
      "Epoch 2994/3000, Training Loss: 0.00025163390717002917, Validation Loss: 0.15727393319185004\n",
      "Epoch 2995/3000, Training Loss: 0.00025150364356507144, Validation Loss: 0.1572841385533787\n",
      "Epoch 2996/3000, Training Loss: 0.00025137356961937114, Validation Loss: 0.15729424730555674\n",
      "Epoch 2997/3000, Training Loss: 0.00025124358866678357, Validation Loss: 0.15730435220864156\n",
      "Epoch 2998/3000, Training Loss: 0.0002511137231460983, Validation Loss: 0.15731452711449903\n",
      "Epoch 2999/3000, Training Loss: 0.00025098399867113833, Validation Loss: 0.15732460897989367\n",
      "Epoch 3000/3000, Training Loss: 0.0002508543756793453, Validation Loss: 0.15733472243944346\n"
     ]
    }
   ],
   "source": [
    "# nn_arch = [\n",
    "#     {'input_dim': 64, 'output_dim': 32, 'activation': 'relu'},\n",
    "#     {'input_dim': 32, 'output_dim': 10, 'activation': 'sigmoid'}  # Assuming a sigmoid output layer for multi-class classification\n",
    "# ]\n",
    "nn_arch = [{'input_dim': 68, 'output_dim': 32, 'activation': 'relu'},\n",
    "                    {'input_dim': 32, 'output_dim': 16, 'activation': 'sigmoid'},\n",
    "                    {'input_dim': 16, 'output_dim': 32, 'activation': 'sigmoid'},\n",
    "                    {'input_dim': 32, 'output_dim': 68, 'activation': 'relu'},\n",
    "                    {'input_dim': 68, 'output_dim': 1, 'activation': 'sigmoid'}]\n",
    "\n",
    "# Instantiate the neural network\n",
    "nn = NeuralNetwork(nn_arch, lr=0.1, seed=57, batch_size=32, epochs=3000, loss_function='binary_cross_entropy')\n",
    "\n",
    "# Train the neural network\n",
    "train_loss, val_loss = nn.fit(X_train.T, y_train, X_val.T, y_val)  # Note: .T is used to transpose the matrices to match the expected shape\n",
    "\n",
    "# Predictions (Optional: evaluate the model's performance)\n",
    "y_hat_val = nn.predict(X_val.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABUg0lEQVR4nO3deXgUVboG8Ld6X5J0ErICSQiyExZNFAOiIhjAZUScC6PIoqBGlrmIOoqMgsw4OCoY5yqMKIuMC+igXmZAIYygSPCKLIKCiAokQoeQkKSzdifd5/5RSSdNFrJ0Up3O+3ueerqq+lT112XP5OXUqSpJCCFARERE5CdUShdARERE5E0MN0RERORXGG6IiIjIrzDcEBERkV9huCEiIiK/wnBDREREfoXhhoiIiPyKRukC2pvL5cK5c+cQGBgISZKULoeIiIiaQAiBoqIidO3aFSpV430znS7cnDt3DjExMUqXQURERC2QlZWF7t27N9qm04WbwMBAAPLBCQoKUrgaIiIiagqbzYaYmBj33/HGdLpwU30qKigoiOGGiIiog2nKkBIOKCYiIiK/wnBDREREfoXhhoiIiPxKpxtzQ0REred0OlFRUaF0GeRndDrdZS/zbgqGGyIiajIhBLKzs1FQUKB0KeSHVCoV4uPjodPpWrUfhhsiImqy6mATEREBk8nEm6GS11TfZNdqtSI2NrZVvy2GGyIiahKn0+kONl26dFG6HPJD4eHhOHfuHCorK6HValu8Hw4oJiKiJqkeY2MymRSuhPxV9ekop9PZqv0w3BARUbPwVBS1FW/9thhuiIiIyK8w3BAREZFfYbghIiJqphtvvBHz589vcvvTp09DkiQcPny4zWqiGgw33uJyAkXngdyflK6EiIiqSJLU6DRjxowW7ffDDz/En/70pya3j4mJgdVqRUJCQos+r6kYomS8FNxbCrOAV4YAGiPwx2ylqyEiIgBWq9U9v2nTJjzzzDM4ceKEe53RaPRoX1FR0aRLkENDQ5tVh1qtRlRUVLO2oZZjz423GEPk18oyoKJM2VqIiNqJEAKljsp2n4QQTaovKirKPVksFkiS5F4uLy9HcHAw3n//fdx4440wGAx4++23kZeXh7vvvhvdu3eHyWTCoEGD8N5773ns99LTUj169MBf/vIX3H///QgMDERsbCxWr17tfv/SHpXdu3dDkiT85z//QVJSEkwmE4YPH+4RvADgz3/+MyIiIhAYGIhZs2bhySefxNChQ1v03woA7HY7fv/73yMiIgIGgwHXXXcd9u/f734/Pz8fU6ZMQXh4OIxGI3r37o1169YBABwOB+bOnYvo6GgYDAb06NEDy5Yta3EtbYk9N96iDwIkNSCcQFkBoDVedhMioo6urMKJAc9sb/fPPbZ0LEw67/wJe+KJJ7B8+XKsW7cOer0e5eXlSExMxBNPPIGgoCBs3boVU6dORc+ePTFs2LAG97N8+XL86U9/wlNPPYV//vOfePjhh3H99dejX79+DW6zaNEiLF++HOHh4UhNTcX999+PvXv3AgDeeecdPPfcc1i5ciVGjBiBjRs3Yvny5YiPj2/xd/3DH/6AzZs346233kJcXBxeeOEFjB07Fj/99BNCQ0Px9NNP49ixY/jkk08QFhaGn376CWVl8j/Y//a3v2HLli14//33ERsbi6ysLGRlZbW4lrbEcOMtkgQYg4HSPKAsHwiKVroiIiJqgvnz52PixIke6x577DH3/Lx58/Dpp5/igw8+aDTc3HLLLZg9ezYAOTC9/PLL2L17d6Ph5rnnnsMNN9wAAHjyySdx6623ory8HAaDAf/zP/+DmTNn4r777gMAPPPMM9ixYweKi4tb9D1LSkqwatUqrF+/HuPHjwcAvPHGG0hPT8eaNWvw+OOPIzMzE1deeSWSkpIAyD1S1TIzM9G7d29cd911kCQJcXFxLaqjPTDceJMxpCbcEBF1AkatGseWjlXkc72l+g95NafTieeffx6bNm3C2bNnYbfbYbfbYTabG93P4MGD3fPVp79ycnKavE10tPyP4pycHMTGxuLEiRPusFTtmmuuwWeffdak73Wpn3/+GRUVFRgxYoR7nVarxTXXXIPjx48DAB5++GHcddddOHjwIFJSUjBhwgQMHz4cADBjxgzcfPPN6Nu3L8aNG4fbbrsNKSkpLaqlrTHceFP1uBuGGyLqJCRJ8trpIaVcGlqWL1+Ol19+GWlpaRg0aBDMZjPmz58Ph8PR6H4uHYgsSRJcLleTt6m+O2/tbS69Y29TxxrVp3rb+vZZvW78+PE4c+YMtm7dip07d2L06NGYM2cOXnrpJVx11VU4deoUPvnkE+zcuROTJk3CmDFj8M9//rPFNbUVDij2JoYbIqIOb8+ePbjjjjtw7733YsiQIejZsydOnjzZ7nX07dsXX3/9tce6b775psX769WrF3Q6Hb788kv3uoqKCnzzzTfo37+/e114eDhmzJiBt99+G2lpaR4Do4OCgjB58mS88cYb2LRpEzZv3oyLFy+2uKa20rHjtq8xVT0lt6TxbkgiIvJdvXr1wubNm5GRkYGQkBCsWLEC2dnZHgGgPcybNw8PPPAAkpKSMHz4cGzatAlHjhxBz549L7vtpVddAcCAAQPw8MMP4/HHH0doaChiY2PxwgsvoLS0FDNnzgQgj+tJTEzEwIEDYbfb8e9//9v9vV9++WVER0dj6NChUKlU+OCDDxAVFYXg4GCvfm9vYLjxJkuM/Frgm6PHiYjo8p5++mmcOnUKY8eOhclkwoMPPogJEyagsLCwXeuYMmUKfvnlFzz22GMoLy/HpEmTMGPGjDq9OfX53e9+V2fdqVOn8Pzzz8PlcmHq1KkoKipCUlIStm/fjpAQ+cyDTqfDwoULcfr0aRiNRowcORIbN24EAAQEBOCvf/0rTp48CbVajauvvhrbtm2DSuV7J4Ek0ZoTeF6wcuVKvPjii7BarRg4cCDS0tIwcuTIetvOmDEDb731Vp31AwYMwPfff9+kz7PZbLBYLCgsLERQUFCraq/j4D+ALXOBK0YDUz/07r6JiBRWXl6OU6dOIT4+HgaDQelyOqWbb74ZUVFR+Mc//qF0KW2isd9Yc/5+Kxq3Nm3ahPnz52PRokU4dOgQRo4cifHjxyMzM7Pe9q+88gqsVqt7ysrKQmhoKP7rv/6rnStvQEjVZXH5pxUtg4iIOr7S0lKsWLEC33//PX744QcsXrwYO3fuxPTp05UuzecpGm5WrFiBmTNnYtasWejfvz/S0tIQExODVatW1dveYrF43G3ym2++QX5+vvseAPWx2+2w2WweU5sJ6yu/XvwFKG/DzyEiIr8nSRK2bduGkSNHIjExEf/617+wefNmjBkzRunSfJ5iY24cDgcOHDiAJ5980mN9SkoKMjIymrSPNWvWYMyYMY3eSGjZsmV49tlnW1VrkwVGAsGxQEEmcPYb4Iqb2udziYjI7xiNRuzcuVPpMjokxXpucnNz4XQ6ERkZ6bE+MjIS2dmXf/Ck1WrFJ598glmzZjXabuHChSgsLHRPbXmr6PO2cogeVeOFvtvcZp9DREREDVP8aqnGbibUmPXr1yM4OBgTJkxotJ1er4der29NiU1SXuHEb/+egZH6K/EXvAPXoXdx2GbBhbBr4DSEQKU1QNLqodIYoNYaodLpodNqoNeooNeo3a86jUqe18rLatXljwURERHVUCzchIWFQa1W1+mlycnJqdObcykhBNauXYupU6dCp9O1ZZlNdvRsIS4U2fHuxa5I0IzCPZpduOrn14CfX2twG7vQwg4tyqGDXcivNlStEzrYoYVD0qFC0qFS0qFSpUelWg+nSg+XSg+h0UOlNUKtM0KlM0KjN0KjN0GnN0JnMENrDobBEg5LSBhCA40INemgUfveJXtERETepFi40el0SExMRHp6Ou688073+vT0dNxxxx2Nbvv555/jp59+ct90yBdc3SMU/3n0Rry/PwtHCpZAykvCUNtnCHechcFVAq1wQCscUKHmynu9VAE9KhCEUqApHTSuqqmZXEJCIczIEmYUqYJQqragRBeGClMkVMHdYAjthsCoXgiL6YtuYcFQsbeIiIg6MEVPSy1YsABTp05FUlISkpOTsXr1amRmZiI1NRWAPF7m7Nmz2LBhg8d2a9aswbBhw5CQkKBE2Q3qFmzEIzf3qVoaCuCPng2EAFyVQGU5UGkHKsrk18rymqmiHE5HGSodZai0l6LSUQ6noxQuRzlcFeVwVZRCVJRDVMjL8nwZUFkOyWmHqrIcKpcDWmc5TK4imEQpVJJACIoRIhUDOA84AZRVTXkAfpbLcwkJVoTBqu+B/NAhkGKuQeygkejVPZqBh4iIOgxFw83kyZORl5eHpUuXwmq1IiEhAdu2bXNf/WS1Wuvc86awsBCbN2/GK6+8okTJrSNJgForT/rABpupqyavjBRyVgBl+XCW5KE4PwfFBTkoL7iAisKzcBWeg6o4G8aybIRVWGGWytANF9DNcQHI3g9kvwnX1xJ+lmKQHTQYhp7Xou+ICQgKj/FGZUREHcaNN96IoUOHIi0tDQDQo0cPzJ8/H/Pnz29wG0mS8NFHH112bOjleGs/nYniA4pnz55d55Hu1davX19nncViQWlpaRtX5UfUWiAgAuqACFgi+8PSUDshUFmUg+xT3yH/52+gPrsf4QVHEO48j97IRG9bJnD433AdehonjIPhSpqFfjf+DpLGN8Y8ERHV5/bbb0dZWVm9l1Tv27cPw4cPx4EDB3DVVVc1a7/79++v8zTx1lqyZAk+/vhjHD582GO91Wp1Px6hraxfvx7z589HQUFBm35Oe1E83JCPkCRogiLRfUgkug8Z7V7tyD+HzCOfo+DHvQiwfoV+rpPoW/4t8OU8XNi7BPkDp6P37Y9C0gcoWDwRUf1mzpyJiRMn4syZM3XuibZ27VoMHTq02cEGkJ+c3V6ioqLa7bP8BS+doUbpQrqi1w13I+mBV9HvmW/w871fY3fUfcgVFoSLPPT5bgXyXhiK8/s2ymOKiIh8yG233YaIiIg6ZwJKS0uxadMmzJw5E3l5ebj77rvRvXt3mEwmDBo0CO+9916j++3Ro4f7FBUAnDx5Etdffz0MBgMGDBiA9PT0Ots88cQT6NOnD0wmE3r27Imnn34aFRUVAOSek2effRbffvstJEmCJEnumiVJwscff+zez9GjR3HTTTfBaDSiS5cuePDBB1FcXOx+f8aMGZgwYQJeeuklREdHo0uXLpgzZ477s1oiMzMTd9xxBwICAhAUFIRJkybh/Pnz7ve//fZbjBo1CoGBgQgKCkJiYiK++eYbAMCZM2dw++23IyQkBGazGQMHDsS2bdtaXEtTsOeGmuWKXn1xRa80FBb9Gdv+dzWGnHwN3ZwXgO0PIfPoR4idvgZgLw5R5yEEUKHAUAGtSR7HeBkajQbTpk3D+vXr8cwzz7jvo/bBBx/A4XBgypQpKC0tRWJiIp544gkEBQVh69atmDp1Knr27Ilhw4Zd9jNcLhcmTpyIsLAwfPXVV7DZbPWOxQkMDMT69evRtWtXHD16FA888AACAwPxhz/8AZMnT8Z3332HTz/91H0KzWKpO5CgtLQU48aNw7XXXov9+/cjJycHs2bNwty5cz0C3K5duxAdHY1du3bhp59+wuTJkzF06FA88MADl/0+lxJCYMKECTCbzfj8889RWVmJ2bNnY/Lkydi9ezcA+QnmV155JVatWgW1Wo3Dhw9Dq9UCAObMmQOHw4EvvvgCZrMZx44dQ0BA2/6dYLihFrEEBuCWexfg1/PTsPkfT+M3RZsQe+5TXEgbibCH/hdScKzSJRJRe6goBf7Stf0/96lzgK5pY17uv/9+vPjii9i9ezdGjRoFQD4lNXHiRISEhCAkJASPPfaYu/28efPw6aef4oMPPmhSuNm5cyeOHz+O06dPo3v37gCAv/zlLxg/frxHuz/+seYK2h49euDRRx/Fpk2b8Ic//AFGoxEBAQHQaDSNnoZ65513UFZWhg0bNrjH/Lz66qu4/fbb8de//tV9n7iQkBC8+uqrUKvV6NevH2699Vb85z//aVG42blzJ44cOYJTp04hJka+oOQf//gHBg4ciP379+Pqq69GZmYmHn/8cfTr1w8A0Lt3b/f2mZmZuOuuuzBo0CAAQM+ePZtdQ3PxtBS1SvfIMExYsBIfDnkd50Uwwst+QcHKFAg+GZ2IfES/fv0wfPhwrF27FgDw888/Y8+ePbj//vsBAE6nE8899xwGDx6MLl26ICAgADt27KhztW5Djh8/jtjYWHewAYDk5OQ67f75z3/iuuuuQ1RUFAICAvD00083+TNqf9aQIUM8BjOPGDECLpcLJ06ccK8bOHAg1Gq1ezk6Oho5OTnN+qzanxkTE+MONgAwYMAABAcH4/jx4wDkW7vMmjULY8aMwfPPP4+ff/7Z3fb3v/89/vznP2PEiBFYvHgxjhw50qI6moM9N9RqapWEyRP/C/8K64GEnVMQ77Ci8PVbYJm3BzB3Ubo8ImpLWpPci6LE5zbDzJkzMXfuXLz22mtYt24d4uLiMHq0fPHE8uXL8fLLLyMtLQ2DBg2C2WzG/Pnz4XA4mrRvUc94w0sfI/TVV1/hd7/7HZ599lmMHTsWFosFGzduxPLly5v1PRp7RFHt9dWnhGq/53K14C6wjXxm7fVLlizBPffcg61bt+KTTz7B4sWLsXHjRtx5552YNWsWxo4di61bt2LHjh1YtmwZli9fjnnz5rWonqZgzw15ze3XX41vRv0DZ1wRsJSfRf5b9wDOSqXLIqK2JEny6aH2npow3qa2SZMmQa1W491338Vbb72F++67z/2Hec+ePbjjjjtw7733YsiQIejZsydOnjzZ5H0PGDAAmZmZOHeuJuTt27fPo83evXsRFxeHRYsWISkpCb1798aZM2c82uh0Ojidzst+1uHDh1FSUuKxb5VKhT59+jSyZctVf7/aD54+duwYCgsL0b9/f/e6Pn364JFHHsGOHTswceJErFu3zv1eTEwMUlNT8eGHH+LRRx/FG2+80Sa1VmO4Ia/6r1HDsGXAchQLA0JyvkLx7peVLomICAEBAZg8eTKeeuopnDt3DjNmzHC/16tXL6SnpyMjIwPHjx/HQw89VOe5h40ZM2YM+vbti2nTpuHbb7/Fnj17sGjRIo82vXr1QmZmJjZu3Iiff/4Zf/vb3/DRRx95tOnRowdOnTqFw4cPIzc3F3a7vc5nTZkyBQaDAdOnT8d3332HXbt2Yd68eZg6depln8t4OU6nE4cPH/aYjh07hjFjxmDw4MGYMmUKDh48iK+//hrTpk3DDTfcgKSkJJSVlWHu3LnYvXs3zpw5g71792L//v3u4DN//nxs374dp06dwsGDB/HZZ595hKK2wHBDXvfAXbfiddNDAADDl38F8n6+zBZERG1v5syZyM/Px5gxYxAbW3PRw9NPP42rrroKY8eOxY033oioqKhm3Q1YpVLho48+gt1uxzXXXINZs2bhueee82hzxx134JFHHsHcuXMxdOhQZGRk4Omnn/Zoc9ddd2HcuHEYNWoUwsPD670c3WQyYfv27bh48SKuvvpq/Pa3v8Xo0aPx6quvNu9g1KO4uBhXXnmlx3TLLbe4L0UPCQnB9ddfjzFjxqBnz57YtGkTAECtViMvLw/Tpk1Dnz59MGnSJIwfPx7PPvssADk0zZkzB/3798e4cePQt29frFy5stX1NkYS9Z0s9GM2mw0WiwWFhYUICgpSuhy/dTgzH7Y3bsf16qPIjxuHkPs2KV0SEbVSeXk5Tp06hfj4eBgMBqXLIT/U2G+sOX+/2XNDbWJobAj2930MTiEh5MynQNZ+pUsiIqJOguGG2szvbhuLD13XAwAK0p9XuBoiIuosGG6ozXQLNuJ0vwfgEhKCM3cCuU2/+oCIiKilGG6oTd158434zDUUAFDy1XpFayEios6B4YbaVK+IQBzschsAQPr2PcDZ8ge3EZFv6GTXoVA78tZvi+GG2lzP4RNxQQTBVJEHcXKH0uUQUQtV3/W2tFSBB2VSp1B9V+jaj45oCT5+gdrc2MEx2PSv6zFL/W+U/N8/ENDvVqVLIqIWUKvVCA4Odj+jyGQyNfgoAKLmcrlcuHDhAkwmEzSa1sUThhtqc4EGLc52vwWw/hu6zM+BSgeg0SldFhG1QPUTq1v6EEaixqhUKsTGxrY6NDPcULvof+V1uHDOgnBnIZC5D+h5g9IlEVELSJKE6OhoREREoKKCY+jIu3Q6HVSq1o+YYbihdjGqfxR2bxmC/9J8gfJjn8LAcEPUoanV6laPiyBqKxxQTO0iPFCPE4HXAgAcJ/+jcDVEROTPGG6o3Rh6jQQABBT+CJQXKlwNERH5K4YbajdD+vfFaVckVBB81hQREbUZhhtqN1f3CME3oi8AoPyXvQpXQ0RE/orhhtpNsEmHU8YEAED5LxkKV0NERP6K4YbalatrIgDAmHsUcLkUroaIiPwRww21q4ieg2EXWuidJUDBaaXLISIiP8RwQ+1qcFw4Toju8oL1iLLFEBGRX2K4oXY1IDoIx0QPAEBp1mFFayEiIv/EcEPtyqhT45yhNwDAnnVI4WqIiMgfMdxQu7OHDQQA6HKPKVwJERH5I4Ybanembv0BAGZ7DmAvVrgaIiLyNww31O5iu3VDngiUFy7+rGwxRETkdxhuqN31jgjEKRENABC5JxWuhoiI/A3DDbW7XhEB+MUlh5ty6wmFqyEiIn+jeLhZuXIl4uPjYTAYkJiYiD179jTa3m63Y9GiRYiLi4Ner8cVV1yBtWvXtlO15A0GrRo5+hgAQPl5hhsiIvIujZIfvmnTJsyfPx8rV67EiBEj8Prrr2P8+PE4duwYYmNj691m0qRJOH/+PNasWYNevXohJycHlZWV7Vw5tVZpYDxQAEh5PyldChER+RlFw82KFSswc+ZMzJo1CwCQlpaG7du3Y9WqVVi2bFmd9p9++ik+//xz/PLLLwgNDQUA9OjRoz1LJm8J7QUUAMai00pXQkREfkax01IOhwMHDhxASkqKx/qUlBRkZNT/xOgtW7YgKSkJL7zwArp164Y+ffrgscceQ1lZWYOfY7fbYbPZPCZSnjmyJwDIz5gqL1S4GiIi8ieK9dzk5ubC6XQiMjLSY31kZCSys7Pr3eaXX37Bl19+CYPBgI8++gi5ubmYPXs2Ll682OC4m2XLluHZZ5/1ev3UOt0iuuCiCECoVAwU/goYLEqXREREfkLxAcWSJHksCyHqrKvmcrkgSRLeeecdXHPNNbjllluwYsUKrF+/vsHem4ULF6KwsNA9ZWVlef07UPPFhJpwToTJCwX8b0JERN6jWLgJCwuDWq2u00uTk5NTpzenWnR0NLp16waLpeZf+f3794cQAr/++mu92+j1egQFBXlMpLzYUBPOVoWbivxMhashIiJ/oli40el0SExMRHp6usf69PR0DB8+vN5tRowYgXPnzqG4uOaW/T/++CNUKhW6d+/epvWSd4UF6HBeksNNWc5pZYshIiK/ouhpqQULFuDNN9/E2rVrcfz4cTzyyCPIzMxEamoqAPmU0rRp09zt77nnHnTp0gX33Xcfjh07hi+++AKPP/447r//fhiNRqW+BrWAJEkoMsg38nNcPKNwNURE5E8UvRR88uTJyMvLw9KlS2G1WpGQkIBt27YhLi4OAGC1WpGZWXPKIiAgAOnp6Zg3bx6SkpLQpUsXTJo0CX/+85+V+grUCnZTV8ABSLazSpdCRER+RBJCCKWLaE82mw0WiwWFhYUcf6Ow5evew6NnUlGsj0DAQj5jioiIGtacv9+KXy1FnZc2RH4Eg9l+AXDyLtNEROQdDDekmKCwKDiFBAkCKLmgdDlEROQnGG5IMZEWMy4gWF4orv/GjURERM3FcEOKibIYkCOC5YWi84rWQkRE/oPhhhRTO9y4bFZliyEiIr/BcEOKCQ/QIwchAIDSi7wcnIiIvIPhhhSjUatQrJHvUuwoYM8NERF5B8MNKarcIIcbYTuncCVEROQvGG5IUQ6T/JBUVUmOwpUQEZG/YLghRYkAOdzoyhhuiIjIOxhuSFHqIPnhmUZ7HuByKlwNERH5A4YbUpQxuOq0FJxAeaHC1RARkT9guCFFhQSaYRMmeaE0T9liiIjILzDckKK6BOhxUQTKCww3RETkBQw3pKhQsw75qA43F5UthoiI/ALDDSkqLECHfBEgL7DnhoiIvIDhhhTVJUDv7rlxFF1QuBoiIvIHDDekKLNOjUIpCABQXshwQ0RErcdwQ4qSJAl2bTAAoJI9N0RE5AUMN6Q4u05+MrjgmBsiIvIChhtSnNMghxupjFdLERFR6zHckOKEMRQAoC7PV7gSIiLyBww3pDjJ1AUAoHcUKFsIERH5BYYbUpwmMAwAoK+0Ac5KhashIqKOjuGGFKcLlHtuJAigvEDZYoiIqMNjuCHFBZmNKBJGeYFPBiciolZiuCHFBRt1sKHqyeDsuSEiolZiuCHFBZu0sAmzvMCeGyIiaiWGG1Kcxait1XPDcENERK3DcEOKsxjZc0NERN7DcEOKCzbV9NxUlPBGfkRE1DoMN6S4AL0GRZB7bhzFfAQDERG1DsMNKU6SJNg1gQAAR3GBssUQEVGHx3BDPqFCGwQAcJUVKFsIERF1eAw35BMqdXLPjeB9boiIqJUUDzcrV65EfHw8DAYDEhMTsWfPngbb7t69G5Ik1Zl++OGHdqyY2oJLbwEAqOw2hSshIqKOTtFws2nTJsyfPx+LFi3CoUOHMHLkSIwfPx6ZmZmNbnfixAlYrVb31Lt373aqmNqK0MunpdR2XgpORESto2i4WbFiBWbOnIlZs2ahf//+SEtLQ0xMDFatWtXodhEREYiKinJParW6nSqmtiIZggEAmgr23BARUesoFm4cDgcOHDiAlJQUj/UpKSnIyMhodNsrr7wS0dHRGD16NHbt2tVoW7vdDpvN5jGR75GMwQAAXUWxsoUQEVGHp1i4yc3NhdPpRGRkpMf6yMhIZGdn17tNdHQ0Vq9ejc2bN+PDDz9E3759MXr0aHzxxRcNfs6yZctgsVjcU0xMjFe/B3mH1hwivwo7UFGucDVERNSRaZQuQJIkj2UhRJ111fr27Yu+ffu6l5OTk5GVlYWXXnoJ119/fb3bLFy4EAsWLHAv22w2BhwfpDdb4BISVJIA7DZAa1C6JCIi6qAU67kJCwuDWq2u00uTk5NTpzenMddeey1OnjzZ4Pt6vR5BQUEeE/meAKMOxTDKC3y+FBERtYJi4Uan0yExMRHp6eke69PT0zF8+PAm7+fQoUOIjo72dnnUzgINGhRVhxt7kbLFEBFRh6boaakFCxZg6tSpSEpKQnJyMlavXo3MzEykpqYCkE8pnT17Fhs2bAAApKWloUePHhg4cCAcDgfefvttbN68GZs3b1bya5AXBOg1KBZGQALDDRERtYqi4Wby5MnIy8vD0qVLYbVakZCQgG3btiEuLg4AYLVaPe5543A48Nhjj+Hs2bMwGo0YOHAgtm7diltuuUWpr0BeEmjQ1pyWYrghIqJWkIQQQuki2pPNZoPFYkFhYSHH3/iQk+eLcO7VW3CD+ggw4e/A0LuVLomIiHxIc/5+K/74BSIACKg15kbwEQxERNQKDDfkEwINWnnMDYDKUoYbIiJqOYYb8gkmrdo95sZRWqBsMURE1KEx3JBPUKkkODRmAEBlGXtuiIio5RhuyGdUagIAAE6GGyIiagWGG/IZTq0cbkQ5ww0REbUcww35DJdWPi0l2flkcCIiajmGG/IZLl0gAEBVwZv4ERFRyzHckO/QyTdlUlWw54aIiFqO4YZ8h0Eec6NluCEiolZguCGfIRksAACts1ThSoiIqCNjuCGfoTbKp6U0LjtQ6VC4GiIi6qgYbshnaI2BNQsOnpoiIqKWYbghn2E0GFAq9PICH55JREQtxHBDPiNAr0YJDPICb+RHREQtxHBDPsOk06BYVIWbCg4qJiKilmG4IZ9h1qtRWt1zw7sUExFRCzHckM8w6zQ1p6U4oJiIiFqI4YZ8hlmvQUn1aSlHibLFEBFRh8VwQz7DpKs1oJjhhoiIWojhhnxGgF6D0qqeG2HnwzOJiKhlGG7IZ5j0NWNuKss55oaIiFqG4YZ8hkmrrhVu2HNDREQtw3BDPkOlklChMgIAnOy5ISKiFmK4IZ9SqTEDAFzsuSEiohZiuCGf4tSYAACCN/EjIqIWYrghn+LUyj03gpeCExFRCzHckG/RyeFGqmC4ISKilmG4IZ8itAEAABUfv0BERC3EcEM+RW2Qw426kk8FJyKilmG4IZ8i6eVwo3Ey3BARUcsw3JBPUVWFG62zDHC5FK6GiIg6IoYb8ilqQyAAQIIAKth7Q0REzcdwQz5FozfDJSR5gZeDExFRCzDckE+p/fBM8IopIiJqAYYb8ilGnRql0MsL7LkhIqIWUDzcrFy5EvHx8TAYDEhMTMSePXuatN3evXuh0WgwdOjQti2Q2pVJp0aJYM8NERG1nKLhZtOmTZg/fz4WLVqEQ4cOYeTIkRg/fjwyMzMb3a6wsBDTpk3D6NGj26lSai8mnRql7tNS7LkhIqLmUzTcrFixAjNnzsSsWbPQv39/pKWlISYmBqtWrWp0u4ceegj33HMPkpOTL/sZdrsdNpvNYyLfZdCqOeaGiIhaRbFw43A4cODAAaSkpHisT0lJQUZGRoPbrVu3Dj///DMWL17cpM9ZtmwZLBaLe4qJiWlV3dS2TDpNrdNS7LkhIqLmUyzc5Obmwul0IjIy0mN9ZGQksrOz693m5MmTePLJJ/HOO+9Ao9E06XMWLlyIwsJC95SVldXq2qnt8LQUERG1VtMSQhuSJMljWQhRZx0AOJ1O3HPPPXj22WfRp0+fJu9fr9dDr9e3uk5qH8baA4rtRcoWQ0REHZJi4SYsLAxqtbpOL01OTk6d3hwAKCoqwjfffINDhw5h7ty5AACXywUhBDQaDXbs2IGbbrqpXWqntmPS1R5zw54bIiJqPsVOS+l0OiQmJiI9Pd1jfXp6OoYPH16nfVBQEI4ePYrDhw+7p9TUVPTt2xeHDx/GsGHD2qt0akMmbc1N/Jx2DigmIqLmU/S01IIFCzB16lQkJSUhOTkZq1evRmZmJlJTUwHI42XOnj2LDRs2QKVSISEhwWP7iIgIGAyGOuup4zLoVCitOi3lLC+GWuF6iIio41E03EyePBl5eXlYunQprFYrEhISsG3bNsTFxQEArFbrZe95Q/5Fp1ahTJLDjaucY26IiKj5JCGEaO5GWVlZkCQJ3bt3BwB8/fXXePfddzFgwAA8+OCDXi/Sm2w2GywWCwoLCxEUFKR0OVSPPy5eiD9LK1EaOwqm+z9WuhwiIvIBzfn73aIxN/fccw927doFAMjOzsbNN9+Mr7/+Gk899RSWLl3akl0SuVVoTAAAwZv4ERFRC7Qo3Hz33Xe45pprAADvv/8+EhISkJGRgXfffRfr16/3Zn3UCbk0ZgCAxKuliIioBVoUbioqKtz3jtm5cyd+85vfAAD69esHq9XqveqoU3Jq5Z4bhhsiImqJFoWbgQMH4u9//zv27NmD9PR0jBs3DgBw7tw5dOnSxasFUieklXtuVBUMN0RE1HwtCjd//etf8frrr+PGG2/E3XffjSFDhgAAtmzZ4j5dRdRi+gAAgLqS4YaIiJqvRZeC33jjjcjNzYXNZkNISIh7/YMPPgiTyeS14qiTqgo3GmcZ4HIBKkUfXk9ERB1Mi/5qlJWVwW63u4PNmTNnkJaWhhMnTiAiIsKrBVLno6oKNwCAilLlCiEiog6pReHmjjvuwIYNGwAABQUFGDZsGJYvX44JEyZg1apVXi2QOh+NzgSXqHp4KgcVExFRM7Uo3Bw8eBAjR44EAPzzn/9EZGQkzpw5gw0bNuBvf/ubVwukzseo19R6eCbvdUNERM3TonBTWlqKwMBAAMCOHTswceJEqFQqXHvttThz5oxXC6TOx6RToxTyrQYYboiIqLlaFG569eqFjz/+GFlZWdi+fTtSUlIAADk5OXykAbWaSadBsTDKCzwtRUREzdSicPPMM8/gscceQ48ePXDNNdcgOTkZgNyLc+WVV3q1QOp8jNraPTcMN0RE1DwtuhT8t7/9La677jpYrVb3PW4AYPTo0bjzzju9Vhx1TkadGqUcc0NERC3UonADAFFRUYiKisKvv/4KSZLQrVs33sCPvMKkU6NEVIcb9twQEVHztOi0lMvlwtKlS2GxWBAXF4fY2FgEBwfjT3/6E1wul7drpE5GPi1VFW7s7LkhIqLmaVHPzaJFi7BmzRo8//zzGDFiBIQQ2Lt3L5YsWYLy8nI899xz3q6TOhGTToN8wdNSRETUMi0KN2+99RbefPNN99PAAWDIkCHo1q0bZs+ezXBDrWLUqWvd54anpYiIqHladFrq4sWL6NevX531/fr1w8WLF1tdFHVuRi3DDRERtVyLws2QIUPw6quv1ln/6quvYvDgwa0uijo3k06NUg4oJiKiFmrRaakXXngBt956K3bu3Ink5GRIkoSMjAxkZWVh27Zt3q6ROhmTTo2SqvvcCEcRJIXrISKijqVFPTc33HADfvzxR9x5550oKCjAxYsXMXHiRHz//fdYt26dt2ukTqb2fW5c5RxQTEREzdPi+9x07dq1zsDhb7/9Fm+99RbWrl3b6sKo8zJq1e7HL7jsxVArXA8REXUsLeq5IWpLGrUKDhWfLUVERC3DcEM+yakxAQAE73NDRETNxHBDPsmllcONxJ4bIiJqpmaNuZk4cWKj7xcUFLSmFiI3oTMDFYCqguGGiIiap1nhxmKxXPb9adOmtaogIgCAzgyUAOrKUsDlAlTsZCQioqZpVrjhZd7UXoQuoGahohTQBzTcmIiIqBb+c5h8klpnhktU3b6P426IiKgZGG7IJ5l0mlrPl+IVU0RE1HQMN+STTDo1SqsewcBwQ0REzcFwQz7JqFOjhA/PJCKiFmC4IZ8kPzyT4YaIiJqP4YZ8kkmncT88k6eliIioORhuyCeZeFqKiIhaSPFws3LlSsTHx8NgMCAxMRF79uxpsO2XX36JESNGoEuXLjAajejXrx9efvnldqyW2os8oLgq3NjZc0NERE3XrJv4edumTZswf/58rFy5EiNGjMDrr7+O8ePH49ixY4iNja3T3mw2Y+7cuRg8eDDMZjO+/PJLPPTQQzCbzXjwwQcV+AbUVow6Ta2eG4YbIiJqOkV7blasWIGZM2di1qxZ6N+/P9LS0hATE4NVq1bV2/7KK6/E3XffjYEDB6JHjx649957MXbs2EZ7e6hjMntcCs7TUkRE1HSKhRuHw4EDBw4gJSXFY31KSgoyMjKatI9Dhw4hIyMDN9xwQ4Nt7HY7bDabx0S+z6hToxhGeYHhhoiImkGxcJObmwun04nIyEiP9ZGRkcjOzm502+7du0Ov1yMpKQlz5szBrFmzGmy7bNkyWCwW9xQTE+OV+qltmXQalHJAMRERtYDiA4olSfJYFkLUWXepPXv24JtvvsHf//53pKWl4b333muw7cKFC1FYWOiesrKyvFI3tS35PjfVp6WKlC2GiIg6FMUGFIeFhUGtVtfppcnJyanTm3Op+Ph4AMCgQYNw/vx5LFmyBHfffXe9bfV6PfR6vXeKpnbjcbUUe26IiKgZFOu50el0SExMRHp6usf69PR0DB8+vMn7EULAbrd7uzxSmMnjaimGGyIiajpFLwVfsGABpk6diqSkJCQnJ2P16tXIzMxEamoqAPmU0tmzZ7FhwwYAwGuvvYbY2Fj069cPgHzfm5deegnz5s1T7DtQ26g9oNhVblP+/CkREXUYioabyZMnIy8vD0uXLoXVakVCQgK2bduGuLg4AIDVakVmZqa7vcvlwsKFC3Hq1CloNBpcccUVeP755/HQQw8p9RWojZh0ahQJk7xQzivciIio6SQhhFC6iPZks9lgsVhQWFiIoKAgpcuhRoxb9CY+1T4Kly4Iqqc4EJyIqDNrzt9v9vaTz3JoAwEAkqMIcLkUroaIiDoKhhvyWS5dVbiB4CMYiIioyRhuyGepdEbYRdWwsPJCZYshIqIOg+GGfJZZr0URqgYV2zmomIiImobhhnyWUaeGzX3FFHtuiIioaRhuyGeZdOqanhteDk5ERE3EcEM+S77XTdWTwXlaioiImojhhnyWSaeBDWZ5gaeliIioiRhuyGd53qWY4YaIiJqG4YZ8llGnhg0MN0RE1DwMN+SzTFpNTc8Nx9wQEVETMdyQzzLra/fcMNwQEVHTMNyQzzJyzA0REbUAww35LPk+N7wUnIiImofhhnyWUctLwYmIqPkYbshnmfW1T0ux54aIiJqG4YZ8lkmnRiEvBSciomZiuCGfZax9KXhlGeCsULYgIiLqEBhuyGeZdGoUVw8oBnhqioiImoThhnyWSa+GE2oUC4O8orxA0XqIiKhjYLghn2XSaQAAReBdiomIqOkYbshnGbVqAICNN/IjIqJmYLghn6VWSdBrVCisvtdNWYGi9RARUcfAcEM+zazXIF8EygulecoWQ0REHQLDDfk0o1aNi+5wc1HZYoiIqENguCGfZtarkQ/23BARUdMx3JBPC9BrkC8C5IUy9twQEdHlMdyQTwswaNlzQ0REzcJwQz4tQF97zA3DDRERXR7DDfm0AF4tRUREzcRwQz4tQK/FRfBqKSIiajqGG/JpAXp1Tc+NoxiotCtbEBER+TyGG/JpAQYNbDDBCflRDOy9ISKiy2G4IZ8WoNcCkFCi5rgbIiJqGoYb8mkBBvnJ4DYpSF7BcENERJfBcEM+LUAvn44qAMMNERE1jeLhZuXKlYiPj4fBYEBiYiL27NnTYNsPP/wQN998M8LDwxEUFITk5GRs3769Haul9iaflkLNvW54l2IiIroMRcPNpk2bMH/+fCxatAiHDh3CyJEjMX78eGRmZtbb/osvvsDNN9+Mbdu24cCBAxg1ahRuv/12HDp0qJ0rp/YSoJdPS+VWP4KhJFfBaoiIqCOQhBBCqQ8fNmwYrrrqKqxatcq9rn///pgwYQKWLVvWpH0MHDgQkydPxjPPPFPv+3a7HXZ7zeXDNpsNMTExKCwsRFBQUOu+ALW5zLxSXP/iLjyu24w5qs1A0v3AbS8rXRYREbUzm80Gi8XSpL/fivXcOBwOHDhwACkpKR7rU1JSkJGR0aR9uFwuFBUVITQ0tME2y5Ytg8VicU8xMTGtqpvaV/WA4rPOYHlFUbZyxRARUYegWLjJzc2F0+lEZGSkx/rIyEhkZzftD9jy5ctRUlKCSZMmNdhm4cKFKCwsdE9ZWVmtqpval7lqQPF5ESyvKLIqVwwREXUIGqULkCTJY1kIUWddfd577z0sWbIE//u//4uIiIgG2+n1euj1+lbXScrQa9TQaVQ47wyRV7DnhoiILkOxnpuwsDCo1eo6vTQ5OTl1enMutWnTJsycORPvv/8+xowZ05Zlkg8I0GtwXlSFm+LzgMupbEFEROTTFAs3Op0OiYmJSE9P91ifnp6O4cOHN7jde++9hxkzZuDdd9/Frbfe2tZlkg8I0GuQBwuEpAKECyi5oHRJRETkwxQ9LbVgwQJMnToVSUlJSE5OxurVq5GZmYnU1FQA8niZs2fPYsOGDQDkYDNt2jS88soruPbaa929PkajERaLRbHvQW0rQK+BCyo4DOHQl52Xx90ERildFhER+ShFw83kyZORl5eHpUuXwmq1IiEhAdu2bUNcXBwAwGq1etzz5vXXX0dlZSXmzJmDOXPmuNdPnz4d69evb+/yqZ1UXzFVpq8ON+cVroiIiHyZ4gOKZ8+ejdmzZ9f73qWBZffu3W1fEPmc6hv5lejDEQzwiikiImqU4o9fILqc6nBj03SRVzDcEBFRIxhuyOcFGeVwk6euuuS/8FcFqyEiIl/HcEM+z2KUH56Zraq6RcDFUwpWQ0REvo7hhnxedbjJQlW4yT+tXDFEROTzGG7I51WHm1POcHlF0TmgolzBioiIyJcx3JDPqw43Z+1GQBcoryzIbGQLIiLqzBhuyOcFGeRwU1heCYT0kFfy1BQRETWA4YZ8XlBVz01hWQUQIt/gERd/UbAiIiLyZQw35POqT0vZyiqAsN7yytwfFayIiIh8GcMN+TyLSQ439koXHKF95ZUXflCwIiIi8mUMN+TzAnQaqCR5vjiolzyTcxwQQrmiiIiofkIAlXZFS1D82VJEl6NSSQgyalFQWoGLxjiEQgLKLgIlF4CACKXLIyLyTS4XUFkuTxVlnq+V5fItNSrLGngtlwNKZVnVa3ndV492tV6ddiCoO7Dge8W+OsMNdQiWqnBTUKGRr5jKPyX33jDcEFFH4HLJf/QvDRnVgaJ2WGhOEKlsZJ9Oh3Lft1LZe5Ex3FCHYKl9xVREfzncXPgB6HmDwpURUYfmcgIVpXIwqPNaBjhK6q7zaFd7vpGeEKeyp2mg0gIaA6A1ABpj1asB0Brl1zrv1Wqj0V/yeun6WvO1t1EQww11CB7hJnIgcGIbYD2icFVE1KaEqBUwSuRXx6WBorGwUSuMOBrYRonQodLUChTG+kOHx3vGJgYRoxwq6muv7lx/7jvXt6UOy30jv7IKoFuivPLsAQUrIiK36gGkHiHk0qm4KmRUzVcHler5ilrztd9DO144oDXJgcD9agS05lrzl7yvM9XMa4w17dzBQl9/AOlkQUMJPMLUIVRfDl5QWgF0vUpeeeEHwF4E6AMVrIyoA3JWysHBXlT1Wgw4iqpeq9ZXv+e4JJA0FEKEs21r1lYFCZ2pVuAweQaP2mGjThi5NLiYPLfTGABJatvvQO2G4YY6hFCTDgCQX+oAAiPlkfi2XwHrt0CP6xSujqiNCSEP0LQXA3ZbrUBSO6AU1bOuer72e8XyOJC2ojEAOrMcQHQNTI29pwuoChxV89WBRaVuu5rJ7zDcUIfQJUAON3nFVaP/uycCx34FzuxjuCHfJoTcw2G3AeU2z9f61rlfCz2XXZXer02tkwOEPkB+KK0+oGq51rwuwDNo1BtAzDU9KjzlQj6Av0LqEELNVeGmpGrwX/z1wLH/BX7ZDdzwuHKFkf8TQg4XZflAWQFQXlBrvrCRgFJY9Vrk3VM21YFDXxVCdLVfa4eTwAaCS61lha9oIWorDDfUIXQxy/8nfLGkquem5yj5Nev/5H8V68wKVUYdQvVVN7WDSVl+1XLt+XreKy8AhKv1NUhqwBAE6IOqXi2XLAfJgcS9zlL3PV0AoOKN5Ykuh+GGOoTqnht3uAntCVhigcJM4EwG0PtmBaujduWsAEovAqV5NVNZ9fLFqoBST0hp7SW/GiNgDAaMIYAhWJ6/NIA0Fl60Jg5YJWonDDfUIYQF1IQbl0tApZKAK0YBB9+S73nDcNMxuVw1j9KoHVZK84DS/HoCzEX5tE9LSWo5nFwaUtzzjbynNbT++xJRu2C4oQ4hpKrnxiWAgrIKuSdnwG/kcHNsCzD+RQ5k9BWVDjmslOQAJblAcc4l8xfkqTgHKM1t4SkfCTCFAqYugLHq1RQqT8bQmpByaWDRBbD3hKgT4F8D6hC0ahWCDBrYyitxscQuh5v4G+Q/aqW5wKnPgV6jlS7Tv1WUAUVWoCgbsJ2TX6uXi7KB4vNyiCkvbP6+jSGAKcwzpJi6XBJeutS8b7Dw0mAiahDDDXUYYQF62MorkVfsQK8IAGotMOAO4Ju1cg8Ow03LOCvlYFKUDRTVCi02a63wYpXHrjSVSgOYwwFzGGCOkOcDwi+Zr14Ok/9bEhF5CcMNdRihZh1+yS1BXvWgYgC4+gE53Bz/F3DxF3mgMdUQQj4FVPgrYDsrv3rMnwWKs5t+akhjBIKigcDqKarmNSBSfkq7OVw+HcSreohIIQw31GHU3OumVriJHAD0uhn4KR344iVgwkqFqlOIsxIozJKDXUFmPSHmXNOuElJpgICoquBSK7AEdvVcNlg4ZoWIfB7DDXUY1Xcpvljs8HzjxiflcHP4HeDqmTUP1vQXFWVA/mk5wFw8BeSfqpkvzGrCnWslOZgEdQMs3eXJPd9NfpSFOZw9LUTkNxhuqMMIC5Bv5HehuNzzje5JwODfAUc2Ah8+BDzwmXxfkY6kLF8OKxd/qQovp2tCTJG18W01BiCkBxAcVxNeaoeYwGhAo2uPb0FE5BMYbqjDiAyS7zOSXVjPaZaxzwGn9wB5J4HNs4DJb/vWH3Qh5IG5tXtd8k/VBJrLDdbVW4DQeHkKiZfHFlXPB0az14WIqBaGG+owoqrCzXlbed03zWHApA3AuluAk9uB96cCE1fLY0Tai7NSvmOyR3CpNX+5JzEHRNUKL1UBpnreGMKxLkRETcRwQx1GlEUON9bCesINIJ+euvs94L27gR8/BVZdB9z8LDBggvd6NhwlQP4Zz16X6vmCzMYfkCip5VNFtXtd3PM9+HwsIiIvYbihDqP6tFReiR0VThe06noCS6/RwH2fAP+8Dyg4I79aFgMJdwJxI4DIgVWnceq5AVylQ77Nf3F2zY3piqw1YSb/tHw/mMZUj3+p3etSHWSCY3k/FyKidsBwQx1GF7MOWrWECqdATpEd3YKN9Tfsngg8nAHse02eCjOBva/IEyBf9mwIloOGpAYqSuQeGaej/v1dyhBcE2Au7YEJiOL4FyIihSkeblauXIkXX3wRVqsVAwcORFpaGkaOHFlvW6vVikcffRQHDhzAyZMn8fvf/x5paWntWzApRqWSEBFowNmCMmQXljccbgBAHwDc+AQw4vfAD1uBX3YBmf8n98C4KuVHNtRHUss3oguIrLkxXUiPqjBTdfrIGNIG346IiLxF0XCzadMmzJ8/HytXrsSIESPw+uuvY/z48Th27BhiY2PrtLfb7QgPD8eiRYvw8ssvK1AxKS3KIoebegcV10drBAb9Vp4AwOWsepSADXBVyEFHa5bHu+gDAH0Qn1lERNTBKRpuVqxYgZkzZ2LWrFkAgLS0NGzfvh2rVq3CsmXL6rTv0aMHXnlFPrWwdu3adq2VfEOU+3LwJoabS6mqBvW240VURETUvhQbHOBwOHDgwAGkpKR4rE9JSUFGRobXPsdut8Nms3lM1HG573XT1J4bIiLqdBQLN7m5uXA6nYiMjPRYHxkZiezsbK99zrJly2CxWNxTTEyM1/ZN7a9rsBxuzhZc5p4xRETUaSl+WYd0yY3JhBB11rXGwoULUVhY6J6ysrK8tm9qf7GhJgBAZl6pwpUQEZGvUmzMTVhYGNRqdZ1empycnDq9Oa2h1+uh1+u9tj9SVlwX+UZ3Z/JKFK6EiIh8lWI9NzqdDomJiUhPT/dYn56ejuHDhytUFfm6mFD58m9beSUKSysUroaIiHyRoldLLViwAFOnTkVSUhKSk5OxevVqZGZmIjU1FYB8Suns2bPYsGGDe5vDhw8DAIqLi3HhwgUcPnwYOp0OAwYMUOIrUDsz6TQID9TjQpEdZy6WYLApWOmSiIjIxygabiZPnoy8vDwsXboUVqsVCQkJ2LZtG+Li4gDIN+3LzMz02ObKK690zx84cADvvvsu4uLicPr06fYsnRQUF2rChSI7Mi+WYnD3YKXLISIiH6P4HYpnz56N2bNn1/ve+vXr66wTQrRxReTrYkNN+OZMPs5wUDEREdVD8auliJortguvmCIiooYx3FCH0zM8AADw04VihSshIiJfxHBDHU7fyEAAwI/ZRTxNSUREdTDcUIcTH2aGRiWhyF4Ja0ufMUVERH6L4YY6HJ1GhZ7h8s38TpwvUrgaIiLyNQw31CH1qXVqioiIqDaGG+qQqsfdnGC4ISKiSzDcUIfUPzoIAHD0bKHClRARka9huKEOaWhsMAD5cnBbOZ8xRURENRhuqEMKC9AjJtQIIYAjWey9ISKiGgw31GENjQkBABzKzFe4EiIi8iUMN9RhXRkTDAA4yHBDRES1MNxQh3V1j1AAwP7T+ahwuhSuhoiIfAXDDXVYA7sGIcSkRbG9EoezCpQuh4iIfATDDXVYKpWE4b3CAAB7TuYqXA0REfkKhhvq0EZWhZvPf7ygcCVEROQrGG6oQxvVLwKSBHybVYBzBWVKl0NERD6A4YY6tMggA66OkwcWbztqVbgaIiLyBQw31OHdOjgaAPCvIww3RETEcEN+4JZB0dCoJHybVYDv+KwpIqJOj+GGOrzwQD3GJUQBAN7+6ozC1RARkdIYbsgvTEvuAQD4+PBZ5BbblS2GiIgUxXBDfuHqHiEY3N2C8goXVu3+WelyiIhIQQw35BckScKCm/sAAP7x1Rmc5WXhRESdFsMN+Y0b+oTjmvhQOCpdePrj7yCEULokIiJSAMMN+Q1JkvDchATo1Cp89kMOPjp0VumSiIhIAQw35Fd6RwZi3k29AACLPvoOx87ZFK6IiIjaG8MN+Z3Zo3phZO8wlFU4Meut/ci6WKp0SURE1I4YbsjvqFUS/ufuK9Ez3IxzheX43eqv8MuFYqXLIiKidsJwQ34p2KTDxgeuRc8wM84WlOGOV/di57HzSpdFRETtgOGG/FZEkAEbH7oWSXEhKLJXYtaGb7Dg/cO4WOJQujQiImpDDDfk1yICDXj3gWtx/4h4SBLw4cGzuP6FXVi+4wTyGXKIiPySJDrZzUBsNhssFgsKCwsRFBSkdDnUjg5m5uOPH32HY1b5CiqdRoXxCVH4bWJ3XNuzC7RqZn0iIl/VnL/fDDfUqbhcAjuOZeN/PvsJ39e6TDzQoMENfcJxfe9wXBUXgivCzZAkScFKiYioNoabRjDcEAAIIXD0bCE27s/C9u+ykXfJKapgkxaDulnQJzIQfSID0DsyEPFdzAg2aRl6iIgU0KHCzcqVK/Hiiy/CarVi4MCBSEtLw8iRIxts//nnn2PBggX4/vvv0bVrV/zhD39Aampqkz+P4YYu5XQJHM4qwGc/nMf+0/n4NqsA9kpXvW1NOjW6BhvRLdiIrsFGRAbp0cWsQ6hZj1CzDl0CdAg16xBs1ELD01xERF7TnL/fmnaqqV6bNm3C/PnzsXLlSowYMQKvv/46xo8fj2PHjiE2NrZO+1OnTuGWW27BAw88gLfffht79+7F7NmzER4ejrvuukuBb0D+QK2SkBgXgsS4EACAo9KF41Ybjltt+PF8MU7mFOHH80U4b7Oj1OHETznF+Cnn8vfNMWrVCDBoEKjXwKzXIECv8Vg26tTQa1QwaOVXvVYNQ61lg1Zd6z0VNCoVtGoJGrUKWpX8qlFL0KrkV41KYq8SEREU7rkZNmwYrrrqKqxatcq9rn///pgwYQKWLVtWp/0TTzyBLVu24Pjx4+51qamp+Pbbb7Fv374mfSZ7bqilyiucsBaW41xBGc7ml+HXgjLkFttxsdiBiyUO5JXYkVfiQEFphWI1alSSO/Co1VKtQCTPqyQ5zKkkeZLnAZVKgrpqnUrVQBv3vFTVHrXm5e2qt1FJcActqaqdVDUvueclefmSNqh6VV3yfk37mvXVbVC1P5W7fe1tPbdTVc2jnn1U1wtUtam1XPN+1fomtL+0DS5pU/N+A/u8ZN+1G1122wY+y3NXl6mnzvrGv0dj9VzK4ztdpm1DmrPvhtp7Yx9y+wbW1/uGt/bdwH6ate/m1dKQS9urVRKiLcbm7eQyOkTPjcPhwIEDB/Dkk096rE9JSUFGRka92+zbtw8pKSke68aOHYs1a9agoqICWq22zjZ2ux12u929bLPxWUPUMgatGvFhZsSHmRttV+l0wVZeiRJ7JYrKK1Fsr0SxvQJF5ZUosTtRbK9AcXklyitdKK9wwl7hQnmlU56vWlde4YK90gV71Tp7pRMVToFKpwsVLvnVVc8/SypdApUugXLUf1qNiKg9RATq8fWiMYp9vmLhJjc3F06nE5GRkR7rIyMjkZ2dXe822dnZ9bavrKxEbm4uoqOj62yzbNkyPPvss94rnOgyNGoVQs3y2Ju25HIJVLhcqHQKVDpr5iucLjnkOF2ocC+74HTJ44tcQp7c8y7AKQRcLgGXkOdF1ftOl4CoWifPV60XqDVf1cZV00YAcFWtF4D8WrVe1Fpf3QZV610CEKi9XdV8rfXVbXDJPjz37bmdq2oe7n0Id12uejqv3TVBeC5fst5zHTxm6mzrsX/hsa7uPup/v/GaPPfdpO/TQD3w+LzL1ey5z0vn61NvjQ22bWB9A1s03L7pbRuqpjn7ltt74Xs28EZzSm9OfY23b+gj676h1yo75lDRMTdA3W41IUSj4wbqa1/f+moLFy7EggUL3Ms2mw0xMTEtLZfIZ6hUEvQqNfSK/6+YiMi3KPZ/i2FhYVCr1XV6aXJycur0zlSLioqqt71Go0GXLl3q3Uav10Ov13unaCIiIvJ5ivUb6XQ6JCYmIj093WN9eno6hg8fXu82ycnJddrv2LEDSUlJ9Y63ISIios5H0ZNiCxYswJtvvom1a9fi+PHjeOSRR5CZmem+b83ChQsxbdo0d/vU1FScOXMGCxYswPHjx7F27VqsWbMGjz32mFJfgYiIiHyMomfrJ0+ejLy8PCxduhRWqxUJCQnYtm0b4uLiAABWqxWZmZnu9vHx8di2bRseeeQRvPbaa+jatSv+9re/8R43RERE5Kb4HYrbG+9zQ0RE1PE05+837w9PREREfoXhhoiIiPwKww0RERH5FYYbIiIi8isMN0RERORXGG6IiIjIrzDcEBERkV9huCEiIiK/wnBDREREfkXRxy8oofqGzDabTeFKiIiIqKmq/2435cEKnS7cFBUVAQBiYmIUroSIiIiaq6ioCBaLpdE2ne7ZUi6XC+fOnUNgYCAkSfLqvm02G2JiYpCVlcXnVl0Gj1XT8Vg1HY9V8/B4NR2PVdO11bESQqCoqAhdu3aFStX4qJpO13OjUqnQvXv3Nv2MoKAg/vibiMeq6Xismo7Hqnl4vJqOx6rp2uJYXa7HphoHFBMREZFfYbghIiIiv8Jw40V6vR6LFy+GXq9XuhSfx2PVdDxWTcdj1Tw8Xk3HY9V0vnCsOt2AYiIiIvJv7LkhIiIiv8JwQ0RERH6F4YaIiIj8CsMNERER+RWGGy9ZuXIl4uPjYTAYkJiYiD179ihdUrtbsmQJJEnymKKiotzvCyGwZMkSdO3aFUajETfeeCO+//57j33Y7XbMmzcPYWFhMJvN+M1vfoNff/21vb+K133xxRe4/fbb0bVrV0iShI8//tjjfW8dm/z8fEydOhUWiwUWiwVTp05FQUFBG38777rcsZoxY0ad39m1117r0aazHKtly5bh6quvRmBgICIiIjBhwgScOHHCow1/W7KmHCv+tmSrVq3C4MGD3TfhS05OxieffOJ+v0P8pgS12saNG4VWqxVvvPGGOHbsmPjv//5vYTabxZkzZ5QurV0tXrxYDBw4UFitVveUk5Pjfv/5558XgYGBYvPmzeLo0aNi8uTJIjo6WthsNneb1NRU0a1bN5Geni4OHjwoRo0aJYYMGSIqKyuV+Epes23bNrFo0SKxefNmAUB89NFHHu9769iMGzdOJCQkiIyMDJGRkSESEhLEbbfd1l5f0ysud6ymT58uxo0b5/E7y8vL82jTWY7V2LFjxbp168R3330nDh8+LG699VYRGxsriouL3W3425I15VjxtyXbsmWL2Lp1qzhx4oQ4ceKEeOqpp4RWqxXfffedEKJj/KYYbrzgmmuuEampqR7r+vXrJ5588kmFKlLG4sWLxZAhQ+p9z+VyiaioKPH888+715WXlwuLxSL+/ve/CyGEKCgoEFqtVmzcuNHd5uzZs0KlUolPP/20TWtvT5f+wfbWsTl27JgAIL766it3m3379gkA4ocffmjjb9U2Ggo3d9xxR4PbdNZjJYQQOTk5AoD4/PPPhRD8bTXm0mMlBH9bjQkJCRFvvvlmh/lN8bRUKzkcDhw4cAApKSke61NSUpCRkaFQVco5efIkunbtivj4ePzud7/DL7/8AgA4deoUsrOzPY6TXq/HDTfc4D5OBw4cQEVFhUebrl27IiEhwa+PpbeOzb59+2CxWDBs2DB3m2uvvRYWi8Xvjt/u3bsRERGBPn364IEHHkBOTo77vc58rAoLCwEAoaGhAPjbasylx6oaf1uenE4nNm7ciJKSEiQnJ3eY3xTDTSvl5ubC6XQiMjLSY31kZCSys7MVqkoZw4YNw4YNG7B9+3a88cYbyM7OxvDhw5GXl+c+Fo0dp+zsbOh0OoSEhDTYxh9569hkZ2cjIiKizv4jIiL86viNHz8e77zzDj777DMsX74c+/fvx0033QS73Q6g8x4rIQQWLFiA6667DgkJCQD422pIfccK4G+rtqNHjyIgIAB6vR6pqan46KOPMGDAgA7zm+p0TwVvK5IkeSwLIeqs83fjx493zw8aNAjJycm44oor8NZbb7kH5bXkOHWWY+mNY1Nfe387fpMnT3bPJyQkICkpCXFxcdi6dSsmTpzY4Hb+fqzmzp2LI0eO4Msvv6zzHn9bnho6Vvxt1ejbty8OHz6MgoICbN68GdOnT8fnn3/uft/Xf1PsuWmlsLAwqNXqOkkzJyenTrLtbMxmMwYNGoSTJ0+6r5pq7DhFRUXB4XAgPz+/wTb+yFvHJioqCufPn6+z/wsXLvj18YuOjkZcXBxOnjwJoHMeq3nz5mHLli3YtWsXunfv7l7P31ZdDR2r+nTm35ZOp0OvXr2QlJSEZcuWYciQIXjllVc6zG+K4aaVdDodEhMTkZ6e7rE+PT0dw4cPV6gq32C323H8+HFER0cjPj4eUVFRHsfJ4XDg888/dx+nxMREaLVajzZWqxXfffedXx9Lbx2b5ORkFBYW4uuvv3a3+b//+z8UFhb69fHLy8tDVlYWoqOjAXSuYyWEwNy5c/Hhhx/is88+Q3x8vMf7/G3VuNyxqk9n/m1dSggBu93ecX5TrR6STO5LwdesWSOOHTsm5s+fL8xmszh9+rTSpbWrRx99VOzevVv88ssv4quvvhK33XabCAwMdB+H559/XlgsFvHhhx+Ko0ePirvvvrveywe7d+8udu7cKQ4ePChuuukmv7gUvKioSBw6dEgcOnRIABArVqwQhw4dct8uwFvHZty4cWLw4MFi3759Yt++fWLQoEEd6hJUIRo/VkVFReLRRx8VGRkZ4tSpU2LXrl0iOTlZdOvWrVMeq4cfflhYLBaxe/duj8uXS0tL3W3425Jd7ljxt1Vj4cKF4osvvhCnTp0SR44cEU899ZRQqVRix44dQoiO8ZtiuPGS1157TcTFxQmdTieuuuoqj8sLO4vqex1otVrRtWtXMXHiRPH999+733e5XGLx4sUiKipK6PV6cf3114ujR4967KOsrEzMnTtXhIaGCqPRKG677TaRmZnZ3l/F63bt2iUA1JmmT58uhPDescnLyxNTpkwRgYGBIjAwUEyZMkXk5+e307f0jsaOVWlpqUhJSRHh4eFCq9WK2NhYMX369DrHobMcq/qOEwCxbt06dxv+tmSXO1b8bdW4//773X/PwsPDxejRo93BRoiO8ZuShBCi9f0/RERERL6BY26IiIjIrzDcEBERkV9huCEiIiK/wnBDREREfoXhhoiIiPwKww0RERH5FYYbIiIi8isMN0RERORXGG6IqFOSJAkff/yx0mUQURtguCGidjdjxgxIklRnGjdunNKlEZEf0ChdABF1TuPGjcO6des81un1eoWqISJ/wp4bIlKEXq9HVFSUxxQSEgJAPmW0atUqjB8/HkajEfHx8fjggw88tj969ChuuukmGI1GdOnSBQ8++CCKi4s92qxduxYDBw6EXq9HdHQ05s6d6/F+bm4u7rzzTphMJvTu3Rtbtmxxv5efn48pU6YgPDwcRqMRvXv3rhPGiMg3MdwQkU96+umncdddd+Hbb7/Fvffei7vvvhvHjx8HAJSWlmLcuHEICQnB/v378cEHH2Dnzp0e4WXVqlWYM2cOHnzwQRw9ehRbtmxBr169PD7j2WefxaRJk3DkyBHccsstmDJlCi5evOj+/GPHjuGTTz7B8ePHsWrVKoSFhbXfASCilvPKs8WJiJph+vTpQq1WC7PZ7DEtXbpUCCEEAJGamuqxzbBhw8TDDz8shBBi9erVIiQkRBQXF7vf37p1q1CpVCI7O1sIIUTXrl3FokWLGqwBgPjjH//oXi4uLhaSJIlPPvlECCHE7bffLu677z7vfGEialccc0NEihg1ahRWrVrlsS40NNQ9n5yc7PFecnIyDh8+DAA4fvw4hgwZArPZ7H5/xIgRcLlcOHHiBCRJwrlz5zB69OhGaxg8eLB73mw2IzAwEDk5OQCAhx9+GHfddRcOHjyIlJQUTJgwAcOHD2/RdyWi9sVwQ0SKMJvNdU4TXY4kSQAAIYR7vr42RqOxSfvTarV1tnW5XACA8ePH48yZM9i6dSt27tyJ0aNHY86cOXjppZeaVTMRtT+OuSEin/TVV1/VWe7Xrx8AYMCAATh8+DBKSkrc7+/duxcqlQp9+vRBYGAgevTogf/85z+tqiE8PBwzZszA22+/jbS0NKxevbpV+yOi9sGeGyJShN1uR3Z2tsc6jUbjHrT7wQcfICkpCddddx3eeecdfP3111izZg0AYMqUKVi8eDGmT5+OJUuW4MKFC5g3bx6mTp2KyMhIAMCSJUuQmpqKiIgIjB8/HkVFRdi7dy/mzZvXpPqeeeYZJCYmYuDAgbDb7fj3v/+N/v37e/EIEFFbYbghIkV8+umniI6O9ljXt29f/PDDDwDkK5k2btyI2bNnIyoqCu+88w4GDBgAADCZTNi+fTv++7//G1dffTVMJhPuuusurFixwr2v6dOno7y8HC+//DIee+wxhIWF4be//W2T69PpdFi4cCFOnz4No9GIkSNHYuPGjV745kTU1iQhhFC6CCKi2iRJwkcffYQJEyYoXQoRdUAcc0NERER+heGGiIiI/ArH3BCRz+HZciJqDfbcEBERkV9huCEiIiK/wnBDREREfoXhhoiIiPwKww0RERH5FYYbIiIi8isMN0RERORXGG6IiIjIr/w/GFVC7OIrwssAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_val[y_hat_val>0.5] = 1\n",
    "y_hat_val[y_hat_val<0.5] = 0\n",
    "\n",
    "final_result = y_hat_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 28\n"
     ]
    }
   ],
   "source": [
    "compare = final_result+y_val\n",
    "cor = 0\n",
    "wrong = 0\n",
    "for i in compare[0]:\n",
    "\n",
    "    if i ==2:cor+=1\n",
    "    elif i ==1:wrong+=1\n",
    "\n",
    "print(cor,wrong)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucsfbp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

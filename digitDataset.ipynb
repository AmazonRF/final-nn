{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "from nn import NeuralNetwork\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target  # Not used in autoencoder training but might be useful for other analyses\n",
    "\n",
    "plt.imshow(np.reshape(X[0],(8,8)),cmap='gray')\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000, Training Loss: 7.33390596627105, Validation Loss: 6.903402395714587\n",
      "Epoch 2/3000, Training Loss: 6.9096076988511745, Validation Loss: 6.5276212748900955\n",
      "Epoch 3/3000, Training Loss: 6.532874315246395, Validation Loss: 6.190292307776067\n",
      "Epoch 4/3000, Training Loss: 6.194705162788329, Validation Loss: 5.884754773701103\n",
      "Epoch 5/3000, Training Loss: 5.888420545713339, Validation Loss: 5.606385959716598\n",
      "Epoch 6/3000, Training Loss: 5.609355199492243, Validation Loss: 5.351958932900445\n",
      "Epoch 7/3000, Training Loss: 5.354168271148017, Validation Loss: 5.118640127701027\n",
      "Epoch 8/3000, Training Loss: 5.1202660631111705, Validation Loss: 4.904327117674652\n",
      "Epoch 9/3000, Training Loss: 4.9052555045393875, Validation Loss: 4.707250529067988\n",
      "Epoch 10/3000, Training Loss: 4.707312138252911, Validation Loss: 4.525904009019242\n",
      "Epoch 11/3000, Training Loss: 4.524945511857429, Validation Loss: 4.358997199472926\n",
      "Epoch 12/3000, Training Loss: 4.356899117837812, Validation Loss: 4.205279200374215\n",
      "Epoch 13/3000, Training Loss: 4.202024373603061, Validation Loss: 4.0636093010605165\n",
      "Epoch 14/3000, Training Loss: 4.059093482601211, Validation Loss: 3.932900456639554\n",
      "Epoch 15/3000, Training Loss: 3.9272006626866474, Validation Loss: 3.8121849773574836\n",
      "Epoch 16/3000, Training Loss: 3.8053310833654246, Validation Loss: 3.700650175999618\n",
      "Epoch 17/3000, Training Loss: 3.692582371045863, Validation Loss: 3.5974147238438654\n",
      "Epoch 18/3000, Training Loss: 3.588078852234703, Validation Loss: 3.5017193402456517\n",
      "Epoch 19/3000, Training Loss: 3.4910895656076857, Validation Loss: 3.412989903612576\n",
      "Epoch 20/3000, Training Loss: 3.401014590386078, Validation Loss: 3.3305184852137626\n",
      "Epoch 21/3000, Training Loss: 3.317204344147078, Validation Loss: 3.253667906210879\n",
      "Epoch 22/3000, Training Loss: 3.2390223041486865, Validation Loss: 3.1819253888700025\n",
      "Epoch 23/3000, Training Loss: 3.1659771420997607, Validation Loss: 3.114851974146307\n",
      "Epoch 24/3000, Training Loss: 3.097550359584753, Validation Loss: 3.051962682752457\n",
      "Epoch 25/3000, Training Loss: 3.033319990677564, Validation Loss: 2.9928853913802276\n",
      "Epoch 26/3000, Training Loss: 2.9729181659174557, Validation Loss: 2.9373245948280697\n",
      "Epoch 27/3000, Training Loss: 2.9160064090784883, Validation Loss: 2.8849285578896673\n",
      "Epoch 28/3000, Training Loss: 2.8622170359470074, Validation Loss: 2.8353765391441503\n",
      "Epoch 29/3000, Training Loss: 2.8112647208829267, Validation Loss: 2.7883904774443096\n",
      "Epoch 30/3000, Training Loss: 2.7628431597299166, Validation Loss: 2.743753781063132\n",
      "Epoch 31/3000, Training Loss: 2.716726983865677, Validation Loss: 2.7012260458520605\n",
      "Epoch 32/3000, Training Loss: 2.6726820170810694, Validation Loss: 2.660598123835327\n",
      "Epoch 33/3000, Training Loss: 2.630536195224825, Validation Loss: 2.6216824107699797\n",
      "Epoch 34/3000, Training Loss: 2.5901162517251115, Validation Loss: 2.5843161191336157\n",
      "Epoch 35/3000, Training Loss: 2.551260733323319, Validation Loss: 2.548353842561352\n",
      "Epoch 36/3000, Training Loss: 2.513818227258213, Validation Loss: 2.5136202904564025\n",
      "Epoch 37/3000, Training Loss: 2.4776421235775903, Validation Loss: 2.480046974812072\n",
      "Epoch 38/3000, Training Loss: 2.4426332772241968, Validation Loss: 2.447546378646917\n",
      "Epoch 39/3000, Training Loss: 2.408723931836901, Validation Loss: 2.416058431557244\n",
      "Epoch 40/3000, Training Loss: 2.3758305419623635, Validation Loss: 2.3854869331061033\n",
      "Epoch 41/3000, Training Loss: 2.3438753766999216, Validation Loss: 2.355759453128129\n",
      "Epoch 42/3000, Training Loss: 2.3127950451836488, Validation Loss: 2.326794109703514\n",
      "Epoch 43/3000, Training Loss: 2.2825176983818345, Validation Loss: 2.298554176710872\n",
      "Epoch 44/3000, Training Loss: 2.252984571325221, Validation Loss: 2.2709700850301706\n",
      "Epoch 45/3000, Training Loss: 2.2241368133789883, Validation Loss: 2.243991080438146\n",
      "Epoch 46/3000, Training Loss: 2.195929499943275, Validation Loss: 2.217610477128726\n",
      "Epoch 47/3000, Training Loss: 2.1683486191582775, Validation Loss: 2.191795035600089\n",
      "Epoch 48/3000, Training Loss: 2.141344150618445, Validation Loss: 2.1664939409351085\n",
      "Epoch 49/3000, Training Loss: 2.1148824732942564, Validation Loss: 2.1416683314553895\n",
      "Epoch 50/3000, Training Loss: 2.088954702425658, Validation Loss: 2.117336398081835\n",
      "Epoch 51/3000, Training Loss: 2.0635413436182026, Validation Loss: 2.0934802601765194\n",
      "Epoch 52/3000, Training Loss: 2.038631901703731, Validation Loss: 2.0700839857195077\n",
      "Epoch 53/3000, Training Loss: 2.014200286645493, Validation Loss: 2.0471388062606835\n",
      "Epoch 54/3000, Training Loss: 1.9902340272112464, Validation Loss: 2.024647319800914\n",
      "Epoch 55/3000, Training Loss: 1.9667506479806198, Validation Loss: 2.0025866101207037\n",
      "Epoch 56/3000, Training Loss: 1.9437173964538113, Validation Loss: 1.980934989609708\n",
      "Epoch 57/3000, Training Loss: 1.9211174023597337, Validation Loss: 1.959679882676878\n",
      "Epoch 58/3000, Training Loss: 1.8989412706972266, Validation Loss: 1.9388218600015548\n",
      "Epoch 59/3000, Training Loss: 1.8771786666440802, Validation Loss: 1.9183459167511954\n",
      "Epoch 60/3000, Training Loss: 1.8558149478906576, Validation Loss: 1.898241434854658\n",
      "Epoch 61/3000, Training Loss: 1.8348515045811613, Validation Loss: 1.8784856578612847\n",
      "Epoch 62/3000, Training Loss: 1.814272057040658, Validation Loss: 1.8590823510463244\n",
      "Epoch 63/3000, Training Loss: 1.7940784144753537, Validation Loss: 1.8400402354524226\n",
      "Epoch 64/3000, Training Loss: 1.774272380727226, Validation Loss: 1.8213492163266538\n",
      "Epoch 65/3000, Training Loss: 1.7548470895905488, Validation Loss: 1.802995128700002\n",
      "Epoch 66/3000, Training Loss: 1.7357857636292224, Validation Loss: 1.7849746556477335\n",
      "Epoch 67/3000, Training Loss: 1.7170740768568982, Validation Loss: 1.7672827719173487\n",
      "Epoch 68/3000, Training Loss: 1.6987176811615197, Validation Loss: 1.7499050973396115\n",
      "Epoch 69/3000, Training Loss: 1.6807015182443323, Validation Loss: 1.7328374675250158\n",
      "Epoch 70/3000, Training Loss: 1.6630210444209146, Validation Loss: 1.7160810779863394\n",
      "Epoch 71/3000, Training Loss: 1.6456766654638593, Validation Loss: 1.6996324390944146\n",
      "Epoch 72/3000, Training Loss: 1.6286584135149489, Validation Loss: 1.6834638792949799\n",
      "Epoch 73/3000, Training Loss: 1.6119436718039988, Validation Loss: 1.6675584852133132\n",
      "Epoch 74/3000, Training Loss: 1.5955310450628826, Validation Loss: 1.6519256007339802\n",
      "Epoch 75/3000, Training Loss: 1.5794241125846813, Validation Loss: 1.636564774676231\n",
      "Epoch 76/3000, Training Loss: 1.5636123779109827, Validation Loss: 1.6214604843040994\n",
      "Epoch 77/3000, Training Loss: 1.548090916472805, Validation Loss: 1.6066240089557489\n",
      "Epoch 78/3000, Training Loss: 1.5328479032940567, Validation Loss: 1.592045289301772\n",
      "Epoch 79/3000, Training Loss: 1.5178752804094713, Validation Loss: 1.5777037334903121\n",
      "Epoch 80/3000, Training Loss: 1.503169258260606, Validation Loss: 1.563607222550001\n",
      "Epoch 81/3000, Training Loss: 1.488727541505899, Validation Loss: 1.5497503084037514\n",
      "Epoch 82/3000, Training Loss: 1.474543847455495, Validation Loss: 1.536125293751586\n",
      "Epoch 83/3000, Training Loss: 1.4606108965942333, Validation Loss: 1.5227279160206175\n",
      "Epoch 84/3000, Training Loss: 1.446925845533291, Validation Loss: 1.5095566549203732\n",
      "Epoch 85/3000, Training Loss: 1.4334777705967439, Validation Loss: 1.496613208549672\n",
      "Epoch 86/3000, Training Loss: 1.4202687583474691, Validation Loss: 1.4838869195716797\n",
      "Epoch 87/3000, Training Loss: 1.4072918883399068, Validation Loss: 1.4713705053860366\n",
      "Epoch 88/3000, Training Loss: 1.3945360862909997, Validation Loss: 1.459066520717014\n",
      "Epoch 89/3000, Training Loss: 1.3819949821080524, Validation Loss: 1.4469628556469736\n",
      "Epoch 90/3000, Training Loss: 1.3696679563504266, Validation Loss: 1.4350550791085217\n",
      "Epoch 91/3000, Training Loss: 1.3575545974005423, Validation Loss: 1.4233351220896835\n",
      "Epoch 92/3000, Training Loss: 1.3456492191189418, Validation Loss: 1.4118019943147604\n",
      "Epoch 93/3000, Training Loss: 1.333939785164744, Validation Loss: 1.4004621765711787\n",
      "Epoch 94/3000, Training Loss: 1.3224302963409278, Validation Loss: 1.3893046830260583\n",
      "Epoch 95/3000, Training Loss: 1.3111206854165807, Validation Loss: 1.3783217601737852\n",
      "Epoch 96/3000, Training Loss: 1.2999997141899255, Validation Loss: 1.3675133578396987\n",
      "Epoch 97/3000, Training Loss: 1.2890591044890773, Validation Loss: 1.356875810113609\n",
      "Epoch 98/3000, Training Loss: 1.278294629467932, Validation Loss: 1.3464059566846187\n",
      "Epoch 99/3000, Training Loss: 1.2677056166084546, Validation Loss: 1.3360937868978284\n",
      "Epoch 100/3000, Training Loss: 1.257280430119058, Validation Loss: 1.3259388629516973\n",
      "Epoch 101/3000, Training Loss: 1.2470241156243018, Validation Loss: 1.315945361954316\n",
      "Epoch 102/3000, Training Loss: 1.2369318666644111, Validation Loss: 1.3061020815047741\n",
      "Epoch 103/3000, Training Loss: 1.2269957538834222, Validation Loss: 1.2964029498478062\n",
      "Epoch 104/3000, Training Loss: 1.2172157752994697, Validation Loss: 1.2868531862993646\n",
      "Epoch 105/3000, Training Loss: 1.2075909162173566, Validation Loss: 1.277442891454595\n",
      "Epoch 106/3000, Training Loss: 1.1981162709296942, Validation Loss: 1.2681726663784265\n",
      "Epoch 107/3000, Training Loss: 1.1887905681997426, Validation Loss: 1.259039316521029\n",
      "Epoch 108/3000, Training Loss: 1.1796077231121762, Validation Loss: 1.2500361226363603\n",
      "Epoch 109/3000, Training Loss: 1.1705603200520769, Validation Loss: 1.2411560390734973\n",
      "Epoch 110/3000, Training Loss: 1.1616452867026155, Validation Loss: 1.2324033787552042\n",
      "Epoch 111/3000, Training Loss: 1.1528620298235124, Validation Loss: 1.223773444327289\n",
      "Epoch 112/3000, Training Loss: 1.1442057195895434, Validation Loss: 1.2152643281704618\n",
      "Epoch 113/3000, Training Loss: 1.1356774020044067, Validation Loss: 1.206877590529404\n",
      "Epoch 114/3000, Training Loss: 1.1272756788085347, Validation Loss: 1.1986098600947779\n",
      "Epoch 115/3000, Training Loss: 1.1189960853069028, Validation Loss: 1.1904548088543863\n",
      "Epoch 116/3000, Training Loss: 1.1108324536458916, Validation Loss: 1.182415312054758\n",
      "Epoch 117/3000, Training Loss: 1.1027899697862429, Validation Loss: 1.174487322030337\n",
      "Epoch 118/3000, Training Loss: 1.0948615656392147, Validation Loss: 1.1666679108992566\n",
      "Epoch 119/3000, Training Loss: 1.087044747755631, Validation Loss: 1.1589581488689482\n",
      "Epoch 120/3000, Training Loss: 1.0793381083746059, Validation Loss: 1.1513533951731212\n",
      "Epoch 121/3000, Training Loss: 1.0717355677604783, Validation Loss: 1.1438467379133739\n",
      "Epoch 122/3000, Training Loss: 1.064231719336492, Validation Loss: 1.136433016187759\n",
      "Epoch 123/3000, Training Loss: 1.0568277971275573, Validation Loss: 1.1291122620548442\n",
      "Epoch 124/3000, Training Loss: 1.0495216229958737, Validation Loss: 1.1218860514733136\n",
      "Epoch 125/3000, Training Loss: 1.0423130735712938, Validation Loss: 1.1147537282546365\n",
      "Epoch 126/3000, Training Loss: 1.0352006920076744, Validation Loss: 1.107717336349874\n",
      "Epoch 127/3000, Training Loss: 1.028185742406727, Validation Loss: 1.1007741639749027\n",
      "Epoch 128/3000, Training Loss: 1.0212640502418466, Validation Loss: 1.093918364189933\n",
      "Epoch 129/3000, Training Loss: 1.0144320148619552, Validation Loss: 1.0871530182808955\n",
      "Epoch 130/3000, Training Loss: 1.0076841590329677, Validation Loss: 1.0804665183153344\n",
      "Epoch 131/3000, Training Loss: 1.0010177028776763, Validation Loss: 1.0738622922502337\n",
      "Epoch 132/3000, Training Loss: 0.9944371035460553, Validation Loss: 1.0673415006089408\n",
      "Epoch 133/3000, Training Loss: 0.9879419806785389, Validation Loss: 1.0609030740567982\n",
      "Epoch 134/3000, Training Loss: 0.9815289055028844, Validation Loss: 1.0545475334297385\n",
      "Epoch 135/3000, Training Loss: 0.9751996839280079, Validation Loss: 1.0482723163938465\n",
      "Epoch 136/3000, Training Loss: 0.9689519186917205, Validation Loss: 1.0420712633485354\n",
      "Epoch 137/3000, Training Loss: 0.9627798261587485, Validation Loss: 1.0359444432537663\n",
      "Epoch 138/3000, Training Loss: 0.9566800323734639, Validation Loss: 1.0298929325406865\n",
      "Epoch 139/3000, Training Loss: 0.9506545497200225, Validation Loss: 1.0239077112729236\n",
      "Epoch 140/3000, Training Loss: 0.9446991588628849, Validation Loss: 1.0179952094701397\n",
      "Epoch 141/3000, Training Loss: 0.9388181203023189, Validation Loss: 1.0121554062554308\n",
      "Epoch 142/3000, Training Loss: 0.9330098173707632, Validation Loss: 1.0063803984796917\n",
      "Epoch 143/3000, Training Loss: 0.9272691916842463, Validation Loss: 1.0006741258290661\n",
      "Epoch 144/3000, Training Loss: 0.9215975377385582, Validation Loss: 0.9950382699128699\n",
      "Epoch 145/3000, Training Loss: 0.9159945469171045, Validation Loss: 0.98947122445967\n",
      "Epoch 146/3000, Training Loss: 0.9104577779249757, Validation Loss: 0.983965791745248\n",
      "Epoch 147/3000, Training Loss: 0.904987045047541, Validation Loss: 0.9785218934000742\n",
      "Epoch 148/3000, Training Loss: 0.8995771233642839, Validation Loss: 0.9731407798009267\n",
      "Epoch 149/3000, Training Loss: 0.8942278897799817, Validation Loss: 0.9678211355778713\n",
      "Epoch 150/3000, Training Loss: 0.8889396157864559, Validation Loss: 0.9625626624456506\n",
      "Epoch 151/3000, Training Loss: 0.8837110223117117, Validation Loss: 0.957363994595964\n",
      "Epoch 152/3000, Training Loss: 0.8785420753919545, Validation Loss: 0.9522252717157017\n",
      "Epoch 153/3000, Training Loss: 0.8734334020204458, Validation Loss: 0.9471416182514099\n",
      "Epoch 154/3000, Training Loss: 0.8683829563515956, Validation Loss: 0.9421155894728347\n",
      "Epoch 155/3000, Training Loss: 0.8633897424491378, Validation Loss: 0.9371455048282145\n",
      "Epoch 156/3000, Training Loss: 0.858451253547388, Validation Loss: 0.9322312639506706\n",
      "Epoch 157/3000, Training Loss: 0.8535658287882814, Validation Loss: 0.9273732098946492\n",
      "Epoch 158/3000, Training Loss: 0.84873435972495, Validation Loss: 0.9225704096701173\n",
      "Epoch 159/3000, Training Loss: 0.8439546884095398, Validation Loss: 0.9178209258864896\n",
      "Epoch 160/3000, Training Loss: 0.8392235729170441, Validation Loss: 0.9131246033658603\n",
      "Epoch 161/3000, Training Loss: 0.834544867935167, Validation Loss: 0.9084803123955841\n",
      "Epoch 162/3000, Training Loss: 0.8299179993071734, Validation Loss: 0.9038865977195653\n",
      "Epoch 163/3000, Training Loss: 0.8253427069482461, Validation Loss: 0.8993397655477104\n",
      "Epoch 164/3000, Training Loss: 0.8208166460971619, Validation Loss: 0.8948415820428226\n",
      "Epoch 165/3000, Training Loss: 0.81633869114846, Validation Loss: 0.8903924772433133\n",
      "Epoch 166/3000, Training Loss: 0.8119079124383884, Validation Loss: 0.8859934840375296\n",
      "Epoch 167/3000, Training Loss: 0.8075254567530659, Validation Loss: 0.8816415292443923\n",
      "Epoch 168/3000, Training Loss: 0.8031900264085068, Validation Loss: 0.8773369019788511\n",
      "Epoch 169/3000, Training Loss: 0.7989002476745545, Validation Loss: 0.8730803455630085\n",
      "Epoch 170/3000, Training Loss: 0.7946544109349782, Validation Loss: 0.868868641918619\n",
      "Epoch 171/3000, Training Loss: 0.790453569984484, Validation Loss: 0.8647016753814729\n",
      "Epoch 172/3000, Training Loss: 0.7862965661187118, Validation Loss: 0.860578422612872\n",
      "Epoch 173/3000, Training Loss: 0.7821855613247591, Validation Loss: 0.8564985493622647\n",
      "Epoch 174/3000, Training Loss: 0.7781184133730841, Validation Loss: 0.8524605311557852\n",
      "Epoch 175/3000, Training Loss: 0.7740930835432765, Validation Loss: 0.8484635184251939\n",
      "Epoch 176/3000, Training Loss: 0.7701078858523545, Validation Loss: 0.8445116739778001\n",
      "Epoch 177/3000, Training Loss: 0.7661625250540635, Validation Loss: 0.840595653306711\n",
      "Epoch 178/3000, Training Loss: 0.7622588277721476, Validation Loss: 0.8367169775298983\n",
      "Epoch 179/3000, Training Loss: 0.7583945525029608, Validation Loss: 0.8328760880061572\n",
      "Epoch 180/3000, Training Loss: 0.7545674514840013, Validation Loss: 0.8290745374037225\n",
      "Epoch 181/3000, Training Loss: 0.7507770198716051, Validation Loss: 0.8253123909722162\n",
      "Epoch 182/3000, Training Loss: 0.747024279181308, Validation Loss: 0.8215858970698208\n",
      "Epoch 183/3000, Training Loss: 0.743309924620373, Validation Loss: 0.8178995306364751\n",
      "Epoch 184/3000, Training Loss: 0.739633392057688, Validation Loss: 0.8142527425308016\n",
      "Epoch 185/3000, Training Loss: 0.7359936762573008, Validation Loss: 0.810646194212504\n",
      "Epoch 186/3000, Training Loss: 0.7323894345149683, Validation Loss: 0.8070741187268556\n",
      "Epoch 187/3000, Training Loss: 0.7288185651022536, Validation Loss: 0.8035372202539527\n",
      "Epoch 188/3000, Training Loss: 0.7252822813212721, Validation Loss: 0.8000358732245434\n",
      "Epoch 189/3000, Training Loss: 0.7217798387295626, Validation Loss: 0.7965683794628033\n",
      "Epoch 190/3000, Training Loss: 0.7183104788222365, Validation Loss: 0.7931326447314686\n",
      "Epoch 191/3000, Training Loss: 0.7148759362614849, Validation Loss: 0.7897290037781207\n",
      "Epoch 192/3000, Training Loss: 0.7114748965849181, Validation Loss: 0.7863565620012123\n",
      "Epoch 193/3000, Training Loss: 0.7081041665940602, Validation Loss: 0.7830148082378571\n",
      "Epoch 194/3000, Training Loss: 0.7047648420477328, Validation Loss: 0.7797055185111637\n",
      "Epoch 195/3000, Training Loss: 0.7014567243692855, Validation Loss: 0.7764272970543232\n",
      "Epoch 196/3000, Training Loss: 0.6981800071575474, Validation Loss: 0.7731829824683372\n",
      "Epoch 197/3000, Training Loss: 0.6949331537421397, Validation Loss: 0.7699692446480899\n",
      "Epoch 198/3000, Training Loss: 0.6917150601032237, Validation Loss: 0.7667864184587119\n",
      "Epoch 199/3000, Training Loss: 0.6885244603399064, Validation Loss: 0.7636345860688704\n",
      "Epoch 200/3000, Training Loss: 0.6853611257601161, Validation Loss: 0.7605123934324476\n",
      "Epoch 201/3000, Training Loss: 0.6822280530108812, Validation Loss: 0.7574223574131879\n",
      "Epoch 202/3000, Training Loss: 0.67912649324804, Validation Loss: 0.7543633596763787\n",
      "Epoch 203/3000, Training Loss: 0.6760521066596448, Validation Loss: 0.7513323996196131\n",
      "Epoch 204/3000, Training Loss: 0.673006306925467, Validation Loss: 0.7483313542425483\n",
      "Epoch 205/3000, Training Loss: 0.669989605536401, Validation Loss: 0.7453572579476925\n",
      "Epoch 206/3000, Training Loss: 0.667001261487444, Validation Loss: 0.7424116144764014\n",
      "Epoch 207/3000, Training Loss: 0.664042546889202, Validation Loss: 0.7394959922065635\n",
      "Epoch 208/3000, Training Loss: 0.6611121295830824, Validation Loss: 0.7366078056133464\n",
      "Epoch 209/3000, Training Loss: 0.6582096667560966, Validation Loss: 0.7337463547090778\n",
      "Epoch 210/3000, Training Loss: 0.6553335679889593, Validation Loss: 0.7309120649007147\n",
      "Epoch 211/3000, Training Loss: 0.6524820459868341, Validation Loss: 0.7281032050351207\n",
      "Epoch 212/3000, Training Loss: 0.6496569586651346, Validation Loss: 0.7253207024520467\n",
      "Epoch 213/3000, Training Loss: 0.6468570792522604, Validation Loss: 0.7225629258154044\n",
      "Epoch 214/3000, Training Loss: 0.6440841591173443, Validation Loss: 0.7198323083262248\n",
      "Epoch 215/3000, Training Loss: 0.6413356343925644, Validation Loss: 0.7171269344641144\n",
      "Epoch 216/3000, Training Loss: 0.6386120595218828, Validation Loss: 0.7144456759766574\n",
      "Epoch 217/3000, Training Loss: 0.6359150301932768, Validation Loss: 0.711790609549623\n",
      "Epoch 218/3000, Training Loss: 0.6332424127535421, Validation Loss: 0.7091598133945367\n",
      "Epoch 219/3000, Training Loss: 0.6305937400358732, Validation Loss: 0.7065546171419186\n",
      "Epoch 220/3000, Training Loss: 0.6279693574911799, Validation Loss: 0.7039735465244704\n",
      "Epoch 221/3000, Training Loss: 0.6253682272475584, Validation Loss: 0.7014160037034798\n",
      "Epoch 222/3000, Training Loss: 0.6227885124043276, Validation Loss: 0.6988816192243991\n",
      "Epoch 223/3000, Training Loss: 0.6202325908234321, Validation Loss: 0.6963709854232972\n",
      "Epoch 224/3000, Training Loss: 0.6177000604677659, Validation Loss: 0.6938846600878321\n",
      "Epoch 225/3000, Training Loss: 0.6151901838616404, Validation Loss: 0.6914191953711337\n",
      "Epoch 226/3000, Training Loss: 0.6127014226607883, Validation Loss: 0.6889753374428517\n",
      "Epoch 227/3000, Training Loss: 0.6102332334608571, Validation Loss: 0.6865484772246755\n",
      "Epoch 228/3000, Training Loss: 0.6077837758245439, Validation Loss: 0.684143305712101\n",
      "Epoch 229/3000, Training Loss: 0.60535517308126, Validation Loss: 0.6817582048682422\n",
      "Epoch 230/3000, Training Loss: 0.6029461386868211, Validation Loss: 0.679393397468304\n",
      "Epoch 231/3000, Training Loss: 0.6005559530071545, Validation Loss: 0.6770508905298088\n",
      "Epoch 232/3000, Training Loss: 0.5981864987410767, Validation Loss: 0.6747290044446626\n",
      "Epoch 233/3000, Training Loss: 0.5958373851891012, Validation Loss: 0.6724317349556814\n",
      "Epoch 234/3000, Training Loss: 0.5935074297265149, Validation Loss: 0.670154762923304\n",
      "Epoch 235/3000, Training Loss: 0.5911933468220109, Validation Loss: 0.6678968228141685\n",
      "Epoch 236/3000, Training Loss: 0.5888939251126397, Validation Loss: 0.665659519779275\n",
      "Epoch 237/3000, Training Loss: 0.5866144213908157, Validation Loss: 0.6634407688520599\n",
      "Epoch 238/3000, Training Loss: 0.5843542107124791, Validation Loss: 0.6612409985234803\n",
      "Epoch 239/3000, Training Loss: 0.5821131777525249, Validation Loss: 0.6590616459682056\n",
      "Epoch 240/3000, Training Loss: 0.5798907193143642, Validation Loss: 0.6569009620205134\n",
      "Epoch 241/3000, Training Loss: 0.5776869819255657, Validation Loss: 0.6547590628869118\n",
      "Epoch 242/3000, Training Loss: 0.5755021562461984, Validation Loss: 0.6526359459167933\n",
      "Epoch 243/3000, Training Loss: 0.5733352460848316, Validation Loss: 0.6505323202249987\n",
      "Epoch 244/3000, Training Loss: 0.5711863464153252, Validation Loss: 0.6484465726877976\n",
      "Epoch 245/3000, Training Loss: 0.569055045239414, Validation Loss: 0.6463782700063544\n",
      "Epoch 246/3000, Training Loss: 0.5669381141504195, Validation Loss: 0.6443288687044889\n",
      "Epoch 247/3000, Training Loss: 0.5648325186335679, Validation Loss: 0.6422933633067405\n",
      "Epoch 248/3000, Training Loss: 0.5627443297267211, Validation Loss: 0.6402751742736781\n",
      "Epoch 249/3000, Training Loss: 0.5606732398738955, Validation Loss: 0.6382746713300139\n",
      "Epoch 250/3000, Training Loss: 0.5586189948405109, Validation Loss: 0.636291858708833\n",
      "Epoch 251/3000, Training Loss: 0.5565827365925614, Validation Loss: 0.6343255714031738\n",
      "Epoch 252/3000, Training Loss: 0.5545622416250292, Validation Loss: 0.6323747927260283\n",
      "Epoch 253/3000, Training Loss: 0.5525575087747616, Validation Loss: 0.6304419547360172\n",
      "Epoch 254/3000, Training Loss: 0.5505692668072032, Validation Loss: 0.6285252679678647\n",
      "Epoch 255/3000, Training Loss: 0.5485958119931889, Validation Loss: 0.6266245150661541\n",
      "Epoch 256/3000, Training Loss: 0.5466382907835684, Validation Loss: 0.6247408880485025\n",
      "Epoch 257/3000, Training Loss: 0.5446958662648645, Validation Loss: 0.6228716763486979\n",
      "Epoch 258/3000, Training Loss: 0.5427689785369503, Validation Loss: 0.6210188179446564\n",
      "Epoch 259/3000, Training Loss: 0.5408576624125089, Validation Loss: 0.6191794770085463\n",
      "Epoch 260/3000, Training Loss: 0.5389621291769471, Validation Loss: 0.6173556593764883\n",
      "Epoch 261/3000, Training Loss: 0.5370816049842478, Validation Loss: 0.6155484792345273\n",
      "Epoch 262/3000, Training Loss: 0.5352162885184654, Validation Loss: 0.6137558591923944\n",
      "Epoch 263/3000, Training Loss: 0.5333646540421076, Validation Loss: 0.6119786154193357\n",
      "Epoch 264/3000, Training Loss: 0.5315278042856639, Validation Loss: 0.6102157807000151\n",
      "Epoch 265/3000, Training Loss: 0.5297046385605746, Validation Loss: 0.6084672148570434\n",
      "Epoch 266/3000, Training Loss: 0.5278940403756097, Validation Loss: 0.6067328629556755\n",
      "Epoch 267/3000, Training Loss: 0.5260976189403299, Validation Loss: 0.6050118118847735\n",
      "Epoch 268/3000, Training Loss: 0.5243148753820456, Validation Loss: 0.6033039755143373\n",
      "Epoch 269/3000, Training Loss: 0.5225457887671723, Validation Loss: 0.6016097219366233\n",
      "Epoch 270/3000, Training Loss: 0.5207896332469906, Validation Loss: 0.5999300190154442\n",
      "Epoch 271/3000, Training Loss: 0.5190466597062169, Validation Loss: 0.5982630677850023\n",
      "Epoch 272/3000, Training Loss: 0.5173162324765599, Validation Loss: 0.5966085052332749\n",
      "Epoch 273/3000, Training Loss: 0.5155984064806927, Validation Loss: 0.5949678642480161\n",
      "Epoch 274/3000, Training Loss: 0.5138937124080881, Validation Loss: 0.5933398204457941\n",
      "Epoch 275/3000, Training Loss: 0.512201531336649, Validation Loss: 0.5917240426137484\n",
      "Epoch 276/3000, Training Loss: 0.5105218131013168, Validation Loss: 0.5901216216366547\n",
      "Epoch 277/3000, Training Loss: 0.5088545852246902, Validation Loss: 0.5885314561851732\n",
      "Epoch 278/3000, Training Loss: 0.5071995162130482, Validation Loss: 0.5869523302290176\n",
      "Epoch 279/3000, Training Loss: 0.5055559584036812, Validation Loss: 0.5853857722324524\n",
      "Epoch 280/3000, Training Loss: 0.5039245843758988, Validation Loss: 0.583831269579652\n",
      "Epoch 281/3000, Training Loss: 0.5023048097108992, Validation Loss: 0.5822901068510382\n",
      "Epoch 282/3000, Training Loss: 0.5006974767136098, Validation Loss: 0.5807608881225319\n",
      "Epoch 283/3000, Training Loss: 0.49910230602551287, Validation Loss: 0.5792434869199778\n",
      "Epoch 284/3000, Training Loss: 0.49751870568969925, Validation Loss: 0.5777386441186398\n",
      "Epoch 285/3000, Training Loss: 0.4959474720948061, Validation Loss: 0.5762460651868049\n",
      "Epoch 286/3000, Training Loss: 0.49438716138528344, Validation Loss: 0.5747648550772806\n",
      "Epoch 287/3000, Training Loss: 0.49283774270989417, Validation Loss: 0.5732933449791349\n",
      "Epoch 288/3000, Training Loss: 0.4912991763521493, Validation Loss: 0.5718321258814681\n",
      "Epoch 289/3000, Training Loss: 0.48977213911307244, Validation Loss: 0.5703821832478485\n",
      "Epoch 290/3000, Training Loss: 0.48825602299103393, Validation Loss: 0.5689446363370552\n",
      "Epoch 291/3000, Training Loss: 0.48675089122520987, Validation Loss: 0.5675161042611014\n",
      "Epoch 292/3000, Training Loss: 0.48525724870517284, Validation Loss: 0.5660991049734565\n",
      "Epoch 293/3000, Training Loss: 0.48377382005138236, Validation Loss: 0.5646916327233787\n",
      "Epoch 294/3000, Training Loss: 0.48229985817324117, Validation Loss: 0.563293386553229\n",
      "Epoch 295/3000, Training Loss: 0.4808363565515588, Validation Loss: 0.5619038992611931\n",
      "Epoch 296/3000, Training Loss: 0.4793831147113017, Validation Loss: 0.5605256514802338\n",
      "Epoch 297/3000, Training Loss: 0.47794000710073975, Validation Loss: 0.5591574234704496\n",
      "Epoch 298/3000, Training Loss: 0.4765065043264409, Validation Loss: 0.5578011086901533\n",
      "Epoch 299/3000, Training Loss: 0.4750830169207579, Validation Loss: 0.5564523039709509\n",
      "Epoch 300/3000, Training Loss: 0.47367032610947074, Validation Loss: 0.5551135672873504\n",
      "Epoch 301/3000, Training Loss: 0.47226752489091056, Validation Loss: 0.5537838524928844\n",
      "Epoch 302/3000, Training Loss: 0.4708738612052091, Validation Loss: 0.5524629082942082\n",
      "Epoch 303/3000, Training Loss: 0.4694902228696771, Validation Loss: 0.5511523558367359\n",
      "Epoch 304/3000, Training Loss: 0.46811601964252464, Validation Loss: 0.5498490261029361\n",
      "Epoch 305/3000, Training Loss: 0.46675071133488827, Validation Loss: 0.5485567286429267\n",
      "Epoch 306/3000, Training Loss: 0.46539449358715274, Validation Loss: 0.5472737028940405\n",
      "Epoch 307/3000, Training Loss: 0.464047232313835, Validation Loss: 0.5460007178957903\n",
      "Epoch 308/3000, Training Loss: 0.46270883431252463, Validation Loss: 0.5447363594898829\n",
      "Epoch 309/3000, Training Loss: 0.46137934692532656, Validation Loss: 0.5434816424590959\n",
      "Epoch 310/3000, Training Loss: 0.46005839763282325, Validation Loss: 0.5422352210298851\n",
      "Epoch 311/3000, Training Loss: 0.4587461092327092, Validation Loss: 0.5409966217655877\n",
      "Epoch 312/3000, Training Loss: 0.457442480063415, Validation Loss: 0.5397686275834019\n",
      "Epoch 313/3000, Training Loss: 0.4561473897355278, Validation Loss: 0.5385476387025826\n",
      "Epoch 314/3000, Training Loss: 0.45486007984061777, Validation Loss: 0.5373354462150606\n",
      "Epoch 315/3000, Training Loss: 0.4535810530040631, Validation Loss: 0.5361300265917237\n",
      "Epoch 316/3000, Training Loss: 0.45231005758947307, Validation Loss: 0.5349343588300302\n",
      "Epoch 317/3000, Training Loss: 0.45104717107256725, Validation Loss: 0.5337464276595582\n",
      "Epoch 318/3000, Training Loss: 0.4497922191840045, Validation Loss: 0.5325656218669896\n",
      "Epoch 319/3000, Training Loss: 0.4485455020303094, Validation Loss: 0.5313932357276898\n",
      "Epoch 320/3000, Training Loss: 0.4473067367227637, Validation Loss: 0.5302283147037038\n",
      "Epoch 321/3000, Training Loss: 0.446075778410264, Validation Loss: 0.5290708204619766\n",
      "Epoch 322/3000, Training Loss: 0.44485171610689395, Validation Loss: 0.5279199428387338\n",
      "Epoch 323/3000, Training Loss: 0.44363526277968873, Validation Loss: 0.5267781155848141\n",
      "Epoch 324/3000, Training Loss: 0.4424262451535391, Validation Loss: 0.5256418729719661\n",
      "Epoch 325/3000, Training Loss: 0.44122485859935207, Validation Loss: 0.5245144785851154\n",
      "Epoch 326/3000, Training Loss: 0.4400314324758027, Validation Loss: 0.5233930714364556\n",
      "Epoch 327/3000, Training Loss: 0.43884623519132393, Validation Loss: 0.5222827315676238\n",
      "Epoch 328/3000, Training Loss: 0.437669107886012, Validation Loss: 0.5211803384894862\n",
      "Epoch 329/3000, Training Loss: 0.43649844286634076, Validation Loss: 0.5200840793874836\n",
      "Epoch 330/3000, Training Loss: 0.435334607326393, Validation Loss: 0.5189946802240296\n",
      "Epoch 331/3000, Training Loss: 0.4341778557973512, Validation Loss: 0.5179130808263536\n",
      "Epoch 332/3000, Training Loss: 0.43302810759045807, Validation Loss: 0.5168394418344469\n",
      "Epoch 333/3000, Training Loss: 0.43188456593708596, Validation Loss: 0.5157718482481664\n",
      "Epoch 334/3000, Training Loss: 0.4307480209284743, Validation Loss: 0.5147115279546681\n",
      "Epoch 335/3000, Training Loss: 0.42961868745775317, Validation Loss: 0.5136580093161944\n",
      "Epoch 336/3000, Training Loss: 0.4284959923853512, Validation Loss: 0.5126071929317879\n",
      "Epoch 337/3000, Training Loss: 0.42737816389337363, Validation Loss: 0.5115622702740661\n",
      "Epoch 338/3000, Training Loss: 0.42626714170825897, Validation Loss: 0.5105249596456068\n",
      "Epoch 339/3000, Training Loss: 0.4251633044775072, Validation Loss: 0.50949365785192\n",
      "Epoch 340/3000, Training Loss: 0.4240662779266676, Validation Loss: 0.5084688177302646\n",
      "Epoch 341/3000, Training Loss: 0.4229757592562654, Validation Loss: 0.5074507798133732\n",
      "Epoch 342/3000, Training Loss: 0.4218916740079761, Validation Loss: 0.5064390097814448\n",
      "Epoch 343/3000, Training Loss: 0.42081413061443174, Validation Loss: 0.505432584646363\n",
      "Epoch 344/3000, Training Loss: 0.4197427969720963, Validation Loss: 0.5044333820905573\n",
      "Epoch 345/3000, Training Loss: 0.41867803673584114, Validation Loss: 0.5034393758121605\n",
      "Epoch 346/3000, Training Loss: 0.4176198617372079, Validation Loss: 0.5024515886282909\n",
      "Epoch 347/3000, Training Loss: 0.41656803164241635, Validation Loss: 0.5014695925288928\n",
      "Epoch 348/3000, Training Loss: 0.4155215157158743, Validation Loss: 0.5004947857774975\n",
      "Epoch 349/3000, Training Loss: 0.4144811730288172, Validation Loss: 0.4995244249008438\n",
      "Epoch 350/3000, Training Loss: 0.41344696683060844, Validation Loss: 0.49855996595466334\n",
      "Epoch 351/3000, Training Loss: 0.41241867325510234, Validation Loss: 0.4976025373489553\n",
      "Epoch 352/3000, Training Loss: 0.4113964122943809, Validation Loss: 0.4966480072719225\n",
      "Epoch 353/3000, Training Loss: 0.41038029540053544, Validation Loss: 0.49570022641470174\n",
      "Epoch 354/3000, Training Loss: 0.4093703832167451, Validation Loss: 0.49475867875129964\n",
      "Epoch 355/3000, Training Loss: 0.4083663065748659, Validation Loss: 0.4938212502253434\n",
      "Epoch 356/3000, Training Loss: 0.4073677626100714, Validation Loss: 0.49289028663845064\n",
      "Epoch 357/3000, Training Loss: 0.4063749860558399, Validation Loss: 0.49196603921530624\n",
      "Epoch 358/3000, Training Loss: 0.4053880922167929, Validation Loss: 0.4910475567680616\n",
      "Epoch 359/3000, Training Loss: 0.40440740533581243, Validation Loss: 0.49013426589882764\n",
      "Epoch 360/3000, Training Loss: 0.40343239415065607, Validation Loss: 0.48922726119493015\n",
      "Epoch 361/3000, Training Loss: 0.4024627825987626, Validation Loss: 0.4883236394261655\n",
      "Epoch 362/3000, Training Loss: 0.40149844125224954, Validation Loss: 0.487426474197809\n",
      "Epoch 363/3000, Training Loss: 0.4005396686963019, Validation Loss: 0.4865348375385583\n",
      "Epoch 364/3000, Training Loss: 0.39958644721514114, Validation Loss: 0.48564695296383825\n",
      "Epoch 365/3000, Training Loss: 0.398638649490353, Validation Loss: 0.48476285911119343\n",
      "Epoch 366/3000, Training Loss: 0.39769637847594, Validation Loss: 0.4838846540472686\n",
      "Epoch 367/3000, Training Loss: 0.39675898637936646, Validation Loss: 0.48301160212109223\n",
      "Epoch 368/3000, Training Loss: 0.3958264423681845, Validation Loss: 0.48214349370920423\n",
      "Epoch 369/3000, Training Loss: 0.3948995113956604, Validation Loss: 0.48127595651423327\n",
      "Epoch 370/3000, Training Loss: 0.39397766408675505, Validation Loss: 0.4804126557837527\n",
      "Epoch 371/3000, Training Loss: 0.39306094038363343, Validation Loss: 0.47955449770062586\n",
      "Epoch 372/3000, Training Loss: 0.392149274127777, Validation Loss: 0.47870068313592595\n",
      "Epoch 373/3000, Training Loss: 0.39124265828635807, Validation Loss: 0.47785201012584494\n",
      "Epoch 374/3000, Training Loss: 0.3903408138762493, Validation Loss: 0.4770080399519371\n",
      "Epoch 375/3000, Training Loss: 0.38944362372296454, Validation Loss: 0.47616802483279985\n",
      "Epoch 376/3000, Training Loss: 0.38855136847673655, Validation Loss: 0.47533341857144074\n",
      "Epoch 377/3000, Training Loss: 0.3876637614391214, Validation Loss: 0.47450193110022004\n",
      "Epoch 378/3000, Training Loss: 0.3867803754070263, Validation Loss: 0.47367404805947394\n",
      "Epoch 379/3000, Training Loss: 0.3859018291276797, Validation Loss: 0.47285198590628735\n",
      "Epoch 380/3000, Training Loss: 0.385027915213408, Validation Loss: 0.47203308097095803\n",
      "Epoch 381/3000, Training Loss: 0.3841586472135165, Validation Loss: 0.4712195848465183\n",
      "Epoch 382/3000, Training Loss: 0.3832936342923186, Validation Loss: 0.47041022602942123\n",
      "Epoch 383/3000, Training Loss: 0.38243186297423715, Validation Loss: 0.46960540462096406\n",
      "Epoch 384/3000, Training Loss: 0.38157466626443737, Validation Loss: 0.46880669016783133\n",
      "Epoch 385/3000, Training Loss: 0.3807220845158753, Validation Loss: 0.4680107015607018\n",
      "Epoch 386/3000, Training Loss: 0.37987432238988605, Validation Loss: 0.46721986533199783\n",
      "Epoch 387/3000, Training Loss: 0.3790313668227986, Validation Loss: 0.46643285188655553\n",
      "Epoch 388/3000, Training Loss: 0.3781928967093974, Validation Loss: 0.46564956043484185\n",
      "Epoch 389/3000, Training Loss: 0.3773591119647856, Validation Loss: 0.46487104345683483\n",
      "Epoch 390/3000, Training Loss: 0.3765296689581623, Validation Loss: 0.4640956970563788\n",
      "Epoch 391/3000, Training Loss: 0.3757044915374536, Validation Loss: 0.4633249178697219\n",
      "Epoch 392/3000, Training Loss: 0.3748842181727658, Validation Loss: 0.4625578617325508\n",
      "Epoch 393/3000, Training Loss: 0.374068255793191, Validation Loss: 0.46179430821098466\n",
      "Epoch 394/3000, Training Loss: 0.3732568390758625, Validation Loss: 0.4610356816102743\n",
      "Epoch 395/3000, Training Loss: 0.37244886915777536, Validation Loss: 0.4602797443865686\n",
      "Epoch 396/3000, Training Loss: 0.37164457467917117, Validation Loss: 0.4595295216836083\n",
      "Epoch 397/3000, Training Loss: 0.3708446236615813, Validation Loss: 0.45878178864607255\n",
      "Epoch 398/3000, Training Loss: 0.3700484407813389, Validation Loss: 0.4580378449978043\n",
      "Epoch 399/3000, Training Loss: 0.36925607903411495, Validation Loss: 0.4572987385879841\n",
      "Epoch 400/3000, Training Loss: 0.36846760309884025, Validation Loss: 0.4565630676472616\n",
      "Epoch 401/3000, Training Loss: 0.3676832169957632, Validation Loss: 0.4558322007940308\n",
      "Epoch 402/3000, Training Loss: 0.3669029010955871, Validation Loss: 0.45510511470111525\n",
      "Epoch 403/3000, Training Loss: 0.3661262301935552, Validation Loss: 0.4543824665433712\n",
      "Epoch 404/3000, Training Loss: 0.3653532997974305, Validation Loss: 0.45366271488938853\n",
      "Epoch 405/3000, Training Loss: 0.3645843150317608, Validation Loss: 0.45294658748756833\n",
      "Epoch 406/3000, Training Loss: 0.36381906924849716, Validation Loss: 0.45223652864739944\n",
      "Epoch 407/3000, Training Loss: 0.36305576901808423, Validation Loss: 0.45152953953519154\n",
      "Epoch 408/3000, Training Loss: 0.36229621140272344, Validation Loss: 0.4508261865156187\n",
      "Epoch 409/3000, Training Loss: 0.3615402910848365, Validation Loss: 0.4501255127166033\n",
      "Epoch 410/3000, Training Loss: 0.3607880581073344, Validation Loss: 0.44942890727972845\n",
      "Epoch 411/3000, Training Loss: 0.36003914237241397, Validation Loss: 0.4487357906074903\n",
      "Epoch 412/3000, Training Loss: 0.35929397800864626, Validation Loss: 0.4480473486182758\n",
      "Epoch 413/3000, Training Loss: 0.358552363894751, Validation Loss: 0.44736046945269625\n",
      "Epoch 414/3000, Training Loss: 0.35781392228981623, Validation Loss: 0.4466771426578733\n",
      "Epoch 415/3000, Training Loss: 0.35707861622610126, Validation Loss: 0.44599766668839547\n",
      "Epoch 416/3000, Training Loss: 0.3563472276989318, Validation Loss: 0.44532321893133375\n",
      "Epoch 417/3000, Training Loss: 0.3556194259315489, Validation Loss: 0.4446503126790687\n",
      "Epoch 418/3000, Training Loss: 0.35489437607248087, Validation Loss: 0.4439799394716216\n",
      "Epoch 419/3000, Training Loss: 0.3541719825335849, Validation Loss: 0.4433124972300869\n",
      "Epoch 420/3000, Training Loss: 0.3534530762622833, Validation Loss: 0.4426484351648636\n",
      "Epoch 421/3000, Training Loss: 0.3527379817371217, Validation Loss: 0.4419890929150408\n",
      "Epoch 422/3000, Training Loss: 0.35202638144510195, Validation Loss: 0.4413307458271264\n",
      "Epoch 423/3000, Training Loss: 0.3513180735198571, Validation Loss: 0.4406768626839315\n",
      "Epoch 424/3000, Training Loss: 0.3506132647997471, Validation Loss: 0.44002657318961225\n",
      "Epoch 425/3000, Training Loss: 0.3499117097580001, Validation Loss: 0.43937951441920914\n",
      "Epoch 426/3000, Training Loss: 0.3492135385580091, Validation Loss: 0.43873582296942204\n",
      "Epoch 427/3000, Training Loss: 0.34851865859865533, Validation Loss: 0.4380942599186579\n",
      "Epoch 428/3000, Training Loss: 0.34782665997278095, Validation Loss: 0.4374557290896653\n",
      "Epoch 429/3000, Training Loss: 0.3471380504390846, Validation Loss: 0.43682073613260003\n",
      "Epoch 430/3000, Training Loss: 0.3464524725859349, Validation Loss: 0.4361885539178479\n",
      "Epoch 431/3000, Training Loss: 0.34576968383716533, Validation Loss: 0.43555949050883147\n",
      "Epoch 432/3000, Training Loss: 0.3450898115966293, Validation Loss: 0.4349337283017914\n",
      "Epoch 433/3000, Training Loss: 0.3444129592300963, Validation Loss: 0.4343106860441593\n",
      "Epoch 434/3000, Training Loss: 0.3437392656939624, Validation Loss: 0.4336905384788515\n",
      "Epoch 435/3000, Training Loss: 0.34306862689975987, Validation Loss: 0.4330718881936433\n",
      "Epoch 436/3000, Training Loss: 0.34240102646646187, Validation Loss: 0.43245840230185373\n",
      "Epoch 437/3000, Training Loss: 0.34173663337861954, Validation Loss: 0.43184845355983625\n",
      "Epoch 438/3000, Training Loss: 0.3410750012494294, Validation Loss: 0.43124072745646036\n",
      "Epoch 439/3000, Training Loss: 0.3404163808172538, Validation Loss: 0.4306355678515086\n",
      "Epoch 440/3000, Training Loss: 0.33976071306366445, Validation Loss: 0.4300330182894321\n",
      "Epoch 441/3000, Training Loss: 0.33910791723899303, Validation Loss: 0.42943183821015557\n",
      "Epoch 442/3000, Training Loss: 0.3384579705096281, Validation Loss: 0.428834584640626\n",
      "Epoch 443/3000, Training Loss: 0.33781128103263397, Validation Loss: 0.4282395990644743\n",
      "Epoch 444/3000, Training Loss: 0.33716753278618683, Validation Loss: 0.4276484470325329\n",
      "Epoch 445/3000, Training Loss: 0.336526730680397, Validation Loss: 0.42705908716164553\n",
      "Epoch 446/3000, Training Loss: 0.3358886250588341, Validation Loss: 0.4264714515742156\n",
      "Epoch 447/3000, Training Loss: 0.33525349099723567, Validation Loss: 0.4258885641573367\n",
      "Epoch 448/3000, Training Loss: 0.33462114754937206, Validation Loss: 0.4253082068291378\n",
      "Epoch 449/3000, Training Loss: 0.33399157387105544, Validation Loss: 0.4247294971410406\n",
      "Epoch 450/3000, Training Loss: 0.33336478646371737, Validation Loss: 0.4241541123641197\n",
      "Epoch 451/3000, Training Loss: 0.33274095152251504, Validation Loss: 0.42358602499385795\n",
      "Epoch 452/3000, Training Loss: 0.332120207893131, Validation Loss: 0.42302209839652694\n",
      "Epoch 453/3000, Training Loss: 0.3315024082296535, Validation Loss: 0.4224607681389512\n",
      "Epoch 454/3000, Training Loss: 0.33088728740379325, Validation Loss: 0.42189830182765936\n",
      "Epoch 455/3000, Training Loss: 0.3302750065347477, Validation Loss: 0.42134211419297335\n",
      "Epoch 456/3000, Training Loss: 0.3296652780579378, Validation Loss: 0.42078887211183824\n",
      "Epoch 457/3000, Training Loss: 0.32905817740256954, Validation Loss: 0.4202375673154086\n",
      "Epoch 458/3000, Training Loss: 0.328453689532628, Validation Loss: 0.41968474810583956\n",
      "Epoch 459/3000, Training Loss: 0.32785220644531743, Validation Loss: 0.41913913748056614\n",
      "Epoch 460/3000, Training Loss: 0.32725328414434834, Validation Loss: 0.4185952959780898\n",
      "Epoch 461/3000, Training Loss: 0.32665700492753846, Validation Loss: 0.4180544934474886\n",
      "Epoch 462/3000, Training Loss: 0.32606351879809237, Validation Loss: 0.41751691201822233\n",
      "Epoch 463/3000, Training Loss: 0.3254728520145158, Validation Loss: 0.41697740288915514\n",
      "Epoch 464/3000, Training Loss: 0.324884692160341, Validation Loss: 0.4164440298967961\n",
      "Epoch 465/3000, Training Loss: 0.3242991065716472, Validation Loss: 0.41591287791172765\n",
      "Epoch 466/3000, Training Loss: 0.32371591366554614, Validation Loss: 0.4153846303362512\n",
      "Epoch 467/3000, Training Loss: 0.32313549431573674, Validation Loss: 0.4148544234434835\n",
      "Epoch 468/3000, Training Loss: 0.32255767655831574, Validation Loss: 0.4143307857231667\n",
      "Epoch 469/3000, Training Loss: 0.32198206157153003, Validation Loss: 0.41380863230491294\n",
      "Epoch 470/3000, Training Loss: 0.32140862450797236, Validation Loss: 0.4132898108984907\n",
      "Epoch 471/3000, Training Loss: 0.3208376408076924, Validation Loss: 0.4127721255843356\n",
      "Epoch 472/3000, Training Loss: 0.3202690153975628, Validation Loss: 0.412252633738922\n",
      "Epoch 473/3000, Training Loss: 0.3197026455541476, Validation Loss: 0.41173926334936645\n",
      "Epoch 474/3000, Training Loss: 0.3191387708124854, Validation Loss: 0.4112289372153138\n",
      "Epoch 475/3000, Training Loss: 0.31857740724297035, Validation Loss: 0.41071506104653344\n",
      "Epoch 476/3000, Training Loss: 0.31801840483439286, Validation Loss: 0.4102099252249289\n",
      "Epoch 477/3000, Training Loss: 0.3174613751836563, Validation Loss: 0.4097054053356142\n",
      "Epoch 478/3000, Training Loss: 0.3169063090412815, Validation Loss: 0.40920324377591505\n",
      "Epoch 479/3000, Training Loss: 0.31635355253978326, Validation Loss: 0.408698121443725\n",
      "Epoch 480/3000, Training Loss: 0.3158028441826491, Validation Loss: 0.4082014850195713\n",
      "Epoch 481/3000, Training Loss: 0.315254224020122, Validation Loss: 0.40770496154763947\n",
      "Epoch 482/3000, Training Loss: 0.3147072832744629, Validation Loss: 0.4072122638537367\n",
      "Epoch 483/3000, Training Loss: 0.3141628310864413, Validation Loss: 0.4067165325816423\n",
      "Epoch 484/3000, Training Loss: 0.31362064369536946, Validation Loss: 0.40622771557321613\n",
      "Epoch 485/3000, Training Loss: 0.3130808819625064, Validation Loss: 0.40574146623623797\n",
      "Epoch 486/3000, Training Loss: 0.31254314926849536, Validation Loss: 0.4052534582961861\n",
      "Epoch 487/3000, Training Loss: 0.31200765008592907, Validation Loss: 0.4047702376143248\n",
      "Epoch 488/3000, Training Loss: 0.31147400423175287, Validation Loss: 0.40428937930906156\n",
      "Epoch 489/3000, Training Loss: 0.31094225138752424, Validation Loss: 0.40381026836778017\n",
      "Epoch 490/3000, Training Loss: 0.31041227687726736, Validation Loss: 0.40332935432824013\n",
      "Epoch 491/3000, Training Loss: 0.30988374618594045, Validation Loss: 0.40285312495608094\n",
      "Epoch 492/3000, Training Loss: 0.3093570639482896, Validation Loss: 0.4023794549474627\n",
      "Epoch 493/3000, Training Loss: 0.3088323883883989, Validation Loss: 0.4019082490849373\n",
      "Epoch 494/3000, Training Loss: 0.3083097536630358, Validation Loss: 0.40143393056477145\n",
      "Epoch 495/3000, Training Loss: 0.3077896402904447, Validation Loss: 0.4009642641337344\n",
      "Epoch 496/3000, Training Loss: 0.30727128497772277, Validation Loss: 0.4004981949450816\n",
      "Epoch 497/3000, Training Loss: 0.3067548583424108, Validation Loss: 0.40003355007760627\n",
      "Epoch 498/3000, Training Loss: 0.30624056941616257, Validation Loss: 0.39956579935917935\n",
      "Epoch 499/3000, Training Loss: 0.30572813168360596, Validation Loss: 0.3991043704951423\n",
      "Epoch 500/3000, Training Loss: 0.30521754101703824, Validation Loss: 0.3986441137714884\n",
      "Epoch 501/3000, Training Loss: 0.3047089973524603, Validation Loss: 0.39818763972666116\n",
      "Epoch 502/3000, Training Loss: 0.3042026291651108, Validation Loss: 0.3977265503647685\n",
      "Epoch 503/3000, Training Loss: 0.30369793237867043, Validation Loss: 0.39727230899928906\n",
      "Epoch 504/3000, Training Loss: 0.3031953167076617, Validation Loss: 0.39682198563966137\n",
      "Epoch 505/3000, Training Loss: 0.30269483413877474, Validation Loss: 0.39636915017720675\n",
      "Epoch 506/3000, Training Loss: 0.30219644784104566, Validation Loss: 0.3959193451985709\n",
      "Epoch 507/3000, Training Loss: 0.3017004329829121, Validation Loss: 0.39547181019106314\n",
      "Epoch 508/3000, Training Loss: 0.3012062525700384, Validation Loss: 0.39502526547417066\n",
      "Epoch 509/3000, Training Loss: 0.30071391564889505, Validation Loss: 0.3945762723379128\n",
      "Epoch 510/3000, Training Loss: 0.30022334360426906, Validation Loss: 0.39413416076378943\n",
      "Epoch 511/3000, Training Loss: 0.2997346311757739, Validation Loss: 0.3936932925217694\n",
      "Epoch 512/3000, Training Loss: 0.2992477798822156, Validation Loss: 0.3932557184641516\n",
      "Epoch 513/3000, Training Loss: 0.29876279143306667, Validation Loss: 0.39281417938540847\n",
      "Epoch 514/3000, Training Loss: 0.29827957460902854, Validation Loss: 0.3923778668242811\n",
      "Epoch 515/3000, Training Loss: 0.2977974905570698, Validation Loss: 0.3919437088054434\n",
      "Epoch 516/3000, Training Loss: 0.29731719917941346, Validation Loss: 0.39150975191585813\n",
      "Epoch 517/3000, Training Loss: 0.2968386475453924, Validation Loss: 0.3910736946338487\n",
      "Epoch 518/3000, Training Loss: 0.2963617990426581, Validation Loss: 0.3906430447367353\n",
      "Epoch 519/3000, Training Loss: 0.29588675953595145, Validation Loss: 0.3902154122855282\n",
      "Epoch 520/3000, Training Loss: 0.2954134780923632, Validation Loss: 0.38978437299239915\n",
      "Epoch 521/3000, Training Loss: 0.2949421081726526, Validation Loss: 0.38935839917230974\n",
      "Epoch 522/3000, Training Loss: 0.29447229144627063, Validation Loss: 0.3889354374038222\n",
      "Epoch 523/3000, Training Loss: 0.29400392486490995, Validation Loss: 0.38851267396504313\n",
      "Epoch 524/3000, Training Loss: 0.2935372679410622, Validation Loss: 0.3880877740641544\n",
      "Epoch 525/3000, Training Loss: 0.2930723601688049, Validation Loss: 0.38766987772856387\n",
      "Epoch 526/3000, Training Loss: 0.2926092886824483, Validation Loss: 0.3872516337702592\n",
      "Epoch 527/3000, Training Loss: 0.29214797091277045, Validation Loss: 0.3868356808525296\n",
      "Epoch 528/3000, Training Loss: 0.2916884132173447, Validation Loss: 0.3864155367085488\n",
      "Epoch 529/3000, Training Loss: 0.2912305948468626, Validation Loss: 0.3860031312670885\n",
      "Epoch 530/3000, Training Loss: 0.29077446126240086, Validation Loss: 0.385592264880664\n",
      "Epoch 531/3000, Training Loss: 0.29032059183105746, Validation Loss: 0.3851790231464575\n",
      "Epoch 532/3000, Training Loss: 0.2898688127437047, Validation Loss: 0.384771047259792\n",
      "Epoch 533/3000, Training Loss: 0.28941834546062184, Validation Loss: 0.384365284250483\n",
      "Epoch 534/3000, Training Loss: 0.2889696551351847, Validation Loss: 0.3839616965323016\n",
      "Epoch 535/3000, Training Loss: 0.2885225927619193, Validation Loss: 0.3835553897785539\n",
      "Epoch 536/3000, Training Loss: 0.28807698700065193, Validation Loss: 0.3831526286782046\n",
      "Epoch 537/3000, Training Loss: 0.2876327743841682, Validation Loss: 0.3827541145022988\n",
      "Epoch 538/3000, Training Loss: 0.2871895279620423, Validation Loss: 0.3823506864983816\n",
      "Epoch 539/3000, Training Loss: 0.28674789027676306, Validation Loss: 0.3819555058386205\n",
      "Epoch 540/3000, Training Loss: 0.2863075673668591, Validation Loss: 0.3815606747634165\n",
      "Epoch 541/3000, Training Loss: 0.2858690120576882, Validation Loss: 0.3811624069151193\n",
      "Epoch 542/3000, Training Loss: 0.28543176407759274, Validation Loss: 0.3807705294668422\n",
      "Epoch 543/3000, Training Loss: 0.2849960414679162, Validation Loss: 0.3803800939361082\n",
      "Epoch 544/3000, Training Loss: 0.2845620534949003, Validation Loss: 0.3799870392778821\n",
      "Epoch 545/3000, Training Loss: 0.28412932727999146, Validation Loss: 0.37960034499503353\n",
      "Epoch 546/3000, Training Loss: 0.2836978326345104, Validation Loss: 0.37921337003292\n",
      "Epoch 547/3000, Training Loss: 0.28326794850785764, Validation Loss: 0.37882523266510904\n",
      "Epoch 548/3000, Training Loss: 0.28283958342080556, Validation Loss: 0.3784408524158617\n",
      "Epoch 549/3000, Training Loss: 0.28241312850762584, Validation Loss: 0.3780590451251921\n",
      "Epoch 550/3000, Training Loss: 0.2819879667163964, Validation Loss: 0.3776727211380521\n",
      "Epoch 551/3000, Training Loss: 0.28156454670201275, Validation Loss: 0.37729195254208897\n",
      "Epoch 552/3000, Training Loss: 0.2811426251988635, Validation Loss: 0.3769136152310434\n",
      "Epoch 553/3000, Training Loss: 0.28072192193259454, Validation Loss: 0.3765302933466267\n",
      "Epoch 554/3000, Training Loss: 0.28030265873983645, Validation Loss: 0.37615450565386416\n",
      "Epoch 555/3000, Training Loss: 0.27988527004319597, Validation Loss: 0.3757816392006092\n",
      "Epoch 556/3000, Training Loss: 0.27946938921043873, Validation Loss: 0.37540470501538314\n",
      "Epoch 557/3000, Training Loss: 0.2790547866740945, Validation Loss: 0.3750328137699709\n",
      "Epoch 558/3000, Training Loss: 0.2786416109025185, Validation Loss: 0.3746621572379437\n",
      "Epoch 559/3000, Training Loss: 0.278229653812477, Validation Loss: 0.37428811760323666\n",
      "Epoch 560/3000, Training Loss: 0.2778188195214171, Validation Loss: 0.3739201310174675\n",
      "Epoch 561/3000, Training Loss: 0.2774094965466848, Validation Loss: 0.3735539687043295\n",
      "Epoch 562/3000, Training Loss: 0.2770014127373204, Validation Loss: 0.3731843620876887\n",
      "Epoch 563/3000, Training Loss: 0.27659463734911977, Validation Loss: 0.37282013754320126\n",
      "Epoch 564/3000, Training Loss: 0.2761879692598857, Validation Loss: 0.3724586077238295\n",
      "Epoch 565/3000, Training Loss: 0.2757817440158265, Validation Loss: 0.3720910766327006\n",
      "Epoch 566/3000, Training Loss: 0.27537679850773084, Validation Loss: 0.37173163535523585\n",
      "Epoch 567/3000, Training Loss: 0.2749732426464775, Validation Loss: 0.371371812923762\n",
      "Epoch 568/3000, Training Loss: 0.2745711960288341, Validation Loss: 0.37100985516590157\n",
      "Epoch 569/3000, Training Loss: 0.27417028177079306, Validation Loss: 0.37065374714224997\n",
      "Epoch 570/3000, Training Loss: 0.27377069439326057, Validation Loss: 0.3702984917887446\n",
      "Epoch 571/3000, Training Loss: 0.273372560031237, Validation Loss: 0.36994012865697\n",
      "Epoch 572/3000, Training Loss: 0.27297533607937813, Validation Loss: 0.3695875351357057\n",
      "Epoch 573/3000, Training Loss: 0.2725795340205505, Validation Loss: 0.3692368565587229\n",
      "Epoch 574/3000, Training Loss: 0.27218493652730547, Validation Loss: 0.3688809200408918\n",
      "Epoch 575/3000, Training Loss: 0.2717916338038789, Validation Loss: 0.36853158903389527\n",
      "Epoch 576/3000, Training Loss: 0.2713996006189971, Validation Loss: 0.36818600809313373\n",
      "Epoch 577/3000, Training Loss: 0.2710089720513847, Validation Loss: 0.3678338200186842\n",
      "Epoch 578/3000, Training Loss: 0.2706197734481384, Validation Loss: 0.36748857086766706\n",
      "Epoch 579/3000, Training Loss: 0.27023168537648756, Validation Loss: 0.36714651466548964\n",
      "Epoch 580/3000, Training Loss: 0.26984512576797465, Validation Loss: 0.366799607294873\n",
      "Epoch 581/3000, Training Loss: 0.2694595136442154, Validation Loss: 0.3664569206901577\n",
      "Epoch 582/3000, Training Loss: 0.26907495732814624, Validation Loss: 0.36611795712243295\n",
      "Epoch 583/3000, Training Loss: 0.26869200267446103, Validation Loss: 0.3657752368256373\n",
      "Epoch 584/3000, Training Loss: 0.2683098616237439, Validation Loss: 0.3654362379379681\n",
      "Epoch 585/3000, Training Loss: 0.267928894333692, Validation Loss: 0.36509887002013464\n",
      "Epoch 586/3000, Training Loss: 0.26754916323355854, Validation Loss: 0.36475625693673774\n",
      "Epoch 587/3000, Training Loss: 0.26717064640173727, Validation Loss: 0.3644199822005592\n",
      "Epoch 588/3000, Training Loss: 0.2667931176637274, Validation Loss: 0.3640830157679981\n",
      "Epoch 589/3000, Training Loss: 0.26641681628650377, Validation Loss: 0.36374451142966907\n",
      "Epoch 590/3000, Training Loss: 0.26604164162642197, Validation Loss: 0.36341098808190153\n",
      "Epoch 591/3000, Training Loss: 0.26566761336672523, Validation Loss: 0.3630804759757473\n",
      "Epoch 592/3000, Training Loss: 0.26529502040442793, Validation Loss: 0.36274376207565606\n",
      "Epoch 593/3000, Training Loss: 0.26492338829363427, Validation Loss: 0.36241435443259534\n",
      "Epoch 594/3000, Training Loss: 0.264553212476647, Validation Loss: 0.36208657190961074\n",
      "Epoch 595/3000, Training Loss: 0.2641840670272619, Validation Loss: 0.36175349503698595\n",
      "Epoch 596/3000, Training Loss: 0.2638160770384755, Validation Loss: 0.3614271054204114\n",
      "Epoch 597/3000, Training Loss: 0.26344919132766054, Validation Loss: 0.36110309065716384\n",
      "Epoch 598/3000, Training Loss: 0.2630837404739987, Validation Loss: 0.36077383336038243\n",
      "Epoch 599/3000, Training Loss: 0.26271923503981537, Validation Loss: 0.36045017377256605\n",
      "Epoch 600/3000, Training Loss: 0.26235592457993034, Validation Loss: 0.36012586947174113\n",
      "Epoch 601/3000, Training Loss: 0.26199408650199196, Validation Loss: 0.35980110576546703\n",
      "Epoch 602/3000, Training Loss: 0.2616330667210629, Validation Loss: 0.359480320332453\n",
      "Epoch 603/3000, Training Loss: 0.26127307730236327, Validation Loss: 0.35916054293762584\n",
      "Epoch 604/3000, Training Loss: 0.2609144420218971, Validation Loss: 0.3588374521336601\n",
      "Epoch 605/3000, Training Loss: 0.26055665734377764, Validation Loss: 0.35851751814608124\n",
      "Epoch 606/3000, Training Loss: 0.2602001679391443, Validation Loss: 0.3582018032501913\n",
      "Epoch 607/3000, Training Loss: 0.2598445003182339, Validation Loss: 0.3578795046689839\n",
      "Epoch 608/3000, Training Loss: 0.25949018486501046, Validation Loss: 0.35756681752820296\n",
      "Epoch 609/3000, Training Loss: 0.259136747708821, Validation Loss: 0.35725223083232294\n",
      "Epoch 610/3000, Training Loss: 0.2587843859280898, Validation Loss: 0.3569354115616542\n",
      "Epoch 611/3000, Training Loss: 0.25843318353030925, Validation Loss: 0.3566219876250937\n",
      "Epoch 612/3000, Training Loss: 0.25808299345946617, Validation Loss: 0.35631345005010995\n",
      "Epoch 613/3000, Training Loss: 0.2577336665697226, Validation Loss: 0.35599971127338387\n",
      "Epoch 614/3000, Training Loss: 0.2573858028447695, Validation Loss: 0.3556902584523707\n",
      "Epoch 615/3000, Training Loss: 0.25703881884917207, Validation Loss: 0.35538481857884435\n",
      "Epoch 616/3000, Training Loss: 0.25669299438367393, Validation Loss: 0.35507105876027023\n",
      "Epoch 617/3000, Training Loss: 0.2563476373820965, Validation Loss: 0.3547642933945688\n",
      "Epoch 618/3000, Training Loss: 0.25600341276093924, Validation Loss: 0.3544593051003611\n",
      "Epoch 619/3000, Training Loss: 0.2556601301029843, Validation Loss: 0.3541512543699628\n",
      "Epoch 620/3000, Training Loss: 0.2553177132421746, Validation Loss: 0.35384632800946975\n",
      "Epoch 621/3000, Training Loss: 0.25497640431961915, Validation Loss: 0.3535447409944302\n",
      "Epoch 622/3000, Training Loss: 0.25463587734438825, Validation Loss: 0.35323672258506233\n",
      "Epoch 623/3000, Training Loss: 0.2542963289145868, Validation Loss: 0.3529380194250326\n",
      "Epoch 624/3000, Training Loss: 0.2539576169530127, Validation Loss: 0.35263742156593836\n",
      "Epoch 625/3000, Training Loss: 0.2536198264620987, Validation Loss: 0.35233480909564335\n",
      "Epoch 626/3000, Training Loss: 0.25328295470237305, Validation Loss: 0.35203660895920474\n",
      "Epoch 627/3000, Training Loss: 0.25294726331066825, Validation Loss: 0.3517406270254616\n",
      "Epoch 628/3000, Training Loss: 0.25261210916917315, Validation Loss: 0.3514411023612464\n",
      "Epoch 629/3000, Training Loss: 0.25227836123271474, Validation Loss: 0.3511463083464434\n",
      "Epoch 630/3000, Training Loss: 0.2519455853985931, Validation Loss: 0.35085379874425693\n",
      "Epoch 631/3000, Training Loss: 0.2516138100145844, Validation Loss: 0.35055606042577064\n",
      "Epoch 632/3000, Training Loss: 0.2512829807254684, Validation Loss: 0.3502642473336229\n",
      "Epoch 633/3000, Training Loss: 0.2509531077439496, Validation Loss: 0.34997400825623887\n",
      "Epoch 634/3000, Training Loss: 0.25062407993486474, Validation Loss: 0.34967905218873324\n",
      "Epoch 635/3000, Training Loss: 0.25029594633477864, Validation Loss: 0.34939101421020285\n",
      "Epoch 636/3000, Training Loss: 0.2499688249202992, Validation Loss: 0.34910249372779417\n",
      "Epoch 637/3000, Training Loss: 0.2496424248348789, Validation Loss: 0.3488087774925859\n",
      "Epoch 638/3000, Training Loss: 0.24931709574824415, Validation Loss: 0.34852576568821625\n",
      "Epoch 639/3000, Training Loss: 0.2489925793764898, Validation Loss: 0.34823789424555396\n",
      "Epoch 640/3000, Training Loss: 0.24866882288070433, Validation Loss: 0.34795421815551986\n",
      "Epoch 641/3000, Training Loss: 0.24834621443078483, Validation Loss: 0.3476645565536873\n",
      "Epoch 642/3000, Training Loss: 0.24802440151322694, Validation Loss: 0.3473819014376141\n",
      "Epoch 643/3000, Training Loss: 0.24770323113603016, Validation Loss: 0.3470991668234619\n",
      "Epoch 644/3000, Training Loss: 0.24738341477148337, Validation Loss: 0.3468142409082521\n",
      "Epoch 645/3000, Training Loss: 0.24706407821998427, Validation Loss: 0.3465331924754928\n",
      "Epoch 646/3000, Training Loss: 0.24674568117763326, Validation Loss: 0.3462547395733101\n",
      "Epoch 647/3000, Training Loss: 0.24642852316869088, Validation Loss: 0.3459709256490037\n",
      "Epoch 648/3000, Training Loss: 0.24611186377061184, Validation Loss: 0.3456922411793619\n",
      "Epoch 649/3000, Training Loss: 0.2457960801933065, Validation Loss: 0.34541686609937805\n",
      "Epoch 650/3000, Training Loss: 0.24548181840658057, Validation Loss: 0.3451356923663139\n",
      "Epoch 651/3000, Training Loss: 0.24516787693065298, Validation Loss: 0.3448584325834456\n",
      "Epoch 652/3000, Training Loss: 0.2448550356026387, Validation Loss: 0.34458721160798633\n",
      "Epoch 653/3000, Training Loss: 0.24454317782319443, Validation Loss: 0.34430728673268185\n",
      "Epoch 654/3000, Training Loss: 0.2442318351172022, Validation Loss: 0.34403369666237094\n",
      "Epoch 655/3000, Training Loss: 0.24392158760840715, Validation Loss: 0.343762980414863\n",
      "Epoch 656/3000, Training Loss: 0.24361216458421295, Validation Loss: 0.3434857204110956\n",
      "Epoch 657/3000, Training Loss: 0.24330350085406116, Validation Loss: 0.3432169565105845\n",
      "Epoch 658/3000, Training Loss: 0.24299574313711678, Validation Loss: 0.3429480168061274\n",
      "Epoch 659/3000, Training Loss: 0.2426888736412579, Validation Loss: 0.34267216838666154\n",
      "Epoch 660/3000, Training Loss: 0.24238262074131584, Validation Loss: 0.3424054277751903\n",
      "Epoch 661/3000, Training Loss: 0.24207693333826927, Validation Loss: 0.3421380591408736\n",
      "Epoch 662/3000, Training Loss: 0.24177216689803574, Validation Loss: 0.3418653309258925\n",
      "Epoch 663/3000, Training Loss: 0.24146803006514847, Validation Loss: 0.3415997382589169\n",
      "Epoch 664/3000, Training Loss: 0.2411648703472211, Validation Loss: 0.3413357335742204\n",
      "Epoch 665/3000, Training Loss: 0.240862316900252, Validation Loss: 0.3410644619962355\n",
      "Epoch 666/3000, Training Loss: 0.24056038218934506, Validation Loss: 0.34080558560187413\n",
      "Epoch 667/3000, Training Loss: 0.2402595307707159, Validation Loss: 0.34054606809354293\n",
      "Epoch 668/3000, Training Loss: 0.2399588977076962, Validation Loss: 0.3402825272096902\n",
      "Epoch 669/3000, Training Loss: 0.23965937503689774, Validation Loss: 0.34002463677552397\n",
      "Epoch 670/3000, Training Loss: 0.23936058301353164, Validation Loss: 0.3397676907659227\n",
      "Epoch 671/3000, Training Loss: 0.2390626148751556, Validation Loss: 0.3395063077020253\n",
      "Epoch 672/3000, Training Loss: 0.23876575608338999, Validation Loss: 0.33925193762709815\n",
      "Epoch 673/3000, Training Loss: 0.23846950291760294, Validation Loss: 0.3389984672669857\n",
      "Epoch 674/3000, Training Loss: 0.23817401893481965, Validation Loss: 0.3387361010215894\n",
      "Epoch 675/3000, Training Loss: 0.2378792833970328, Validation Loss: 0.3384838763259218\n",
      "Epoch 676/3000, Training Loss: 0.2375849364467349, Validation Loss: 0.33823026917811094\n",
      "Epoch 677/3000, Training Loss: 0.23729140437597052, Validation Loss: 0.3379726946562083\n",
      "Epoch 678/3000, Training Loss: 0.23699895734739285, Validation Loss: 0.3377215114920528\n",
      "Epoch 679/3000, Training Loss: 0.23670708577244987, Validation Loss: 0.33746948144786715\n",
      "Epoch 680/3000, Training Loss: 0.23641584090850362, Validation Loss: 0.33721330584766424\n",
      "Epoch 681/3000, Training Loss: 0.23612556844717925, Validation Loss: 0.33696491629664854\n",
      "Epoch 682/3000, Training Loss: 0.23583542572652158, Validation Loss: 0.3367138989509555\n",
      "Epoch 683/3000, Training Loss: 0.2355462278580006, Validation Loss: 0.33646123208396467\n",
      "Epoch 684/3000, Training Loss: 0.23525817858986992, Validation Loss: 0.3362115245303434\n",
      "Epoch 685/3000, Training Loss: 0.2349703184447486, Validation Loss: 0.33596621439068175\n",
      "Epoch 686/3000, Training Loss: 0.23468355070423616, Validation Loss: 0.3357172436500607\n",
      "Epoch 687/3000, Training Loss: 0.23439754479927993, Validation Loss: 0.3354676331184828\n",
      "Epoch 688/3000, Training Loss: 0.2341120699383782, Validation Loss: 0.33522234090880326\n",
      "Epoch 689/3000, Training Loss: 0.2338273160390456, Validation Loss: 0.3349758702307057\n",
      "Epoch 690/3000, Training Loss: 0.23354351090106076, Validation Loss: 0.33472779588393337\n",
      "Epoch 691/3000, Training Loss: 0.2332600699331508, Validation Loss: 0.3344851215659441\n",
      "Epoch 692/3000, Training Loss: 0.23297759563575832, Validation Loss: 0.3342405706169116\n",
      "Epoch 693/3000, Training Loss: 0.23269576679110568, Validation Loss: 0.33399260606157644\n",
      "Epoch 694/3000, Training Loss: 0.2324147197124109, Validation Loss: 0.33375243483833444\n",
      "Epoch 695/3000, Training Loss: 0.2321344014197887, Validation Loss: 0.333511434284495\n",
      "Epoch 696/3000, Training Loss: 0.2318550181725445, Validation Loss: 0.3332659807855184\n",
      "Epoch 697/3000, Training Loss: 0.23157606342455608, Validation Loss: 0.333026433365893\n",
      "Epoch 698/3000, Training Loss: 0.23129812561772634, Validation Loss: 0.3327871273059278\n",
      "Epoch 699/3000, Training Loss: 0.2310203321449885, Validation Loss: 0.3325423369828969\n",
      "Epoch 700/3000, Training Loss: 0.23074328371324143, Validation Loss: 0.33230735363717845\n",
      "Epoch 701/3000, Training Loss: 0.23046706046426627, Validation Loss: 0.3320676196864785\n",
      "Epoch 702/3000, Training Loss: 0.23019132810879384, Validation Loss: 0.33182812447060644\n",
      "Epoch 703/3000, Training Loss: 0.22991650285869975, Validation Loss: 0.3315926996674412\n",
      "Epoch 704/3000, Training Loss: 0.22964224646383585, Validation Loss: 0.3313577648846996\n",
      "Epoch 705/3000, Training Loss: 0.2293687879557619, Validation Loss: 0.3311150093813115\n",
      "Epoch 706/3000, Training Loss: 0.22909563678912181, Validation Loss: 0.33088181810502026\n",
      "Epoch 707/3000, Training Loss: 0.22882367166738532, Validation Loss: 0.33064949220015466\n",
      "Epoch 708/3000, Training Loss: 0.22855188731747864, Validation Loss: 0.3304070326306126\n",
      "Epoch 709/3000, Training Loss: 0.228281144711065, Validation Loss: 0.3301785371299622\n",
      "Epoch 710/3000, Training Loss: 0.2280109791635732, Validation Loss: 0.32994398307711303\n",
      "Epoch 711/3000, Training Loss: 0.22774143008441822, Validation Loss: 0.3297076326482654\n",
      "Epoch 712/3000, Training Loss: 0.2274729977461921, Validation Loss: 0.3294773674449807\n",
      "Epoch 713/3000, Training Loss: 0.2272050231350941, Validation Loss: 0.32924862351060696\n",
      "Epoch 714/3000, Training Loss: 0.22693767950955662, Validation Loss: 0.3290130836525893\n",
      "Epoch 715/3000, Training Loss: 0.22667133473915388, Validation Loss: 0.3287833480321486\n",
      "Epoch 716/3000, Training Loss: 0.22640549008668856, Validation Loss: 0.32855294944395985\n",
      "Epoch 717/3000, Training Loss: 0.22614016898119357, Validation Loss: 0.3283252322781139\n",
      "Epoch 718/3000, Training Loss: 0.22587572867450217, Validation Loss: 0.32809022040405444\n",
      "Epoch 719/3000, Training Loss: 0.22561201199115655, Validation Loss: 0.32786347114955894\n",
      "Epoch 720/3000, Training Loss: 0.22534858327057442, Validation Loss: 0.327637211330766\n",
      "Epoch 721/3000, Training Loss: 0.22508595030214992, Validation Loss: 0.3274071758686034\n",
      "Epoch 722/3000, Training Loss: 0.2248243071344438, Validation Loss: 0.32717965963082773\n",
      "Epoch 723/3000, Training Loss: 0.22456284757210085, Validation Loss: 0.3269541470071863\n",
      "Epoch 724/3000, Training Loss: 0.22430237142505918, Validation Loss: 0.32672642586325745\n",
      "Epoch 725/3000, Training Loss: 0.22404229720195135, Validation Loss: 0.32650294659648993\n",
      "Epoch 726/3000, Training Loss: 0.22378279016820674, Validation Loss: 0.3262786750443718\n",
      "Epoch 727/3000, Training Loss: 0.22352431893640515, Validation Loss: 0.3260506468832797\n",
      "Epoch 728/3000, Training Loss: 0.22326640841249343, Validation Loss: 0.325828526318182\n",
      "Epoch 729/3000, Training Loss: 0.2230094989088068, Validation Loss: 0.32560704300265203\n",
      "Epoch 730/3000, Training Loss: 0.22275303053985643, Validation Loss: 0.32538129107142666\n",
      "Epoch 731/3000, Training Loss: 0.2224969995307506, Validation Loss: 0.3251640931607101\n",
      "Epoch 732/3000, Training Loss: 0.22224114539831957, Validation Loss: 0.32494453574642573\n",
      "Epoch 733/3000, Training Loss: 0.22198623940866868, Validation Loss: 0.3247208166575046\n",
      "Epoch 734/3000, Training Loss: 0.22173189034342755, Validation Loss: 0.32450180599343537\n",
      "Epoch 735/3000, Training Loss: 0.22147772241775449, Validation Loss: 0.3242855522979002\n",
      "Epoch 736/3000, Training Loss: 0.22122459237721195, Validation Loss: 0.3240635227431868\n",
      "Epoch 737/3000, Training Loss: 0.22097174956861343, Validation Loss: 0.32384338653368633\n",
      "Epoch 738/3000, Training Loss: 0.22071962037143403, Validation Loss: 0.3236302374705988\n",
      "Epoch 739/3000, Training Loss: 0.22046809113584656, Validation Loss: 0.32340996485113105\n",
      "Epoch 740/3000, Training Loss: 0.2202169738175485, Validation Loss: 0.32319380797486735\n",
      "Epoch 741/3000, Training Loss: 0.21996671732655793, Validation Loss: 0.3229795701672784\n",
      "Epoch 742/3000, Training Loss: 0.21971699510526863, Validation Loss: 0.3227651857981429\n",
      "Epoch 743/3000, Training Loss: 0.21946764374160957, Validation Loss: 0.32254693213444774\n",
      "Epoch 744/3000, Training Loss: 0.21921921053013993, Validation Loss: 0.3223332280183322\n",
      "Epoch 745/3000, Training Loss: 0.218971019084156, Validation Loss: 0.32212031795293666\n",
      "Epoch 746/3000, Training Loss: 0.21872361776058033, Validation Loss: 0.32190562473645895\n",
      "Epoch 747/3000, Training Loss: 0.2184767630261381, Validation Loss: 0.3216938001503074\n",
      "Epoch 748/3000, Training Loss: 0.21823021052331312, Validation Loss: 0.32148334283654434\n",
      "Epoch 749/3000, Training Loss: 0.2179839222387156, Validation Loss: 0.321267555536541\n",
      "Epoch 750/3000, Training Loss: 0.217738339598236, Validation Loss: 0.32105932651999625\n",
      "Epoch 751/3000, Training Loss: 0.2174930527318263, Validation Loss: 0.32085146636016154\n",
      "Epoch 752/3000, Training Loss: 0.2172486693207464, Validation Loss: 0.32063649938620714\n",
      "Epoch 753/3000, Training Loss: 0.21700468118860541, Validation Loss: 0.3204266564768772\n",
      "Epoch 754/3000, Training Loss: 0.21676110844404475, Validation Loss: 0.3202214317938914\n",
      "Epoch 755/3000, Training Loss: 0.21651837396481718, Validation Loss: 0.3200082618919703\n",
      "Epoch 756/3000, Training Loss: 0.21627614715955745, Validation Loss: 0.31980378296133516\n",
      "Epoch 757/3000, Training Loss: 0.21603450696651935, Validation Loss: 0.3195955020061849\n",
      "Epoch 758/3000, Training Loss: 0.2157933482041293, Validation Loss: 0.3193907415657851\n",
      "Epoch 759/3000, Training Loss: 0.21555278737813294, Validation Loss: 0.3191801337075088\n",
      "Epoch 760/3000, Training Loss: 0.21531285230409475, Validation Loss: 0.3189753039540392\n",
      "Epoch 761/3000, Training Loss: 0.21507364086730116, Validation Loss: 0.31877266034940327\n",
      "Epoch 762/3000, Training Loss: 0.2148348313506148, Validation Loss: 0.31856387364351524\n",
      "Epoch 763/3000, Training Loss: 0.2145969675556127, Validation Loss: 0.31836008522822\n",
      "Epoch 764/3000, Training Loss: 0.21435922605254282, Validation Loss: 0.31815897329120846\n",
      "Epoch 765/3000, Training Loss: 0.21412230549858038, Validation Loss: 0.3179533501873101\n",
      "Epoch 766/3000, Training Loss: 0.21388599546639664, Validation Loss: 0.3177512726000608\n",
      "Epoch 767/3000, Training Loss: 0.21364999164396567, Validation Loss: 0.3175527740945365\n",
      "Epoch 768/3000, Training Loss: 0.21341472503100478, Validation Loss: 0.31734578736323177\n",
      "Epoch 769/3000, Training Loss: 0.2131804023988794, Validation Loss: 0.3171493656936396\n",
      "Epoch 770/3000, Training Loss: 0.21294631612285908, Validation Loss: 0.31694836580240765\n",
      "Epoch 771/3000, Training Loss: 0.21271277194036778, Validation Loss: 0.31675020095905004\n",
      "Epoch 772/3000, Training Loss: 0.21247999628545264, Validation Loss: 0.3165472392714844\n",
      "Epoch 773/3000, Training Loss: 0.21224756836690797, Validation Loss: 0.31635111959630646\n",
      "Epoch 774/3000, Training Loss: 0.2120158888571073, Validation Loss: 0.31615368076260325\n",
      "Epoch 775/3000, Training Loss: 0.21178477244379576, Validation Loss: 0.315952320811173\n",
      "Epoch 776/3000, Training Loss: 0.21155444575388185, Validation Loss: 0.31575439319248866\n",
      "Epoch 777/3000, Training Loss: 0.2113243347468078, Validation Loss: 0.31556022220623703\n",
      "Epoch 778/3000, Training Loss: 0.21109500186058783, Validation Loss: 0.31536123671538163\n",
      "Epoch 779/3000, Training Loss: 0.21086622006530747, Validation Loss: 0.3151653425927956\n",
      "Epoch 780/3000, Training Loss: 0.21063796240596558, Validation Loss: 0.31497338262412844\n",
      "Epoch 781/3000, Training Loss: 0.2104103535597053, Validation Loss: 0.31477942326752684\n",
      "Epoch 782/3000, Training Loss: 0.21018362352880357, Validation Loss: 0.31457942695119534\n",
      "Epoch 783/3000, Training Loss: 0.2099573462200097, Validation Loss: 0.31438540797285297\n",
      "Epoch 784/3000, Training Loss: 0.2097315904567829, Validation Loss: 0.31419094706007167\n",
      "Epoch 785/3000, Training Loss: 0.20950669259527768, Validation Loss: 0.31399894779716403\n",
      "Epoch 786/3000, Training Loss: 0.20928137309383182, Validation Loss: 0.3138013198420149\n",
      "Epoch 787/3000, Training Loss: 0.20905693817658783, Validation Loss: 0.31361158852602944\n",
      "Epoch 788/3000, Training Loss: 0.20883272179965195, Validation Loss: 0.3134215676921374\n",
      "Epoch 789/3000, Training Loss: 0.2086088752068532, Validation Loss: 0.3132243149146774\n",
      "Epoch 790/3000, Training Loss: 0.2083859507361851, Validation Loss: 0.31303753116373056\n",
      "Epoch 791/3000, Training Loss: 0.20816301610771232, Validation Loss: 0.31284452748122166\n",
      "Epoch 792/3000, Training Loss: 0.20794053363527365, Validation Loss: 0.3126582430156234\n",
      "Epoch 793/3000, Training Loss: 0.20771898262465344, Validation Loss: 0.31246292856244556\n",
      "Epoch 794/3000, Training Loss: 0.20749737254877504, Validation Loss: 0.31227579023715735\n",
      "Epoch 795/3000, Training Loss: 0.2072768923916397, Validation Loss: 0.31208681454972975\n",
      "Epoch 796/3000, Training Loss: 0.20705638463584483, Validation Loss: 0.3119025329685109\n",
      "Epoch 797/3000, Training Loss: 0.20683676839640183, Validation Loss: 0.3117095680976599\n",
      "Epoch 798/3000, Training Loss: 0.20661731306034048, Validation Loss: 0.31152178202003533\n",
      "Epoch 799/3000, Training Loss: 0.20639839266162385, Validation Loss: 0.3113351053140266\n",
      "Epoch 800/3000, Training Loss: 0.20618013519848805, Validation Loss: 0.31114544801914584\n",
      "Epoch 801/3000, Training Loss: 0.2059619833380608, Validation Loss: 0.3109600001483342\n",
      "Epoch 802/3000, Training Loss: 0.2057447249866364, Validation Loss: 0.3107795035394579\n",
      "Epoch 803/3000, Training Loss: 0.20552752587426584, Validation Loss: 0.31059326095965367\n",
      "Epoch 804/3000, Training Loss: 0.2053110839477197, Validation Loss: 0.3104045592632516\n",
      "Epoch 805/3000, Training Loss: 0.2050950756786529, Validation Loss: 0.3102224187110555\n",
      "Epoch 806/3000, Training Loss: 0.2048793858199544, Validation Loss: 0.31003705848323776\n",
      "Epoch 807/3000, Training Loss: 0.20466419434083066, Validation Loss: 0.30985487130910894\n",
      "Epoch 808/3000, Training Loss: 0.20444976371728285, Validation Loss: 0.3096693899073569\n",
      "Epoch 809/3000, Training Loss: 0.2042352510975748, Validation Loss: 0.3094887333923098\n",
      "Epoch 810/3000, Training Loss: 0.20402152648733823, Validation Loss: 0.30930602405251323\n",
      "Epoch 811/3000, Training Loss: 0.20380782550129523, Validation Loss: 0.3091226112308626\n",
      "Epoch 812/3000, Training Loss: 0.20359522982392725, Validation Loss: 0.3089401436436193\n",
      "Epoch 813/3000, Training Loss: 0.20338261932612497, Validation Loss: 0.3087635000078537\n",
      "Epoch 814/3000, Training Loss: 0.20317050657627858, Validation Loss: 0.30858262545891885\n",
      "Epoch 815/3000, Training Loss: 0.2029588676818083, Validation Loss: 0.30839671047237216\n",
      "Epoch 816/3000, Training Loss: 0.20274767950525377, Validation Loss: 0.3082192557203127\n",
      "Epoch 817/3000, Training Loss: 0.20253726114980594, Validation Loss: 0.30804342432604337\n",
      "Epoch 818/3000, Training Loss: 0.20232733508580755, Validation Loss: 0.3078598557406926\n",
      "Epoch 819/3000, Training Loss: 0.20211761805377906, Validation Loss: 0.3076862176341695\n",
      "Epoch 820/3000, Training Loss: 0.20190853011643395, Validation Loss: 0.3075074881168234\n",
      "Epoch 821/3000, Training Loss: 0.20169977958864743, Validation Loss: 0.30733418423273284\n",
      "Epoch 822/3000, Training Loss: 0.20149142475041099, Validation Loss: 0.3071547957562571\n",
      "Epoch 823/3000, Training Loss: 0.20128356584272386, Validation Loss: 0.30698144389004306\n",
      "Epoch 824/3000, Training Loss: 0.20107569878012857, Validation Loss: 0.30680605785386106\n",
      "Epoch 825/3000, Training Loss: 0.20086855200843434, Validation Loss: 0.306633881891245\n",
      "Epoch 826/3000, Training Loss: 0.20066195228761227, Validation Loss: 0.3064559249860206\n",
      "Epoch 827/3000, Training Loss: 0.2004556384270112, Validation Loss: 0.30628360324528336\n",
      "Epoch 828/3000, Training Loss: 0.20024929962460933, Validation Loss: 0.3061097507299058\n",
      "Epoch 829/3000, Training Loss: 0.20004405906399217, Validation Loss: 0.3059337774161762\n",
      "Epoch 830/3000, Training Loss: 0.19983894211328043, Validation Loss: 0.30575998493549017\n",
      "Epoch 831/3000, Training Loss: 0.19963424345534814, Validation Loss: 0.30559259268226546\n",
      "Epoch 832/3000, Training Loss: 0.19943015490966287, Validation Loss: 0.30542118223094855\n",
      "Epoch 833/3000, Training Loss: 0.19922643725983585, Validation Loss: 0.3052461064564868\n",
      "Epoch 834/3000, Training Loss: 0.19902280817766974, Validation Loss: 0.30507453527861683\n",
      "Epoch 835/3000, Training Loss: 0.1988199966357864, Validation Loss: 0.3049064666229325\n",
      "Epoch 836/3000, Training Loss: 0.19861734990573493, Validation Loss: 0.3047358574519016\n",
      "Epoch 837/3000, Training Loss: 0.19841527305304463, Validation Loss: 0.304563318832523\n",
      "Epoch 838/3000, Training Loss: 0.1982134121367144, Validation Loss: 0.30439497079663436\n",
      "Epoch 839/3000, Training Loss: 0.19801191120837833, Validation Loss: 0.304226959707943\n",
      "Epoch 840/3000, Training Loss: 0.19781073611889521, Validation Loss: 0.30405198830244357\n",
      "Epoch 841/3000, Training Loss: 0.19761018135546954, Validation Loss: 0.3038864729448181\n",
      "Epoch 842/3000, Training Loss: 0.1974099953383804, Validation Loss: 0.3037196865957808\n",
      "Epoch 843/3000, Training Loss: 0.19720998519772903, Validation Loss: 0.30355291534841605\n",
      "Epoch 844/3000, Training Loss: 0.1970106876983888, Validation Loss: 0.30338040952274303\n",
      "Epoch 845/3000, Training Loss: 0.19681189910949803, Validation Loss: 0.3032157690023124\n",
      "Epoch 846/3000, Training Loss: 0.19661328830475602, Validation Loss: 0.3030483463209331\n",
      "Epoch 847/3000, Training Loss: 0.1964155552489863, Validation Loss: 0.3028793657728816\n",
      "Epoch 848/3000, Training Loss: 0.19621774538010348, Validation Loss: 0.30271338862729147\n",
      "Epoch 849/3000, Training Loss: 0.19602039013706501, Validation Loss: 0.3025485598924916\n",
      "Epoch 850/3000, Training Loss: 0.19582339802880666, Validation Loss: 0.3023840457320058\n",
      "Epoch 851/3000, Training Loss: 0.1956269574420396, Validation Loss: 0.30221591528103026\n",
      "Epoch 852/3000, Training Loss: 0.19543069628341833, Validation Loss: 0.3020521809475385\n",
      "Epoch 853/3000, Training Loss: 0.19523523594358957, Validation Loss: 0.30188959749525696\n",
      "Epoch 854/3000, Training Loss: 0.19503976445526672, Validation Loss: 0.30172691212532704\n",
      "Epoch 855/3000, Training Loss: 0.19484439129589065, Validation Loss: 0.30155921075572983\n",
      "Epoch 856/3000, Training Loss: 0.19465020210723297, Validation Loss: 0.3013963748448318\n",
      "Epoch 857/3000, Training Loss: 0.19445556584968915, Validation Loss: 0.3012375155129315\n",
      "Epoch 858/3000, Training Loss: 0.19426153459092318, Validation Loss: 0.3010753337203926\n",
      "Epoch 859/3000, Training Loss: 0.1940679557504619, Validation Loss: 0.30090824933500293\n",
      "Epoch 860/3000, Training Loss: 0.19387493264177874, Validation Loss: 0.300749954419622\n",
      "Epoch 861/3000, Training Loss: 0.1936816076157296, Validation Loss: 0.30059088457252703\n",
      "Epoch 862/3000, Training Loss: 0.1934894463593802, Validation Loss: 0.3004251472023224\n",
      "Epoch 863/3000, Training Loss: 0.19329736604197, Validation Loss: 0.3002636113967266\n",
      "Epoch 864/3000, Training Loss: 0.1931057697673106, Validation Loss: 0.3001055013104487\n",
      "Epoch 865/3000, Training Loss: 0.19291419927269207, Validation Loss: 0.2999488286760957\n",
      "Epoch 866/3000, Training Loss: 0.19272328554007168, Validation Loss: 0.29978546988672256\n",
      "Epoch 867/3000, Training Loss: 0.19253296167584252, Validation Loss: 0.29962454283797624\n",
      "Epoch 868/3000, Training Loss: 0.192342952131964, Validation Loss: 0.2994687266221517\n",
      "Epoch 869/3000, Training Loss: 0.1921529966453925, Validation Loss: 0.2993103936571523\n",
      "Epoch 870/3000, Training Loss: 0.1919637246608716, Validation Loss: 0.29914790849381445\n",
      "Epoch 871/3000, Training Loss: 0.19177514116474548, Validation Loss: 0.298990621171237\n",
      "Epoch 872/3000, Training Loss: 0.1915862284095419, Validation Loss: 0.2988318184276413\n",
      "Epoch 873/3000, Training Loss: 0.19139798230625904, Validation Loss: 0.2986714467984754\n",
      "Epoch 874/3000, Training Loss: 0.19121008438569298, Validation Loss: 0.29851363778028916\n",
      "Epoch 875/3000, Training Loss: 0.19102267370919374, Validation Loss: 0.2983583184683671\n",
      "Epoch 876/3000, Training Loss: 0.19083507151871063, Validation Loss: 0.2982043168841653\n",
      "Epoch 877/3000, Training Loss: 0.1906485760246439, Validation Loss: 0.29803894203893877\n",
      "Epoch 878/3000, Training Loss: 0.19046215639412178, Validation Loss: 0.29788712911364335\n",
      "Epoch 879/3000, Training Loss: 0.19027571840061877, Validation Loss: 0.2977331039106316\n",
      "Epoch 880/3000, Training Loss: 0.19009016834869866, Validation Loss: 0.2975781945279342\n",
      "Epoch 881/3000, Training Loss: 0.18990467856866455, Validation Loss: 0.2974189330739651\n",
      "Epoch 882/3000, Training Loss: 0.18971956621051583, Validation Loss: 0.29726551196710027\n",
      "Epoch 883/3000, Training Loss: 0.18953485220579255, Validation Loss: 0.2971118492606224\n",
      "Epoch 884/3000, Training Loss: 0.18935044647618773, Validation Loss: 0.29696010263899525\n",
      "Epoch 885/3000, Training Loss: 0.18916630482361027, Validation Loss: 0.29679990931713307\n",
      "Epoch 886/3000, Training Loss: 0.1889830903959464, Validation Loss: 0.29665127032923394\n",
      "Epoch 887/3000, Training Loss: 0.18879908327097372, Validation Loss: 0.29649921870392854\n",
      "Epoch 888/3000, Training Loss: 0.1886166856774856, Validation Loss: 0.29634125789996635\n",
      "Epoch 889/3000, Training Loss: 0.1884339371235955, Validation Loss: 0.2961922767713963\n",
      "Epoch 890/3000, Training Loss: 0.1882513754996457, Validation Loss: 0.2960436788516678\n",
      "Epoch 891/3000, Training Loss: 0.1880692979977454, Validation Loss: 0.29589012705965867\n",
      "Epoch 892/3000, Training Loss: 0.1878870680514493, Validation Loss: 0.2957371441597384\n",
      "Epoch 893/3000, Training Loss: 0.18770510483533187, Validation Loss: 0.2955889094039974\n",
      "Epoch 894/3000, Training Loss: 0.18752407461007675, Validation Loss: 0.2954364051998528\n",
      "Epoch 895/3000, Training Loss: 0.18734278743891897, Validation Loss: 0.2952930563150017\n",
      "Epoch 896/3000, Training Loss: 0.1871619435819659, Validation Loss: 0.29513563818050664\n",
      "Epoch 897/3000, Training Loss: 0.1869817524328836, Validation Loss: 0.294990220718139\n",
      "Epoch 898/3000, Training Loss: 0.18680126845608738, Validation Loss: 0.2948429900372726\n",
      "Epoch 899/3000, Training Loss: 0.18662176210583595, Validation Loss: 0.29468876289692925\n",
      "Epoch 900/3000, Training Loss: 0.18644267007667925, Validation Loss: 0.29454515543422416\n",
      "Epoch 901/3000, Training Loss: 0.1862635603859359, Validation Loss: 0.29440071138043056\n",
      "Epoch 902/3000, Training Loss: 0.18608484162982378, Validation Loss: 0.2942515270196276\n",
      "Epoch 903/3000, Training Loss: 0.18590691915192853, Validation Loss: 0.294104964429813\n",
      "Epoch 904/3000, Training Loss: 0.18572868611510307, Validation Loss: 0.2939573328944737\n",
      "Epoch 905/3000, Training Loss: 0.18555122250023776, Validation Loss: 0.2938154378688041\n",
      "Epoch 906/3000, Training Loss: 0.18537392836265992, Validation Loss: 0.29366882424707275\n",
      "Epoch 907/3000, Training Loss: 0.18519720523263386, Validation Loss: 0.29351868785157337\n",
      "Epoch 908/3000, Training Loss: 0.18502065886140837, Validation Loss: 0.2933783133442977\n",
      "Epoch 909/3000, Training Loss: 0.18484420871759769, Validation Loss: 0.2932309775458864\n",
      "Epoch 910/3000, Training Loss: 0.1846679973346726, Validation Loss: 0.29308502472918024\n",
      "Epoch 911/3000, Training Loss: 0.18449256424407362, Validation Loss: 0.2929440694196454\n",
      "Epoch 912/3000, Training Loss: 0.18431684091150027, Validation Loss: 0.292799530104094\n",
      "Epoch 913/3000, Training Loss: 0.18414123386196551, Validation Loss: 0.2926594620128574\n",
      "Epoch 914/3000, Training Loss: 0.18396654599618623, Validation Loss: 0.29251034285035143\n",
      "Epoch 915/3000, Training Loss: 0.18379175357859617, Validation Loss: 0.29237019654835505\n",
      "Epoch 916/3000, Training Loss: 0.18361711892577395, Validation Loss: 0.292230399994812\n",
      "Epoch 917/3000, Training Loss: 0.1834432037610334, Validation Loss: 0.2920888080272534\n",
      "Epoch 918/3000, Training Loss: 0.18326923129060269, Validation Loss: 0.291946070367083\n",
      "Epoch 919/3000, Training Loss: 0.18309577686093403, Validation Loss: 0.29180295695963904\n",
      "Epoch 920/3000, Training Loss: 0.18292267008424926, Validation Loss: 0.2916644435494271\n",
      "Epoch 921/3000, Training Loss: 0.18274950122238492, Validation Loss: 0.29152149659321663\n",
      "Epoch 922/3000, Training Loss: 0.18257719250300858, Validation Loss: 0.2913791618724924\n",
      "Epoch 923/3000, Training Loss: 0.18240480888604432, Validation Loss: 0.2912436252811708\n",
      "Epoch 924/3000, Training Loss: 0.1822326787974004, Validation Loss: 0.2911025458541543\n",
      "Epoch 925/3000, Training Loss: 0.18206132160179894, Validation Loss: 0.29095820932913313\n",
      "Epoch 926/3000, Training Loss: 0.18188990290176252, Validation Loss: 0.29082256469427964\n",
      "Epoch 927/3000, Training Loss: 0.18171883061642066, Validation Loss: 0.2906827666266629\n",
      "Epoch 928/3000, Training Loss: 0.1815482581019729, Validation Loss: 0.2905463254271397\n",
      "Epoch 929/3000, Training Loss: 0.18137791724434144, Validation Loss: 0.2904019922411892\n",
      "Epoch 930/3000, Training Loss: 0.1812082344545484, Validation Loss: 0.29026662901369393\n",
      "Epoch 931/3000, Training Loss: 0.1810384125707889, Validation Loss: 0.2901318660359221\n",
      "Epoch 932/3000, Training Loss: 0.18086907095806196, Validation Loss: 0.2899880764607164\n",
      "Epoch 933/3000, Training Loss: 0.1807002205466471, Validation Loss: 0.2898503652604093\n",
      "Epoch 934/3000, Training Loss: 0.18053134523178965, Validation Loss: 0.2897174854784799\n",
      "Epoch 935/3000, Training Loss: 0.18036274078462736, Validation Loss: 0.28957843102228054\n",
      "Epoch 936/3000, Training Loss: 0.18019475956160474, Validation Loss: 0.2894400952685898\n",
      "Epoch 937/3000, Training Loss: 0.18002682581512097, Validation Loss: 0.2893024504081721\n",
      "Epoch 938/3000, Training Loss: 0.17985902924476152, Validation Loss: 0.289168177584869\n",
      "Epoch 939/3000, Training Loss: 0.1796918524401041, Validation Loss: 0.2890369750734805\n",
      "Epoch 940/3000, Training Loss: 0.1795247598596261, Validation Loss: 0.28889271225559526\n",
      "Epoch 941/3000, Training Loss: 0.17935802632790174, Validation Loss: 0.28876045866855615\n",
      "Epoch 942/3000, Training Loss: 0.1791914757703074, Validation Loss: 0.2886255301629275\n",
      "Epoch 943/3000, Training Loss: 0.17902527737168164, Validation Loss: 0.28848705182817985\n",
      "Epoch 944/3000, Training Loss: 0.17885948992788542, Validation Loss: 0.28835631642023796\n",
      "Epoch 945/3000, Training Loss: 0.17869378667933805, Validation Loss: 0.2882208739938456\n",
      "Epoch 946/3000, Training Loss: 0.17852832111824232, Validation Loss: 0.2880880710411299\n",
      "Epoch 947/3000, Training Loss: 0.17836339471769283, Validation Loss: 0.28794957042258634\n",
      "Epoch 948/3000, Training Loss: 0.1781986632057205, Validation Loss: 0.2878189924451188\n",
      "Epoch 949/3000, Training Loss: 0.17803398948249535, Validation Loss: 0.28768747474128875\n",
      "Epoch 950/3000, Training Loss: 0.1778699360458559, Validation Loss: 0.287548812120496\n",
      "Epoch 951/3000, Training Loss: 0.17770585031094957, Validation Loss: 0.28741921533040043\n",
      "Epoch 952/3000, Training Loss: 0.17754242591361477, Validation Loss: 0.2872856373099259\n",
      "Epoch 953/3000, Training Loss: 0.17737911553850153, Validation Loss: 0.28715753980047654\n",
      "Epoch 954/3000, Training Loss: 0.17721573757195314, Validation Loss: 0.2870210874934996\n",
      "Epoch 955/3000, Training Loss: 0.17705330122578744, Validation Loss: 0.28689016929436384\n",
      "Epoch 956/3000, Training Loss: 0.1768905158638124, Validation Loss: 0.28676078924849785\n",
      "Epoch 957/3000, Training Loss: 0.17672848503043678, Validation Loss: 0.28662442255485165\n",
      "Epoch 958/3000, Training Loss: 0.1765667440857734, Validation Loss: 0.2864960761078916\n",
      "Epoch 959/3000, Training Loss: 0.17640473588880987, Validation Loss: 0.28636783094222895\n",
      "Epoch 960/3000, Training Loss: 0.17624374679255753, Validation Loss: 0.2862397385396373\n",
      "Epoch 961/3000, Training Loss: 0.1760825823396838, Validation Loss: 0.2861050992044574\n",
      "Epoch 962/3000, Training Loss: 0.17592205478840445, Validation Loss: 0.2859760220557341\n",
      "Epoch 963/3000, Training Loss: 0.1757616703574277, Validation Loss: 0.2858500066940961\n",
      "Epoch 964/3000, Training Loss: 0.17560114328266296, Validation Loss: 0.28571577997357744\n",
      "Epoch 965/3000, Training Loss: 0.17544186575254483, Validation Loss: 0.28558810300111065\n",
      "Epoch 966/3000, Training Loss: 0.17528187342188095, Validation Loss: 0.2854618893142514\n",
      "Epoch 967/3000, Training Loss: 0.1751228717770948, Validation Loss: 0.28533415507459253\n",
      "Epoch 968/3000, Training Loss: 0.17496395409963011, Validation Loss: 0.2852022392738946\n",
      "Epoch 969/3000, Training Loss: 0.1748048516325646, Validation Loss: 0.2850741404269069\n",
      "Epoch 970/3000, Training Loss: 0.17464700885515252, Validation Loss: 0.2849493505824499\n",
      "Epoch 971/3000, Training Loss: 0.1744884580148367, Validation Loss: 0.28482026157819923\n",
      "Epoch 972/3000, Training Loss: 0.17433063333419974, Validation Loss: 0.2846933976436504\n",
      "Epoch 973/3000, Training Loss: 0.17417313406913995, Validation Loss: 0.28456913771954057\n",
      "Epoch 974/3000, Training Loss: 0.17401557443688453, Validation Loss: 0.2844408420244902\n",
      "Epoch 975/3000, Training Loss: 0.17385894228179716, Validation Loss: 0.2843126353685393\n",
      "Epoch 976/3000, Training Loss: 0.17370183940640285, Validation Loss: 0.28419004534268383\n",
      "Epoch 977/3000, Training Loss: 0.1735451895281068, Validation Loss: 0.2840646798497317\n",
      "Epoch 978/3000, Training Loss: 0.17338934785800308, Validation Loss: 0.28393511000798755\n",
      "Epoch 979/3000, Training Loss: 0.17323284099578307, Validation Loss: 0.2838099501155063\n",
      "Epoch 980/3000, Training Loss: 0.1730774311244898, Validation Loss: 0.28368716669302496\n",
      "Epoch 981/3000, Training Loss: 0.1729217601732883, Validation Loss: 0.2835629345510993\n",
      "Epoch 982/3000, Training Loss: 0.1727664893347124, Validation Loss: 0.2834359879561727\n",
      "Epoch 983/3000, Training Loss: 0.17261185929365508, Validation Loss: 0.2833145150125532\n",
      "Epoch 984/3000, Training Loss: 0.17245699572930892, Validation Loss: 0.2831889967174218\n",
      "Epoch 985/3000, Training Loss: 0.17230250348668255, Validation Loss: 0.28306388901766616\n",
      "Epoch 986/3000, Training Loss: 0.17214834213670258, Validation Loss: 0.28293967817504656\n",
      "Epoch 987/3000, Training Loss: 0.17199426989520916, Validation Loss: 0.2828190319641592\n",
      "Epoch 988/3000, Training Loss: 0.1718409565807326, Validation Loss: 0.2826935363872795\n",
      "Epoch 989/3000, Training Loss: 0.17168727328870723, Validation Loss: 0.2825716948148084\n",
      "Epoch 990/3000, Training Loss: 0.17153418914310853, Validation Loss: 0.2824515359610277\n",
      "Epoch 991/3000, Training Loss: 0.17138121009346471, Validation Loss: 0.2823232827656417\n",
      "Epoch 992/3000, Training Loss: 0.17122857910845665, Validation Loss: 0.2822042364833863\n",
      "Epoch 993/3000, Training Loss: 0.17107614425228201, Validation Loss: 0.2820855195380673\n",
      "Epoch 994/3000, Training Loss: 0.17092398614986176, Validation Loss: 0.28196303590542177\n",
      "Epoch 995/3000, Training Loss: 0.17077213938272684, Validation Loss: 0.28184112237376413\n",
      "Epoch 996/3000, Training Loss: 0.17062049888535585, Validation Loss: 0.28171909116357147\n",
      "Epoch 997/3000, Training Loss: 0.17046907188329505, Validation Loss: 0.2816019085952887\n",
      "Epoch 998/3000, Training Loss: 0.17031875832082471, Validation Loss: 0.281476328434684\n",
      "Epoch 999/3000, Training Loss: 0.17016820731128815, Validation Loss: 0.28135677526420094\n",
      "Epoch 1000/3000, Training Loss: 0.1700179212336564, Validation Loss: 0.2812389502046629\n",
      "Epoch 1001/3000, Training Loss: 0.1698681185666922, Validation Loss: 0.28111257136286066\n",
      "Epoch 1002/3000, Training Loss: 0.16971859376434614, Validation Loss: 0.28099627575195063\n",
      "Epoch 1003/3000, Training Loss: 0.1695690091747731, Validation Loss: 0.28087782881968604\n",
      "Epoch 1004/3000, Training Loss: 0.16941997815242568, Validation Loss: 0.2807589186806113\n",
      "Epoch 1005/3000, Training Loss: 0.16927073195907189, Validation Loss: 0.2806372852120445\n",
      "Epoch 1006/3000, Training Loss: 0.16912234722612804, Validation Loss: 0.28051617287626646\n",
      "Epoch 1007/3000, Training Loss: 0.16897398280128692, Validation Loss: 0.2804023291802153\n",
      "Epoch 1008/3000, Training Loss: 0.16882552875427423, Validation Loss: 0.28028644884384196\n",
      "Epoch 1009/3000, Training Loss: 0.1686776393871345, Validation Loss: 0.28016238304423\n",
      "Epoch 1010/3000, Training Loss: 0.16853005880877214, Validation Loss: 0.28004710613153555\n",
      "Epoch 1011/3000, Training Loss: 0.16838251957210454, Validation Loss: 0.2799318976193088\n",
      "Epoch 1012/3000, Training Loss: 0.16823549710277652, Validation Loss: 0.27981576886031934\n",
      "Epoch 1013/3000, Training Loss: 0.1680886430540451, Validation Loss: 0.27969585371307076\n",
      "Epoch 1014/3000, Training Loss: 0.1679421329231738, Validation Loss: 0.27957848799133567\n",
      "Epoch 1015/3000, Training Loss: 0.16779566257747808, Validation Loss: 0.279462109704188\n",
      "Epoch 1016/3000, Training Loss: 0.16764936363196206, Validation Loss: 0.279346826662178\n",
      "Epoch 1017/3000, Training Loss: 0.16750324780200262, Validation Loss: 0.2792287551943779\n",
      "Epoch 1018/3000, Training Loss: 0.1673577780715526, Validation Loss: 0.27911263744978704\n",
      "Epoch 1019/3000, Training Loss: 0.16721233339796426, Validation Loss: 0.2789993430879673\n",
      "Epoch 1020/3000, Training Loss: 0.16706657930784768, Validation Loss: 0.27888489236174185\n",
      "Epoch 1021/3000, Training Loss: 0.1669217528595688, Validation Loss: 0.27876283730201923\n",
      "Epoch 1022/3000, Training Loss: 0.16677718288410248, Validation Loss: 0.27865072025770266\n",
      "Epoch 1023/3000, Training Loss: 0.16663312028792301, Validation Loss: 0.2785347320972653\n",
      "Epoch 1024/3000, Training Loss: 0.16648893868892664, Validation Loss: 0.27842496758834184\n",
      "Epoch 1025/3000, Training Loss: 0.16634504359467334, Validation Loss: 0.2783049863400654\n",
      "Epoch 1026/3000, Training Loss: 0.16620180639678353, Validation Loss: 0.2781918151928337\n",
      "Epoch 1027/3000, Training Loss: 0.16605845931357904, Validation Loss: 0.2780806703612901\n",
      "Epoch 1028/3000, Training Loss: 0.1659155687484818, Validation Loss: 0.27795845352230775\n",
      "Epoch 1029/3000, Training Loss: 0.16577304461741074, Validation Loss: 0.2778484599711002\n",
      "Epoch 1030/3000, Training Loss: 0.16563059208249684, Validation Loss: 0.2777336113093423\n",
      "Epoch 1031/3000, Training Loss: 0.16548868856621984, Validation Loss: 0.27762372462835133\n",
      "Epoch 1032/3000, Training Loss: 0.16534659373104457, Validation Loss: 0.2775057465144228\n",
      "Epoch 1033/3000, Training Loss: 0.165205341967045, Validation Loss: 0.2773913254512155\n",
      "Epoch 1034/3000, Training Loss: 0.1650636941215928, Validation Loss: 0.2772825523348018\n",
      "Epoch 1035/3000, Training Loss: 0.1649226847682985, Validation Loss: 0.27716152889878715\n",
      "Epoch 1036/3000, Training Loss: 0.16478191335150882, Validation Loss: 0.27705390078124786\n",
      "Epoch 1037/3000, Training Loss: 0.16464099556296394, Validation Loss: 0.2769419862923197\n",
      "Epoch 1038/3000, Training Loss: 0.16450079158349734, Validation Loss: 0.27682805981841024\n",
      "Epoch 1039/3000, Training Loss: 0.16436062011246716, Validation Loss: 0.2767140868999832\n",
      "Epoch 1040/3000, Training Loss: 0.16422064328981506, Validation Loss: 0.2766010559708766\n",
      "Epoch 1041/3000, Training Loss: 0.16408078750469468, Validation Loss: 0.2764940699616079\n",
      "Epoch 1042/3000, Training Loss: 0.16394121500358513, Validation Loss: 0.27637687088918583\n",
      "Epoch 1043/3000, Training Loss: 0.1638023104825496, Validation Loss: 0.276268628916191\n",
      "Epoch 1044/3000, Training Loss: 0.16366281976775499, Validation Loss: 0.2761599407043526\n",
      "Epoch 1045/3000, Training Loss: 0.16352454717693826, Validation Loss: 0.27604301214012605\n",
      "Epoch 1046/3000, Training Loss: 0.16338563331821193, Validation Loss: 0.27593518825863594\n",
      "Epoch 1047/3000, Training Loss: 0.16324732594164895, Validation Loss: 0.27582480755113964\n",
      "Epoch 1048/3000, Training Loss: 0.1631093331804298, Validation Loss: 0.27571014759625767\n",
      "Epoch 1049/3000, Training Loss: 0.1629708721847407, Validation Loss: 0.2756013158282815\n",
      "Epoch 1050/3000, Training Loss: 0.16283376315124665, Validation Loss: 0.2754925475788496\n",
      "Epoch 1051/3000, Training Loss: 0.16269589602252077, Validation Loss: 0.27537965685386967\n",
      "Epoch 1052/3000, Training Loss: 0.16255882913943986, Validation Loss: 0.2752702799838512\n",
      "Epoch 1053/3000, Training Loss: 0.16242165021505045, Validation Loss: 0.2751625694376163\n",
      "Epoch 1054/3000, Training Loss: 0.16228474564144219, Validation Loss: 0.275054615494146\n",
      "Epoch 1055/3000, Training Loss: 0.1621487214159789, Validation Loss: 0.27494099177853365\n",
      "Epoch 1056/3000, Training Loss: 0.16201225052896254, Validation Loss: 0.27483670045004144\n",
      "Epoch 1057/3000, Training Loss: 0.1618762206602812, Validation Loss: 0.27472825487135333\n",
      "Epoch 1058/3000, Training Loss: 0.1617404347418801, Validation Loss: 0.2746165818018951\n",
      "Epoch 1059/3000, Training Loss: 0.1616046580769783, Validation Loss: 0.27450780082376\n",
      "Epoch 1060/3000, Training Loss: 0.16146942545376525, Validation Loss: 0.2744063894598755\n",
      "Epoch 1061/3000, Training Loss: 0.16133395340628512, Validation Loss: 0.27429428374279474\n",
      "Epoch 1062/3000, Training Loss: 0.16119936422943315, Validation Loss: 0.27418898272509606\n",
      "Epoch 1063/3000, Training Loss: 0.16106415779244826, Validation Loss: 0.27408446583411983\n",
      "Epoch 1064/3000, Training Loss: 0.16092965482436938, Validation Loss: 0.2739713839244986\n",
      "Epoch 1065/3000, Training Loss: 0.16079562093926494, Validation Loss: 0.27386654105249814\n",
      "Epoch 1066/3000, Training Loss: 0.1606608424932888, Validation Loss: 0.2737612693600739\n",
      "Epoch 1067/3000, Training Loss: 0.160527439184049, Validation Loss: 0.2736525186168789\n",
      "Epoch 1068/3000, Training Loss: 0.16039341908460686, Validation Loss: 0.27354746199697566\n",
      "Epoch 1069/3000, Training Loss: 0.1602600302119374, Validation Loss: 0.2734467922330301\n",
      "Epoch 1070/3000, Training Loss: 0.16012661857876215, Validation Loss: 0.27333935273378623\n",
      "Epoch 1071/3000, Training Loss: 0.1599935030214735, Validation Loss: 0.27323379811756504\n",
      "Epoch 1072/3000, Training Loss: 0.15986069261689523, Validation Loss: 0.27313410023881945\n",
      "Epoch 1073/3000, Training Loss: 0.15972777017350098, Validation Loss: 0.2730310951253771\n",
      "Epoch 1074/3000, Training Loss: 0.15959554513167615, Validation Loss: 0.27292167467554107\n",
      "Epoch 1075/3000, Training Loss: 0.15946323217994918, Validation Loss: 0.27282003154938883\n",
      "Epoch 1076/3000, Training Loss: 0.15933076117841186, Validation Loss: 0.2727184981306279\n",
      "Epoch 1077/3000, Training Loss: 0.15919873826008782, Validation Loss: 0.27261458244732545\n",
      "Epoch 1078/3000, Training Loss: 0.1590670796548408, Validation Loss: 0.2725104358519801\n",
      "Epoch 1079/3000, Training Loss: 0.15893522883984348, Validation Loss: 0.2724086078402768\n",
      "Epoch 1080/3000, Training Loss: 0.15880389689700442, Validation Loss: 0.2723015305536969\n",
      "Epoch 1081/3000, Training Loss: 0.15867266691371934, Validation Loss: 0.2722013790800368\n",
      "Epoch 1082/3000, Training Loss: 0.15854131367965454, Validation Loss: 0.27210314286754506\n",
      "Epoch 1083/3000, Training Loss: 0.15841056697882247, Validation Loss: 0.271996043424687\n",
      "Epoch 1084/3000, Training Loss: 0.15827966053457346, Validation Loss: 0.27189256767123193\n",
      "Epoch 1085/3000, Training Loss: 0.15814894182260905, Validation Loss: 0.27179347726028424\n",
      "Epoch 1086/3000, Training Loss: 0.1580186275580841, Validation Loss: 0.2716878039905331\n",
      "Epoch 1087/3000, Training Loss: 0.1578882362807109, Validation Loss: 0.2715854020970489\n",
      "Epoch 1088/3000, Training Loss: 0.15775894895393017, Validation Loss: 0.27148705917018273\n",
      "Epoch 1089/3000, Training Loss: 0.15762911340497832, Validation Loss: 0.27137928008797096\n",
      "Epoch 1090/3000, Training Loss: 0.15749992901828003, Validation Loss: 0.27128329181698596\n",
      "Epoch 1091/3000, Training Loss: 0.15737069111324614, Validation Loss: 0.271183719687376\n",
      "Epoch 1092/3000, Training Loss: 0.15724176102372364, Validation Loss: 0.2710768697637074\n",
      "Epoch 1093/3000, Training Loss: 0.1571131123880608, Validation Loss: 0.27097988563704856\n",
      "Epoch 1094/3000, Training Loss: 0.1569845247071703, Validation Loss: 0.27088092213132065\n",
      "Epoch 1095/3000, Training Loss: 0.156855841693786, Validation Loss: 0.2707773847388822\n",
      "Epoch 1096/3000, Training Loss: 0.15672734054880735, Validation Loss: 0.27067771208884617\n",
      "Epoch 1097/3000, Training Loss: 0.1565994219447748, Validation Loss: 0.270579539199648\n",
      "Epoch 1098/3000, Training Loss: 0.15647136021928615, Validation Loss: 0.27048478005109666\n",
      "Epoch 1099/3000, Training Loss: 0.15634355598270797, Validation Loss: 0.2703794996802852\n",
      "Epoch 1100/3000, Training Loss: 0.1562160186851486, Validation Loss: 0.2702841352436219\n",
      "Epoch 1101/3000, Training Loss: 0.1560885048741208, Validation Loss: 0.27017907424951604\n",
      "Epoch 1102/3000, Training Loss: 0.1559610381485022, Validation Loss: 0.2700843252047828\n",
      "Epoch 1103/3000, Training Loss: 0.15583418326827156, Validation Loss: 0.26998633994573107\n",
      "Epoch 1104/3000, Training Loss: 0.1557073770476059, Validation Loss: 0.26988673195544305\n",
      "Epoch 1105/3000, Training Loss: 0.15558044404992943, Validation Loss: 0.2697915535257604\n",
      "Epoch 1106/3000, Training Loss: 0.15545425747434072, Validation Loss: 0.2696863676611045\n",
      "Epoch 1107/3000, Training Loss: 0.15532798463703226, Validation Loss: 0.2695922358546372\n",
      "Epoch 1108/3000, Training Loss: 0.1552019350694341, Validation Loss: 0.2694958431025547\n",
      "Epoch 1109/3000, Training Loss: 0.1550762601640543, Validation Loss: 0.2693975052097831\n",
      "Epoch 1110/3000, Training Loss: 0.154950012329948, Validation Loss: 0.2693042114039649\n",
      "Epoch 1111/3000, Training Loss: 0.15482499684566728, Validation Loss: 0.2692038498416622\n",
      "Epoch 1112/3000, Training Loss: 0.1546991655374315, Validation Loss: 0.2691114375782762\n",
      "Epoch 1113/3000, Training Loss: 0.154573905872475, Validation Loss: 0.26901677912586985\n",
      "Epoch 1114/3000, Training Loss: 0.1544488141004091, Validation Loss: 0.26891796226922354\n",
      "Epoch 1115/3000, Training Loss: 0.15432379476033325, Validation Loss: 0.2688227921287229\n",
      "Epoch 1116/3000, Training Loss: 0.15419903428519247, Validation Loss: 0.268723594508188\n",
      "Epoch 1117/3000, Training Loss: 0.15407444088952593, Validation Loss: 0.2686316061736958\n",
      "Epoch 1118/3000, Training Loss: 0.15395021392432082, Validation Loss: 0.26854039605991625\n",
      "Epoch 1119/3000, Training Loss: 0.1538258242527942, Validation Loss: 0.2684406165418423\n",
      "Epoch 1120/3000, Training Loss: 0.15370208406685293, Validation Loss: 0.26834823803320995\n",
      "Epoch 1121/3000, Training Loss: 0.15357809658219598, Validation Loss: 0.2682513401593714\n",
      "Epoch 1122/3000, Training Loss: 0.15345428226297994, Validation Loss: 0.26815827862837155\n",
      "Epoch 1123/3000, Training Loss: 0.15333107473912488, Validation Loss: 0.26807025192245015\n",
      "Epoch 1124/3000, Training Loss: 0.153207575814825, Validation Loss: 0.2679679334139102\n",
      "Epoch 1125/3000, Training Loss: 0.15308452049115695, Validation Loss: 0.26787624352215245\n",
      "Epoch 1126/3000, Training Loss: 0.1529615667194417, Validation Loss: 0.26777679996768833\n",
      "Epoch 1127/3000, Training Loss: 0.15283872862328482, Validation Loss: 0.2676839876507098\n",
      "Epoch 1128/3000, Training Loss: 0.15271631599533592, Validation Loss: 0.2675947938807606\n",
      "Epoch 1129/3000, Training Loss: 0.152593751074986, Validation Loss: 0.2674920104879951\n",
      "Epoch 1130/3000, Training Loss: 0.15247132476603412, Validation Loss: 0.26740293056103304\n",
      "Epoch 1131/3000, Training Loss: 0.15234921814290428, Validation Loss: 0.26730840502123815\n",
      "Epoch 1132/3000, Training Loss: 0.15222737315651894, Validation Loss: 0.2672137059720436\n",
      "Epoch 1133/3000, Training Loss: 0.1521054325747344, Validation Loss: 0.2671243448172773\n",
      "Epoch 1134/3000, Training Loss: 0.15198389964991008, Validation Loss: 0.2670287745024372\n",
      "Epoch 1135/3000, Training Loss: 0.1518624005074721, Validation Loss: 0.26693164565806254\n",
      "Epoch 1136/3000, Training Loss: 0.15174098199865946, Validation Loss: 0.2668369966545573\n",
      "Epoch 1137/3000, Training Loss: 0.15162019630295365, Validation Loss: 0.26674666804099817\n",
      "Epoch 1138/3000, Training Loss: 0.1514988079819729, Validation Loss: 0.2666601836124766\n",
      "Epoch 1139/3000, Training Loss: 0.15137841021950946, Validation Loss: 0.266559358208488\n",
      "Epoch 1140/3000, Training Loss: 0.15125773846358237, Validation Loss: 0.2664694619130459\n",
      "Epoch 1141/3000, Training Loss: 0.15113689793187485, Validation Loss: 0.26637449094048654\n",
      "Epoch 1142/3000, Training Loss: 0.1510170832261493, Validation Loss: 0.2662815313687974\n",
      "Epoch 1143/3000, Training Loss: 0.15089708590581458, Validation Loss: 0.26619246619711356\n",
      "Epoch 1144/3000, Training Loss: 0.15077715707978137, Validation Loss: 0.26609568747693235\n",
      "Epoch 1145/3000, Training Loss: 0.15065745528578997, Validation Loss: 0.26600286400678125\n",
      "Epoch 1146/3000, Training Loss: 0.15053795366638356, Validation Loss: 0.2659089964981641\n",
      "Epoch 1147/3000, Training Loss: 0.15041858137012423, Validation Loss: 0.2658191991209978\n",
      "Epoch 1148/3000, Training Loss: 0.15029948013265662, Validation Loss: 0.2657237502515831\n",
      "Epoch 1149/3000, Training Loss: 0.1501803887378877, Validation Loss: 0.26562988968247564\n",
      "Epoch 1150/3000, Training Loss: 0.15006132088711277, Validation Loss: 0.26554177065758605\n",
      "Epoch 1151/3000, Training Loss: 0.14994267296965083, Validation Loss: 0.2654442203852558\n",
      "Epoch 1152/3000, Training Loss: 0.14982408170195427, Validation Loss: 0.26535575781384757\n",
      "Epoch 1153/3000, Training Loss: 0.14970553983255575, Validation Loss: 0.2652656672020299\n",
      "Epoch 1154/3000, Training Loss: 0.14958762441160023, Validation Loss: 0.26517057669811184\n",
      "Epoch 1155/3000, Training Loss: 0.14946910824073642, Validation Loss: 0.2650825706217959\n",
      "Epoch 1156/3000, Training Loss: 0.14935139055624821, Validation Loss: 0.26498860828903853\n",
      "Epoch 1157/3000, Training Loss: 0.14923366577062125, Validation Loss: 0.26489883220317717\n",
      "Epoch 1158/3000, Training Loss: 0.1491157708945862, Validation Loss: 0.2648094342708091\n",
      "Epoch 1159/3000, Training Loss: 0.14899852139702904, Validation Loss: 0.264715009149749\n",
      "Epoch 1160/3000, Training Loss: 0.1488810889700492, Validation Loss: 0.2646258286397227\n",
      "Epoch 1161/3000, Training Loss: 0.14876374838400405, Validation Loss: 0.2645364933455852\n",
      "Epoch 1162/3000, Training Loss: 0.14864713169918897, Validation Loss: 0.26444247739469684\n",
      "Epoch 1163/3000, Training Loss: 0.1485299268581989, Validation Loss: 0.2643552833472245\n",
      "Epoch 1164/3000, Training Loss: 0.14841333609637888, Validation Loss: 0.2642638984636813\n",
      "Epoch 1165/3000, Training Loss: 0.14829651764264315, Validation Loss: 0.2641808588765212\n",
      "Epoch 1166/3000, Training Loss: 0.14818014168280286, Validation Loss: 0.2640828806978923\n",
      "Epoch 1167/3000, Training Loss: 0.14806432683244197, Validation Loss: 0.26399827568282425\n",
      "Epoch 1168/3000, Training Loss: 0.14794782738437878, Validation Loss: 0.2639089974163786\n",
      "Epoch 1169/3000, Training Loss: 0.1478320170440673, Validation Loss: 0.26381952095372424\n",
      "Epoch 1170/3000, Training Loss: 0.14771622454011277, Validation Loss: 0.263734613951391\n",
      "Epoch 1171/3000, Training Loss: 0.14760047024162073, Validation Loss: 0.2636383339125421\n",
      "Epoch 1172/3000, Training Loss: 0.1474851833662904, Validation Loss: 0.263555788563258\n",
      "Epoch 1173/3000, Training Loss: 0.14736977243307914, Validation Loss: 0.2634669161210036\n",
      "Epoch 1174/3000, Training Loss: 0.14725463896169383, Validation Loss: 0.26337738494144725\n",
      "Epoch 1175/3000, Training Loss: 0.14713978747421422, Validation Loss: 0.26329093082626376\n",
      "Epoch 1176/3000, Training Loss: 0.14702490345847255, Validation Loss: 0.2631950168405954\n",
      "Epoch 1177/3000, Training Loss: 0.14691026509329302, Validation Loss: 0.26311153160493733\n",
      "Epoch 1178/3000, Training Loss: 0.14679597010149373, Validation Loss: 0.26302596065308337\n",
      "Epoch 1179/3000, Training Loss: 0.14668153527868436, Validation Loss: 0.26293347995987576\n",
      "Epoch 1180/3000, Training Loss: 0.14656756559742856, Validation Loss: 0.26284806294264706\n",
      "Epoch 1181/3000, Training Loss: 0.14645376189330275, Validation Loss: 0.2627596206247767\n",
      "Epoch 1182/3000, Training Loss: 0.14633961211104302, Validation Loss: 0.26267285042295413\n",
      "Epoch 1183/3000, Training Loss: 0.14622628341832156, Validation Loss: 0.2625806385627674\n",
      "Epoch 1184/3000, Training Loss: 0.14611264763733495, Validation Loss: 0.26249671651393386\n",
      "Epoch 1185/3000, Training Loss: 0.1459996048125503, Validation Loss: 0.26241199268453846\n",
      "Epoch 1186/3000, Training Loss: 0.1458864915575889, Validation Loss: 0.26232231932375516\n",
      "Epoch 1187/3000, Training Loss: 0.14577337061289683, Validation Loss: 0.2622337709741002\n",
      "Epoch 1188/3000, Training Loss: 0.14566037300024898, Validation Loss: 0.262149435873159\n",
      "Epoch 1189/3000, Training Loss: 0.14554790818542931, Validation Loss: 0.26206076516535237\n",
      "Epoch 1190/3000, Training Loss: 0.14543522238638518, Validation Loss: 0.26197339167280753\n",
      "Epoch 1191/3000, Training Loss: 0.14532313870041702, Validation Loss: 0.2618901754385982\n",
      "Epoch 1192/3000, Training Loss: 0.14521060171594083, Validation Loss: 0.2618010374867199\n",
      "Epoch 1193/3000, Training Loss: 0.1450984001400872, Validation Loss: 0.26171393509868196\n",
      "Epoch 1194/3000, Training Loss: 0.1449865657943809, Validation Loss: 0.2616325558360856\n",
      "Epoch 1195/3000, Training Loss: 0.14487452895603084, Validation Loss: 0.26154795809757625\n",
      "Epoch 1196/3000, Training Loss: 0.14476330261126266, Validation Loss: 0.26145881776579655\n",
      "Epoch 1197/3000, Training Loss: 0.14465176441634894, Validation Loss: 0.26137606724172835\n",
      "Epoch 1198/3000, Training Loss: 0.14454043059223964, Validation Loss: 0.26129078970317254\n",
      "Epoch 1199/3000, Training Loss: 0.14442940961117698, Validation Loss: 0.2612059231905195\n",
      "Epoch 1200/3000, Training Loss: 0.14431854963941007, Validation Loss: 0.26112755609163096\n",
      "Epoch 1201/3000, Training Loss: 0.14420787528194753, Validation Loss: 0.2610420798968104\n",
      "Epoch 1202/3000, Training Loss: 0.14409714484794914, Validation Loss: 0.26095976205760973\n",
      "Epoch 1203/3000, Training Loss: 0.1439864753668502, Validation Loss: 0.26087342994454865\n",
      "Epoch 1204/3000, Training Loss: 0.1438764023950175, Validation Loss: 0.26078688150004226\n",
      "Epoch 1205/3000, Training Loss: 0.14376610468257495, Validation Loss: 0.26070413494484523\n",
      "Epoch 1206/3000, Training Loss: 0.1436559986940524, Validation Loss: 0.2606249015731641\n",
      "Epoch 1207/3000, Training Loss: 0.14354634993473755, Validation Loss: 0.26053952517845275\n",
      "Epoch 1208/3000, Training Loss: 0.14343656272831837, Validation Loss: 0.2604583226022356\n",
      "Epoch 1209/3000, Training Loss: 0.14332691168805017, Validation Loss: 0.2603744628716931\n",
      "Epoch 1210/3000, Training Loss: 0.14321767483640804, Validation Loss: 0.26029197047968966\n",
      "Epoch 1211/3000, Training Loss: 0.14310820754704529, Validation Loss: 0.26021322740248176\n",
      "Epoch 1212/3000, Training Loss: 0.14299915133725022, Validation Loss: 0.2601318882847778\n",
      "Epoch 1213/3000, Training Loss: 0.14289012460825515, Validation Loss: 0.2600475369657872\n",
      "Epoch 1214/3000, Training Loss: 0.14278132582175626, Validation Loss: 0.25996653074852627\n",
      "Epoch 1215/3000, Training Loss: 0.14267258507182723, Validation Loss: 0.2598837305051937\n",
      "Epoch 1216/3000, Training Loss: 0.14256398690937908, Validation Loss: 0.25980180612268455\n",
      "Epoch 1217/3000, Training Loss: 0.14245539339103916, Validation Loss: 0.25972690921270414\n",
      "Epoch 1218/3000, Training Loss: 0.1423471938494471, Validation Loss: 0.25964213803499897\n",
      "Epoch 1219/3000, Training Loss: 0.14223866231324442, Validation Loss: 0.2595597137278632\n",
      "Epoch 1220/3000, Training Loss: 0.14213106177098506, Validation Loss: 0.25948313867574196\n",
      "Epoch 1221/3000, Training Loss: 0.1420230211471918, Validation Loss: 0.25939859430180207\n",
      "Epoch 1222/3000, Training Loss: 0.1419155493327153, Validation Loss: 0.25932119338501874\n",
      "Epoch 1223/3000, Training Loss: 0.1418081461062174, Validation Loss: 0.2592386292842705\n",
      "Epoch 1224/3000, Training Loss: 0.14170063385688636, Validation Loss: 0.25916304246219596\n",
      "Epoch 1225/3000, Training Loss: 0.1415934117301917, Validation Loss: 0.2590751324208302\n",
      "Epoch 1226/3000, Training Loss: 0.1414865526309107, Validation Loss: 0.25900233876720224\n",
      "Epoch 1227/3000, Training Loss: 0.14137926237681098, Validation Loss: 0.25891807648845594\n",
      "Epoch 1228/3000, Training Loss: 0.14127277895404305, Validation Loss: 0.25884156306927253\n",
      "Epoch 1229/3000, Training Loss: 0.14116578640395822, Validation Loss: 0.2587637054153588\n",
      "Epoch 1230/3000, Training Loss: 0.1410594535838028, Validation Loss: 0.25867979850458994\n",
      "Epoch 1231/3000, Training Loss: 0.1409529551532378, Validation Loss: 0.25860263386317695\n",
      "Epoch 1232/3000, Training Loss: 0.14084640646663824, Validation Loss: 0.2585213670938955\n",
      "Epoch 1233/3000, Training Loss: 0.14074081529087376, Validation Loss: 0.258442449427024\n",
      "Epoch 1234/3000, Training Loss: 0.14063420250991351, Validation Loss: 0.25836711835716575\n",
      "Epoch 1235/3000, Training Loss: 0.14052859381687569, Validation Loss: 0.25828324689938514\n",
      "Epoch 1236/3000, Training Loss: 0.14042269485751793, Validation Loss: 0.25820655890327504\n",
      "Epoch 1237/3000, Training Loss: 0.1403168169151812, Validation Loss: 0.25812809308346263\n",
      "Epoch 1238/3000, Training Loss: 0.14021177815719635, Validation Loss: 0.25804973826805694\n",
      "Epoch 1239/3000, Training Loss: 0.1401059546738261, Validation Loss: 0.25797262363046675\n",
      "Epoch 1240/3000, Training Loss: 0.14000114910833225, Validation Loss: 0.2578904403052223\n",
      "Epoch 1241/3000, Training Loss: 0.13989608820396093, Validation Loss: 0.2578174582125419\n",
      "Epoch 1242/3000, Training Loss: 0.1397908418720177, Validation Loss: 0.25773197807941567\n",
      "Epoch 1243/3000, Training Loss: 0.13968638678592735, Validation Loss: 0.25765575220817877\n",
      "Epoch 1244/3000, Training Loss: 0.13958150041699655, Validation Loss: 0.25758215012941754\n",
      "Epoch 1245/3000, Training Loss: 0.1394770919760849, Validation Loss: 0.25750014393340614\n",
      "Epoch 1246/3000, Training Loss: 0.13937272915984789, Validation Loss: 0.2574274370187231\n",
      "Epoch 1247/3000, Training Loss: 0.13926834550439215, Validation Loss: 0.25734608347269883\n",
      "Epoch 1248/3000, Training Loss: 0.13916428472120804, Validation Loss: 0.25727270991885404\n",
      "Epoch 1249/3000, Training Loss: 0.1390602310174508, Validation Loss: 0.2571869262460205\n",
      "Epoch 1250/3000, Training Loss: 0.13895650362245152, Validation Loss: 0.25711392753199064\n",
      "Epoch 1251/3000, Training Loss: 0.13885265403495078, Validation Loss: 0.2570404997697384\n",
      "Epoch 1252/3000, Training Loss: 0.1387491669248684, Validation Loss: 0.2569570985261675\n",
      "Epoch 1253/3000, Training Loss: 0.1386458573988544, Validation Loss: 0.25688442831266406\n",
      "Epoch 1254/3000, Training Loss: 0.13854226851020884, Validation Loss: 0.2568053214330554\n",
      "Epoch 1255/3000, Training Loss: 0.13843907786305173, Validation Loss: 0.2567307008157331\n",
      "Epoch 1256/3000, Training Loss: 0.13833595459612233, Validation Loss: 0.2566557592119237\n",
      "Epoch 1257/3000, Training Loss: 0.13823206791043766, Validation Loss: 0.2565772470221843\n",
      "Epoch 1258/3000, Training Loss: 0.1381292571814654, Validation Loss: 0.2565025789819086\n",
      "Epoch 1259/3000, Training Loss: 0.1380254987779588, Validation Loss: 0.2564180164121981\n",
      "Epoch 1260/3000, Training Loss: 0.1379223073478099, Validation Loss: 0.25634715534896424\n",
      "Epoch 1261/3000, Training Loss: 0.13781948630168292, Validation Loss: 0.2562721887145049\n",
      "Epoch 1262/3000, Training Loss: 0.13771646380604394, Validation Loss: 0.2561932532562326\n",
      "Epoch 1263/3000, Training Loss: 0.13761346747414974, Validation Loss: 0.2561191502632846\n",
      "Epoch 1264/3000, Training Loss: 0.137510943279909, Validation Loss: 0.2560449249234557\n",
      "Epoch 1265/3000, Training Loss: 0.13740837941163336, Validation Loss: 0.2559678749299946\n",
      "Epoch 1266/3000, Training Loss: 0.13730580218809355, Validation Loss: 0.2558946553701978\n",
      "Epoch 1267/3000, Training Loss: 0.13720368504989006, Validation Loss: 0.2558157245405217\n",
      "Epoch 1268/3000, Training Loss: 0.1371013044034173, Validation Loss: 0.2557425179753833\n",
      "Epoch 1269/3000, Training Loss: 0.13699950696222513, Validation Loss: 0.25566415526281305\n",
      "Epoch 1270/3000, Training Loss: 0.1368974258803748, Validation Loss: 0.2555885093084352\n",
      "Epoch 1271/3000, Training Loss: 0.13679571101365398, Validation Loss: 0.25552024049679645\n",
      "Epoch 1272/3000, Training Loss: 0.1366940003812758, Validation Loss: 0.2554403609842242\n",
      "Epoch 1273/3000, Training Loss: 0.13659206250155737, Validation Loss: 0.2553669701414126\n",
      "Epoch 1274/3000, Training Loss: 0.13649071216174638, Validation Loss: 0.2552920553335492\n",
      "Epoch 1275/3000, Training Loss: 0.1363891832410836, Validation Loss: 0.2552200290235388\n",
      "Epoch 1276/3000, Training Loss: 0.13628803989410912, Validation Loss: 0.2551433308425032\n",
      "Epoch 1277/3000, Training Loss: 0.13618667928282507, Validation Loss: 0.25507067150181534\n",
      "Epoch 1278/3000, Training Loss: 0.13608605695692325, Validation Loss: 0.25499304532765826\n",
      "Epoch 1279/3000, Training Loss: 0.13598501240406052, Validation Loss: 0.2549174082066026\n",
      "Epoch 1280/3000, Training Loss: 0.13588415250582866, Validation Loss: 0.2548460764808737\n",
      "Epoch 1281/3000, Training Loss: 0.13578376008495063, Validation Loss: 0.25477150857324077\n",
      "Epoch 1282/3000, Training Loss: 0.13568299911064466, Validation Loss: 0.25469848858705035\n",
      "Epoch 1283/3000, Training Loss: 0.13558287287058815, Validation Loss: 0.2546250486560404\n",
      "Epoch 1284/3000, Training Loss: 0.13548252635332242, Validation Loss: 0.2545529613841594\n",
      "Epoch 1285/3000, Training Loss: 0.13538217620858545, Validation Loss: 0.2544776065059037\n",
      "Epoch 1286/3000, Training Loss: 0.13528240470667222, Validation Loss: 0.2544022362383362\n",
      "Epoch 1287/3000, Training Loss: 0.13518234245676214, Validation Loss: 0.25433337070812484\n",
      "Epoch 1288/3000, Training Loss: 0.13508251424049084, Validation Loss: 0.2542586954932998\n",
      "Epoch 1289/3000, Training Loss: 0.13498296026181916, Validation Loss: 0.2541856825888987\n",
      "Epoch 1290/3000, Training Loss: 0.13488306051945745, Validation Loss: 0.25411300278137394\n",
      "Epoch 1291/3000, Training Loss: 0.13478381260582992, Validation Loss: 0.25403984004315183\n",
      "Epoch 1292/3000, Training Loss: 0.13468440652353172, Validation Loss: 0.2539697054933371\n",
      "Epoch 1293/3000, Training Loss: 0.13458517954835017, Validation Loss: 0.25389971616672485\n",
      "Epoch 1294/3000, Training Loss: 0.13448605343543635, Validation Loss: 0.25382245129731384\n",
      "Epoch 1295/3000, Training Loss: 0.13438709584363695, Validation Loss: 0.2537570886735285\n",
      "Epoch 1296/3000, Training Loss: 0.13428826426288415, Validation Loss: 0.25367996891658995\n",
      "Epoch 1297/3000, Training Loss: 0.13418934088359558, Validation Loss: 0.25360995461533237\n",
      "Epoch 1298/3000, Training Loss: 0.13409105745092673, Validation Loss: 0.2535454197781546\n",
      "Epoch 1299/3000, Training Loss: 0.13399247784540624, Validation Loss: 0.2534637080502726\n",
      "Epoch 1300/3000, Training Loss: 0.13389397813816567, Validation Loss: 0.2533992568081809\n",
      "Epoch 1301/3000, Training Loss: 0.13379578917046958, Validation Loss: 0.25332296052488423\n",
      "Epoch 1302/3000, Training Loss: 0.13369744478530263, Validation Loss: 0.2532559226192536\n",
      "Epoch 1303/3000, Training Loss: 0.133599717531638, Validation Loss: 0.2531887204022323\n",
      "Epoch 1304/3000, Training Loss: 0.13350152020112013, Validation Loss: 0.2531135372909151\n",
      "Epoch 1305/3000, Training Loss: 0.1334040459090115, Validation Loss: 0.25304543851327466\n",
      "Epoch 1306/3000, Training Loss: 0.13330638142011259, Validation Loss: 0.25297176446205216\n",
      "Epoch 1307/3000, Training Loss: 0.13320851580484228, Validation Loss: 0.25290626200539107\n",
      "Epoch 1308/3000, Training Loss: 0.13311147125940664, Validation Loss: 0.25283395320400276\n",
      "Epoch 1309/3000, Training Loss: 0.13301409961017926, Validation Loss: 0.2527617179959765\n",
      "Epoch 1310/3000, Training Loss: 0.1329171378421233, Validation Loss: 0.2526942039408666\n",
      "Epoch 1311/3000, Training Loss: 0.1328200392022703, Validation Loss: 0.2526242365370446\n",
      "Epoch 1312/3000, Training Loss: 0.13272304476518834, Validation Loss: 0.252558699159715\n",
      "Epoch 1313/3000, Training Loss: 0.13262646855123317, Validation Loss: 0.25248516021907375\n",
      "Epoch 1314/3000, Training Loss: 0.13252959874860393, Validation Loss: 0.2524181480943722\n",
      "Epoch 1315/3000, Training Loss: 0.13243320468104247, Validation Loss: 0.2523555302587016\n",
      "Epoch 1316/3000, Training Loss: 0.1323368040403522, Validation Loss: 0.252279236895156\n",
      "Epoch 1317/3000, Training Loss: 0.13224033675278526, Validation Loss: 0.25221397694385533\n",
      "Epoch 1318/3000, Training Loss: 0.1321443720977946, Validation Loss: 0.2521416653725404\n",
      "Epoch 1319/3000, Training Loss: 0.13204805437477873, Validation Loss: 0.25207720956340246\n",
      "Epoch 1320/3000, Training Loss: 0.13195238318577943, Validation Loss: 0.2520090985759493\n",
      "Epoch 1321/3000, Training Loss: 0.13185657659413733, Validation Loss: 0.2519381972675111\n",
      "Epoch 1322/3000, Training Loss: 0.13176061685761833, Validation Loss: 0.25187205376656957\n",
      "Epoch 1323/3000, Training Loss: 0.13166548041914547, Validation Loss: 0.25180152469696326\n",
      "Epoch 1324/3000, Training Loss: 0.13156970895699577, Validation Loss: 0.25173418178322315\n",
      "Epoch 1325/3000, Training Loss: 0.13147446596370585, Validation Loss: 0.25167120462608294\n",
      "Epoch 1326/3000, Training Loss: 0.1313793193047763, Validation Loss: 0.25159942953198233\n",
      "Epoch 1327/3000, Training Loss: 0.13128428063226033, Validation Loss: 0.2515385316884314\n",
      "Epoch 1328/3000, Training Loss: 0.13118975243000683, Validation Loss: 0.2514768101570503\n",
      "Epoch 1329/3000, Training Loss: 0.13109526503632596, Validation Loss: 0.2514120395167114\n",
      "Epoch 1330/3000, Training Loss: 0.1310013079328667, Validation Loss: 0.2513487702001802\n",
      "Epoch 1331/3000, Training Loss: 0.13090668298049687, Validation Loss: 0.2512837263746358\n",
      "Epoch 1332/3000, Training Loss: 0.13081321813328403, Validation Loss: 0.25122835747427225\n",
      "Epoch 1333/3000, Training Loss: 0.13071894395683245, Validation Loss: 0.2511573764390473\n",
      "Epoch 1334/3000, Training Loss: 0.1306248524459088, Validation Loss: 0.25110276995070274\n",
      "Epoch 1335/3000, Training Loss: 0.13053149480633788, Validation Loss: 0.25103808122408283\n",
      "Epoch 1336/3000, Training Loss: 0.13043756069962548, Validation Loss: 0.2509713577474074\n",
      "Epoch 1337/3000, Training Loss: 0.1303444831763575, Validation Loss: 0.2509107045703591\n",
      "Epoch 1338/3000, Training Loss: 0.13025080551834128, Validation Loss: 0.25085084885050024\n",
      "Epoch 1339/3000, Training Loss: 0.1301577096894369, Validation Loss: 0.25079099555955253\n",
      "Epoch 1340/3000, Training Loss: 0.13006445623832047, Validation Loss: 0.2507262262722632\n",
      "Epoch 1341/3000, Training Loss: 0.12997139378829742, Validation Loss: 0.2506716329560916\n",
      "Epoch 1342/3000, Training Loss: 0.1298789675706144, Validation Loss: 0.2506066442654336\n",
      "Epoch 1343/3000, Training Loss: 0.12978588367250277, Validation Loss: 0.2505405100692068\n",
      "Epoch 1344/3000, Training Loss: 0.12969314632481338, Validation Loss: 0.25048322196058886\n",
      "Epoch 1345/3000, Training Loss: 0.12960067901891023, Validation Loss: 0.25042139290576937\n",
      "Epoch 1346/3000, Training Loss: 0.12950817278035007, Validation Loss: 0.25036284702492406\n",
      "Epoch 1347/3000, Training Loss: 0.1294157749237061, Validation Loss: 0.2503040311723889\n",
      "Epoch 1348/3000, Training Loss: 0.129323268035446, Validation Loss: 0.25023400943388274\n",
      "Epoch 1349/3000, Training Loss: 0.1292311336678983, Validation Loss: 0.25017903310991213\n",
      "Epoch 1350/3000, Training Loss: 0.1291388489401907, Validation Loss: 0.25011336276478324\n",
      "Epoch 1351/3000, Training Loss: 0.1290468148601912, Validation Loss: 0.25005802828121015\n",
      "Epoch 1352/3000, Training Loss: 0.1289549340837258, Validation Loss: 0.24999094300015756\n",
      "Epoch 1353/3000, Training Loss: 0.1288628229578773, Validation Loss: 0.24993450421418917\n",
      "Epoch 1354/3000, Training Loss: 0.12877136491429914, Validation Loss: 0.24987702115182775\n",
      "Epoch 1355/3000, Training Loss: 0.1286795064487707, Validation Loss: 0.2498106860650638\n",
      "Epoch 1356/3000, Training Loss: 0.12858795608541881, Validation Loss: 0.2497527537432052\n",
      "Epoch 1357/3000, Training Loss: 0.12849654715221798, Validation Loss: 0.24968866296516273\n",
      "Epoch 1358/3000, Training Loss: 0.12840521631211532, Validation Loss: 0.24963090422796638\n",
      "Epoch 1359/3000, Training Loss: 0.12831401614843127, Validation Loss: 0.24957016593980133\n",
      "Epoch 1360/3000, Training Loss: 0.1282227787091878, Validation Loss: 0.24951451455348775\n",
      "Epoch 1361/3000, Training Loss: 0.12813201764897614, Validation Loss: 0.24945673795403187\n",
      "Epoch 1362/3000, Training Loss: 0.12804070819680013, Validation Loss: 0.2493883368953765\n",
      "Epoch 1363/3000, Training Loss: 0.1279504061570116, Validation Loss: 0.24933310565148353\n",
      "Epoch 1364/3000, Training Loss: 0.12785956766617154, Validation Loss: 0.2492709063849387\n",
      "Epoch 1365/3000, Training Loss: 0.1277688016067752, Validation Loss: 0.24921206242759494\n",
      "Epoch 1366/3000, Training Loss: 0.1276783278170348, Validation Loss: 0.24915001394044115\n",
      "Epoch 1367/3000, Training Loss: 0.12758787760303283, Validation Loss: 0.24909796495082498\n",
      "Epoch 1368/3000, Training Loss: 0.12749763121983107, Validation Loss: 0.2490365947151743\n",
      "Epoch 1369/3000, Training Loss: 0.12740745787426874, Validation Loss: 0.24897079258501972\n",
      "Epoch 1370/3000, Training Loss: 0.1273172294889183, Validation Loss: 0.24891509409635376\n",
      "Epoch 1371/3000, Training Loss: 0.12722730777042324, Validation Loss: 0.2488526432110448\n",
      "Epoch 1372/3000, Training Loss: 0.12713733325224566, Validation Loss: 0.2487973899568335\n",
      "Epoch 1373/3000, Training Loss: 0.12704750377401666, Validation Loss: 0.24874333606741467\n",
      "Epoch 1374/3000, Training Loss: 0.1269579261006158, Validation Loss: 0.24868175207429113\n",
      "Epoch 1375/3000, Training Loss: 0.12686820541371732, Validation Loss: 0.2486252955528576\n",
      "Epoch 1376/3000, Training Loss: 0.12677886778037667, Validation Loss: 0.24855783729960973\n",
      "Epoch 1377/3000, Training Loss: 0.12668928055986134, Validation Loss: 0.24850503560138956\n",
      "Epoch 1378/3000, Training Loss: 0.12660023690823624, Validation Loss: 0.24844411528016178\n",
      "Epoch 1379/3000, Training Loss: 0.1265109297943579, Validation Loss: 0.24838903768038587\n",
      "Epoch 1380/3000, Training Loss: 0.1264218477122713, Validation Loss: 0.2483306849034943\n",
      "Epoch 1381/3000, Training Loss: 0.12633274987945947, Validation Loss: 0.24827186363554998\n",
      "Epoch 1382/3000, Training Loss: 0.12624407155432174, Validation Loss: 0.2482187475534619\n",
      "Epoch 1383/3000, Training Loss: 0.1261551005164655, Validation Loss: 0.24815620918640424\n",
      "Epoch 1384/3000, Training Loss: 0.1260664940652232, Validation Loss: 0.24809667385908432\n",
      "Epoch 1385/3000, Training Loss: 0.12597830864652731, Validation Loss: 0.2480360541747099\n",
      "Epoch 1386/3000, Training Loss: 0.12588939115775172, Validation Loss: 0.2479854371653728\n",
      "Epoch 1387/3000, Training Loss: 0.12580142112941292, Validation Loss: 0.247929765492046\n",
      "Epoch 1388/3000, Training Loss: 0.1257130128064482, Validation Loss: 0.24787032604977993\n",
      "Epoch 1389/3000, Training Loss: 0.12562499261595642, Validation Loss: 0.24781505401865106\n",
      "Epoch 1390/3000, Training Loss: 0.1255370461256933, Validation Loss: 0.24775929112516054\n",
      "Epoch 1391/3000, Training Loss: 0.1254490184630469, Validation Loss: 0.2477019600695905\n",
      "Epoch 1392/3000, Training Loss: 0.1253612554049089, Validation Loss: 0.24764127190621996\n",
      "Epoch 1393/3000, Training Loss: 0.12527352245355708, Validation Loss: 0.24758472592769773\n",
      "Epoch 1394/3000, Training Loss: 0.1251861829266307, Validation Loss: 0.24753059227359184\n",
      "Epoch 1395/3000, Training Loss: 0.12509813257031038, Validation Loss: 0.24747212573205948\n",
      "Epoch 1396/3000, Training Loss: 0.1250114115826827, Validation Loss: 0.24741748109512252\n",
      "Epoch 1397/3000, Training Loss: 0.12492367168123092, Validation Loss: 0.2473608963256331\n",
      "Epoch 1398/3000, Training Loss: 0.12483638826594458, Validation Loss: 0.24730844550534378\n",
      "Epoch 1399/3000, Training Loss: 0.12474960330229913, Validation Loss: 0.24725563563528027\n",
      "Epoch 1400/3000, Training Loss: 0.12466220873642146, Validation Loss: 0.24718779618380882\n",
      "Epoch 1401/3000, Training Loss: 0.12457554115357462, Validation Loss: 0.24713697014477665\n",
      "Epoch 1402/3000, Training Loss: 0.12448853925260328, Validation Loss: 0.2470799849924105\n",
      "Epoch 1403/3000, Training Loss: 0.12440194114149813, Validation Loss: 0.2470260664772286\n",
      "Epoch 1404/3000, Training Loss: 0.12431524797736046, Validation Loss: 0.24696790720802786\n",
      "Epoch 1405/3000, Training Loss: 0.12422859048657094, Validation Loss: 0.24691462509671672\n",
      "Epoch 1406/3000, Training Loss: 0.12414238510664614, Validation Loss: 0.24686119019982913\n",
      "Epoch 1407/3000, Training Loss: 0.1240557744025007, Validation Loss: 0.24680820770356968\n",
      "Epoch 1408/3000, Training Loss: 0.12396994013205183, Validation Loss: 0.24675137482583115\n",
      "Epoch 1409/3000, Training Loss: 0.12388368017870667, Validation Loss: 0.24668867209140394\n",
      "Epoch 1410/3000, Training Loss: 0.12379773760460415, Validation Loss: 0.24664179606969253\n",
      "Epoch 1411/3000, Training Loss: 0.1237121303834046, Validation Loss: 0.2465810754102216\n",
      "Epoch 1412/3000, Training Loss: 0.12362597136334319, Validation Loss: 0.2465291054791371\n",
      "Epoch 1413/3000, Training Loss: 0.12354061194561629, Validation Loss: 0.24647992703045288\n",
      "Epoch 1414/3000, Training Loss: 0.12345474069168225, Validation Loss: 0.24642080090104448\n",
      "Epoch 1415/3000, Training Loss: 0.12336939615844536, Validation Loss: 0.24637176781166387\n",
      "Epoch 1416/3000, Training Loss: 0.12328375271283315, Validation Loss: 0.24631644804730984\n",
      "Epoch 1417/3000, Training Loss: 0.12319852298402657, Validation Loss: 0.24626480807662945\n",
      "Epoch 1418/3000, Training Loss: 0.12311311566716, Validation Loss: 0.2462080760060281\n",
      "Epoch 1419/3000, Training Loss: 0.12302798453641632, Validation Loss: 0.24614965277965672\n",
      "Epoch 1420/3000, Training Loss: 0.12294279542990443, Validation Loss: 0.24610424750662363\n",
      "Epoch 1421/3000, Training Loss: 0.12285790978646917, Validation Loss: 0.24604101238239373\n",
      "Epoch 1422/3000, Training Loss: 0.1227728401594079, Validation Loss: 0.24599200306354818\n",
      "Epoch 1423/3000, Training Loss: 0.12268796626571352, Validation Loss: 0.2459360948899704\n",
      "Epoch 1424/3000, Training Loss: 0.1226032592612266, Validation Loss: 0.2458863224348317\n",
      "Epoch 1425/3000, Training Loss: 0.12251829598709484, Validation Loss: 0.24583779276473827\n",
      "Epoch 1426/3000, Training Loss: 0.12243396476683815, Validation Loss: 0.2457773109565513\n",
      "Epoch 1427/3000, Training Loss: 0.12234889970455967, Validation Loss: 0.2457226496501079\n",
      "Epoch 1428/3000, Training Loss: 0.12226522359023807, Validation Loss: 0.24566609864907085\n",
      "Epoch 1429/3000, Training Loss: 0.12218035908168239, Validation Loss: 0.24561775814601422\n",
      "Epoch 1430/3000, Training Loss: 0.12209621662005547, Validation Loss: 0.2455621940610048\n",
      "Epoch 1431/3000, Training Loss: 0.1220120578130863, Validation Loss: 0.2455074846020124\n",
      "Epoch 1432/3000, Training Loss: 0.12192790413363742, Validation Loss: 0.24545570518012597\n",
      "Epoch 1433/3000, Training Loss: 0.12184412236934813, Validation Loss: 0.24540516133145956\n",
      "Epoch 1434/3000, Training Loss: 0.12175988426671455, Validation Loss: 0.2453495818130902\n",
      "Epoch 1435/3000, Training Loss: 0.12167669220853915, Validation Loss: 0.2452978621562849\n",
      "Epoch 1436/3000, Training Loss: 0.12159229333666348, Validation Loss: 0.24525116340783595\n",
      "Epoch 1437/3000, Training Loss: 0.12150938357005206, Validation Loss: 0.24518964121604622\n",
      "Epoch 1438/3000, Training Loss: 0.12142541840836218, Validation Loss: 0.2451406146812383\n",
      "Epoch 1439/3000, Training Loss: 0.12134240000558004, Validation Loss: 0.2450852095806975\n",
      "Epoch 1440/3000, Training Loss: 0.12125887110457362, Validation Loss: 0.2450338836574111\n",
      "Epoch 1441/3000, Training Loss: 0.12117542257666153, Validation Loss: 0.244979749199047\n",
      "Epoch 1442/3000, Training Loss: 0.12109255416059576, Validation Loss: 0.24493381503724937\n",
      "Epoch 1443/3000, Training Loss: 0.12100918880245094, Validation Loss: 0.244876311017816\n",
      "Epoch 1444/3000, Training Loss: 0.12092645562400871, Validation Loss: 0.24482746591145169\n",
      "Epoch 1445/3000, Training Loss: 0.12084331891384777, Validation Loss: 0.24477088257194635\n",
      "Epoch 1446/3000, Training Loss: 0.12076084793985253, Validation Loss: 0.24472298249843422\n",
      "Epoch 1447/3000, Training Loss: 0.12067780717726531, Validation Loss: 0.24466815298738134\n",
      "Epoch 1448/3000, Training Loss: 0.12059564180577002, Validation Loss: 0.24461358804362218\n",
      "Epoch 1449/3000, Training Loss: 0.12051282293015843, Validation Loss: 0.2445653149291071\n",
      "Epoch 1450/3000, Training Loss: 0.12043034572785544, Validation Loss: 0.24451302611251957\n",
      "Epoch 1451/3000, Training Loss: 0.12034789152548514, Validation Loss: 0.24446485196220427\n",
      "Epoch 1452/3000, Training Loss: 0.12026589536449435, Validation Loss: 0.2444078568653709\n",
      "Epoch 1453/3000, Training Loss: 0.12018373803176269, Validation Loss: 0.2443599621745771\n",
      "Epoch 1454/3000, Training Loss: 0.1201018489982651, Validation Loss: 0.24430190387098777\n",
      "Epoch 1455/3000, Training Loss: 0.12002011298449988, Validation Loss: 0.24425148768497235\n",
      "Epoch 1456/3000, Training Loss: 0.11993818242410463, Validation Loss: 0.2442022018277725\n",
      "Epoch 1457/3000, Training Loss: 0.1198568772808026, Validation Loss: 0.2441474195569777\n",
      "Epoch 1458/3000, Training Loss: 0.11977471518496902, Validation Loss: 0.2440947548239375\n",
      "Epoch 1459/3000, Training Loss: 0.1196939845675418, Validation Loss: 0.24403816618144356\n",
      "Epoch 1460/3000, Training Loss: 0.11961194405583363, Validation Loss: 0.24399014408609587\n",
      "Epoch 1461/3000, Training Loss: 0.11953086625824283, Validation Loss: 0.24393768356563375\n",
      "Epoch 1462/3000, Training Loss: 0.11944961237314063, Validation Loss: 0.24388754689622094\n",
      "Epoch 1463/3000, Training Loss: 0.11936817859353875, Validation Loss: 0.24383518878704954\n",
      "Epoch 1464/3000, Training Loss: 0.11928749112093931, Validation Loss: 0.24378379607474562\n",
      "Epoch 1465/3000, Training Loss: 0.11920604755239483, Validation Loss: 0.24372916686563476\n",
      "Epoch 1466/3000, Training Loss: 0.1191253994317042, Validation Loss: 0.2436846299488631\n",
      "Epoch 1467/3000, Training Loss: 0.11904416471840132, Validation Loss: 0.24362453467259457\n",
      "Epoch 1468/3000, Training Loss: 0.11896368966473032, Validation Loss: 0.24357718075629256\n",
      "Epoch 1469/3000, Training Loss: 0.11888278332825193, Validation Loss: 0.24352246043279888\n",
      "Epoch 1470/3000, Training Loss: 0.11880229533584964, Validation Loss: 0.24346663762082454\n",
      "Epoch 1471/3000, Training Loss: 0.11872144109919029, Validation Loss: 0.24342418276019884\n",
      "Epoch 1472/3000, Training Loss: 0.118641415042225, Validation Loss: 0.24336812013780162\n",
      "Epoch 1473/3000, Training Loss: 0.11856043998282975, Validation Loss: 0.24331538526264618\n",
      "Epoch 1474/3000, Training Loss: 0.11848067271308048, Validation Loss: 0.24326094052446787\n",
      "Epoch 1475/3000, Training Loss: 0.11840005412055638, Validation Loss: 0.24321271322216428\n",
      "Epoch 1476/3000, Training Loss: 0.11831999237265482, Validation Loss: 0.2431625824912732\n",
      "Epoch 1477/3000, Training Loss: 0.1182400170886131, Validation Loss: 0.24311288932220845\n",
      "Epoch 1478/3000, Training Loss: 0.11815942928857187, Validation Loss: 0.24306020710466136\n",
      "Epoch 1479/3000, Training Loss: 0.11807997205251645, Validation Loss: 0.24301211761903968\n",
      "Epoch 1480/3000, Training Loss: 0.11799969753406239, Validation Loss: 0.24295367665206194\n",
      "Epoch 1481/3000, Training Loss: 0.1179203726221941, Validation Loss: 0.24290512852531002\n",
      "Epoch 1482/3000, Training Loss: 0.11784032131338414, Validation Loss: 0.2428575307921358\n",
      "Epoch 1483/3000, Training Loss: 0.11776090991637594, Validation Loss: 0.24280159923554073\n",
      "Epoch 1484/3000, Training Loss: 0.11768120462354972, Validation Loss: 0.2427572874217254\n",
      "Epoch 1485/3000, Training Loss: 0.11760188878626399, Validation Loss: 0.24270119900292864\n",
      "Epoch 1486/3000, Training Loss: 0.11752236557516482, Validation Loss: 0.2426538244202449\n",
      "Epoch 1487/3000, Training Loss: 0.11744321133534978, Validation Loss: 0.24259718835140368\n",
      "Epoch 1488/3000, Training Loss: 0.11736386234068771, Validation Loss: 0.24255387564086506\n",
      "Epoch 1489/3000, Training Loss: 0.1172847700422136, Validation Loss: 0.24249993683650675\n",
      "Epoch 1490/3000, Training Loss: 0.11720566936018374, Validation Loss: 0.242453603062475\n",
      "Epoch 1491/3000, Training Loss: 0.11712665940039892, Validation Loss: 0.24240636256801903\n",
      "Epoch 1492/3000, Training Loss: 0.11704797953282171, Validation Loss: 0.2423542412248811\n",
      "Epoch 1493/3000, Training Loss: 0.1169688355756547, Validation Loss: 0.24229964417730945\n",
      "Epoch 1494/3000, Training Loss: 0.11689051189910876, Validation Loss: 0.24224589295485713\n",
      "Epoch 1495/3000, Training Loss: 0.11681167006384413, Validation Loss: 0.24220205358607808\n",
      "Epoch 1496/3000, Training Loss: 0.11673327775640249, Validation Loss: 0.24215024489812775\n",
      "Epoch 1497/3000, Training Loss: 0.11665474153655879, Validation Loss: 0.24210556275781067\n",
      "Epoch 1498/3000, Training Loss: 0.11657640795585622, Validation Loss: 0.24205094184097303\n",
      "Epoch 1499/3000, Training Loss: 0.11649816113654317, Validation Loss: 0.24200405001908332\n",
      "Epoch 1500/3000, Training Loss: 0.11641986069364094, Validation Loss: 0.2419633139477337\n",
      "Epoch 1501/3000, Training Loss: 0.11634183495713012, Validation Loss: 0.24190817527175607\n",
      "Epoch 1502/3000, Training Loss: 0.11626380638061437, Validation Loss: 0.24186185107598757\n",
      "Epoch 1503/3000, Training Loss: 0.11618579720324075, Validation Loss: 0.2418084740804705\n",
      "Epoch 1504/3000, Training Loss: 0.1161080499828456, Validation Loss: 0.2417643339461577\n",
      "Epoch 1505/3000, Training Loss: 0.11603028305692528, Validation Loss: 0.24171226880329247\n",
      "Epoch 1506/3000, Training Loss: 0.11595254956775945, Validation Loss: 0.24166407050631236\n",
      "Epoch 1507/3000, Training Loss: 0.11587513014078568, Validation Loss: 0.24161325375103468\n",
      "Epoch 1508/3000, Training Loss: 0.1157976302021629, Validation Loss: 0.2415652194950456\n",
      "Epoch 1509/3000, Training Loss: 0.11572029705418367, Validation Loss: 0.24151309335498444\n",
      "Epoch 1510/3000, Training Loss: 0.11564281206385585, Validation Loss: 0.24147027015742614\n",
      "Epoch 1511/3000, Training Loss: 0.11556590648002733, Validation Loss: 0.24142350102605462\n",
      "Epoch 1512/3000, Training Loss: 0.11548848913214112, Validation Loss: 0.2413747827839731\n",
      "Epoch 1513/3000, Training Loss: 0.11541132085006663, Validation Loss: 0.24133092016213067\n",
      "Epoch 1514/3000, Training Loss: 0.11533442505833674, Validation Loss: 0.2412747266326668\n",
      "Epoch 1515/3000, Training Loss: 0.11525745118658995, Validation Loss: 0.241226938724476\n",
      "Epoch 1516/3000, Training Loss: 0.11518060097437986, Validation Loss: 0.24117510706786646\n",
      "Epoch 1517/3000, Training Loss: 0.11510393372583703, Validation Loss: 0.24113042334860543\n",
      "Epoch 1518/3000, Training Loss: 0.11502710199291742, Validation Loss: 0.24107966666773525\n",
      "Epoch 1519/3000, Training Loss: 0.1149504624163973, Validation Loss: 0.24103873230632664\n",
      "Epoch 1520/3000, Training Loss: 0.11487406823756156, Validation Loss: 0.24099336903613705\n",
      "Epoch 1521/3000, Training Loss: 0.11479758584972626, Validation Loss: 0.24094272930321886\n",
      "Epoch 1522/3000, Training Loss: 0.11472145369194808, Validation Loss: 0.24089251368346543\n",
      "Epoch 1523/3000, Training Loss: 0.1146452500599245, Validation Loss: 0.24083935211226296\n",
      "Epoch 1524/3000, Training Loss: 0.11456930513771038, Validation Loss: 0.24079160093428487\n",
      "Epoch 1525/3000, Training Loss: 0.11449320534044861, Validation Loss: 0.2407394600626445\n",
      "Epoch 1526/3000, Training Loss: 0.11441728252471986, Validation Loss: 0.2407014481176181\n",
      "Epoch 1527/3000, Training Loss: 0.1143413019450838, Validation Loss: 0.24064784861572516\n",
      "Epoch 1528/3000, Training Loss: 0.11426567131602258, Validation Loss: 0.24060402040198356\n",
      "Epoch 1529/3000, Training Loss: 0.11418972012417371, Validation Loss: 0.24055612108162122\n",
      "Epoch 1530/3000, Training Loss: 0.11411436461042176, Validation Loss: 0.24050465893253975\n",
      "Epoch 1531/3000, Training Loss: 0.11403856874978215, Validation Loss: 0.24045962987296987\n",
      "Epoch 1532/3000, Training Loss: 0.11396332091968953, Validation Loss: 0.2404059885107503\n",
      "Epoch 1533/3000, Training Loss: 0.11388777030964414, Validation Loss: 0.24036038132587093\n",
      "Epoch 1534/3000, Training Loss: 0.11381264489285764, Validation Loss: 0.24031313046267105\n",
      "Epoch 1535/3000, Training Loss: 0.11373713606212488, Validation Loss: 0.24026679193352066\n",
      "Epoch 1536/3000, Training Loss: 0.1136621069901206, Validation Loss: 0.24021660875399867\n",
      "Epoch 1537/3000, Training Loss: 0.1135869811555278, Validation Loss: 0.24016948427101759\n",
      "Epoch 1538/3000, Training Loss: 0.11351184071583033, Validation Loss: 0.2401191392363849\n",
      "Epoch 1539/3000, Training Loss: 0.11343703508063765, Validation Loss: 0.2400760522206228\n",
      "Epoch 1540/3000, Training Loss: 0.11336219118973136, Validation Loss: 0.2400323450266065\n",
      "Epoch 1541/3000, Training Loss: 0.11328719541513955, Validation Loss: 0.2399767052363024\n",
      "Epoch 1542/3000, Training Loss: 0.11321271152675551, Validation Loss: 0.23993575580062584\n",
      "Epoch 1543/3000, Training Loss: 0.11313788482481539, Validation Loss: 0.23988517179504287\n",
      "Epoch 1544/3000, Training Loss: 0.1130634143192508, Validation Loss: 0.2398389359824452\n",
      "Epoch 1545/3000, Training Loss: 0.11298898619591595, Validation Loss: 0.23978823540929506\n",
      "Epoch 1546/3000, Training Loss: 0.11291456978259407, Validation Loss: 0.23974572443121103\n",
      "Epoch 1547/3000, Training Loss: 0.11284010629686822, Validation Loss: 0.23969526366922403\n",
      "Epoch 1548/3000, Training Loss: 0.11276605440773227, Validation Loss: 0.23965034016938103\n",
      "Epoch 1549/3000, Training Loss: 0.11269151938971922, Validation Loss: 0.2396089913991713\n",
      "Epoch 1550/3000, Training Loss: 0.1126178146782985, Validation Loss: 0.23955195324149295\n",
      "Epoch 1551/3000, Training Loss: 0.11254343033064403, Validation Loss: 0.23951302505371785\n",
      "Epoch 1552/3000, Training Loss: 0.11246975383427463, Validation Loss: 0.23946063813270352\n",
      "Epoch 1553/3000, Training Loss: 0.11239583436205246, Validation Loss: 0.23941471002360826\n",
      "Epoch 1554/3000, Training Loss: 0.11232190916283354, Validation Loss: 0.2393684584236444\n",
      "Epoch 1555/3000, Training Loss: 0.11224817432651786, Validation Loss: 0.23932326442178095\n",
      "Epoch 1556/3000, Training Loss: 0.11217448270506628, Validation Loss: 0.2392758336981879\n",
      "Epoch 1557/3000, Training Loss: 0.11210072586413981, Validation Loss: 0.23922911852457326\n",
      "Epoch 1558/3000, Training Loss: 0.11202721247897629, Validation Loss: 0.23918352495493167\n",
      "Epoch 1559/3000, Training Loss: 0.11195376068288501, Validation Loss: 0.2391340619682971\n",
      "Epoch 1560/3000, Training Loss: 0.11188056863179692, Validation Loss: 0.23909117336853863\n",
      "Epoch 1561/3000, Training Loss: 0.11180702733119399, Validation Loss: 0.2390438421928053\n",
      "Epoch 1562/3000, Training Loss: 0.11173394298279693, Validation Loss: 0.23900042717850983\n",
      "Epoch 1563/3000, Training Loss: 0.11166056274013782, Validation Loss: 0.23894993949323737\n",
      "Epoch 1564/3000, Training Loss: 0.11158761256007348, Validation Loss: 0.23891107122267974\n",
      "Epoch 1565/3000, Training Loss: 0.11151457093888353, Validation Loss: 0.2388601786716023\n",
      "Epoch 1566/3000, Training Loss: 0.1114414774281232, Validation Loss: 0.2388181443310203\n",
      "Epoch 1567/3000, Training Loss: 0.1113686824022939, Validation Loss: 0.2387674893740194\n",
      "Epoch 1568/3000, Training Loss: 0.11129613127982994, Validation Loss: 0.23872778037664324\n",
      "Epoch 1569/3000, Training Loss: 0.1112231099342773, Validation Loss: 0.2386803806594273\n",
      "Epoch 1570/3000, Training Loss: 0.11115078062064868, Validation Loss: 0.23863335625243692\n",
      "Epoch 1571/3000, Training Loss: 0.11107808462174493, Validation Loss: 0.23859159221872026\n",
      "Epoch 1572/3000, Training Loss: 0.11100545930129324, Validation Loss: 0.23854294298614265\n",
      "Epoch 1573/3000, Training Loss: 0.11093327548450847, Validation Loss: 0.23850068922835257\n",
      "Epoch 1574/3000, Training Loss: 0.11086080388989049, Validation Loss: 0.23845064453643056\n",
      "Epoch 1575/3000, Training Loss: 0.110788603019882, Validation Loss: 0.23840824673096092\n",
      "Epoch 1576/3000, Training Loss: 0.11071632945754095, Validation Loss: 0.23836664060130117\n",
      "Epoch 1577/3000, Training Loss: 0.11064436850235196, Validation Loss: 0.23832204242854355\n",
      "Epoch 1578/3000, Training Loss: 0.11057214620291214, Validation Loss: 0.23828165703424145\n",
      "Epoch 1579/3000, Training Loss: 0.11050050375394796, Validation Loss: 0.23823438638663558\n",
      "Epoch 1580/3000, Training Loss: 0.11042840420099687, Validation Loss: 0.23818595033818785\n",
      "Epoch 1581/3000, Training Loss: 0.11035677184312205, Validation Loss: 0.23814010121689294\n",
      "Epoch 1582/3000, Training Loss: 0.1102847648672727, Validation Loss: 0.2380940660418003\n",
      "Epoch 1583/3000, Training Loss: 0.11021328040731408, Validation Loss: 0.23804667295505463\n",
      "Epoch 1584/3000, Training Loss: 0.11014150070018104, Validation Loss: 0.23800682392786948\n",
      "Epoch 1585/3000, Training Loss: 0.11007009633541753, Validation Loss: 0.23795790432361622\n",
      "Epoch 1586/3000, Training Loss: 0.10999844955763698, Validation Loss: 0.23791904611919654\n",
      "Epoch 1587/3000, Training Loss: 0.10992722210995032, Validation Loss: 0.23787567965310707\n",
      "Epoch 1588/3000, Training Loss: 0.10985578539970563, Validation Loss: 0.23782858604016777\n",
      "Epoch 1589/3000, Training Loss: 0.1097846180985554, Validation Loss: 0.23779285316708698\n",
      "Epoch 1590/3000, Training Loss: 0.10971343698056113, Validation Loss: 0.23773647636236037\n",
      "Epoch 1591/3000, Training Loss: 0.1096423567303245, Validation Loss: 0.23769702241187796\n",
      "Epoch 1592/3000, Training Loss: 0.10957118280749358, Validation Loss: 0.23765011972319308\n",
      "Epoch 1593/3000, Training Loss: 0.1095004958201108, Validation Loss: 0.23760866307954495\n",
      "Epoch 1594/3000, Training Loss: 0.10942932152653151, Validation Loss: 0.23756201019278653\n",
      "Epoch 1595/3000, Training Loss: 0.10935873781422788, Validation Loss: 0.23752304556373677\n",
      "Epoch 1596/3000, Training Loss: 0.10928778305882335, Validation Loss: 0.2374820582811153\n",
      "Epoch 1597/3000, Training Loss: 0.10921740219852524, Validation Loss: 0.23743331338321233\n",
      "Epoch 1598/3000, Training Loss: 0.10914641701855828, Validation Loss: 0.237395309158864\n",
      "Epoch 1599/3000, Training Loss: 0.1090763002107764, Validation Loss: 0.2373436663365081\n",
      "Epoch 1600/3000, Training Loss: 0.10900550686744094, Validation Loss: 0.2373082500663722\n",
      "Epoch 1601/3000, Training Loss: 0.10893524374504521, Validation Loss: 0.23725794794698252\n",
      "Epoch 1602/3000, Training Loss: 0.10886488587842703, Validation Loss: 0.2372163268467456\n",
      "Epoch 1603/3000, Training Loss: 0.10879470096140129, Validation Loss: 0.23716944134600182\n",
      "Epoch 1604/3000, Training Loss: 0.10872435891429656, Validation Loss: 0.23713398172015998\n",
      "Epoch 1605/3000, Training Loss: 0.10865457941402364, Validation Loss: 0.23709025234860417\n",
      "Epoch 1606/3000, Training Loss: 0.10858394913008482, Validation Loss: 0.2370468556032829\n",
      "Epoch 1607/3000, Training Loss: 0.10851432028342634, Validation Loss: 0.23700398210089288\n",
      "Epoch 1608/3000, Training Loss: 0.10844396077060917, Validation Loss: 0.23695805691890068\n",
      "Epoch 1609/3000, Training Loss: 0.10837431935364401, Validation Loss: 0.23691775657031053\n",
      "Epoch 1610/3000, Training Loss: 0.10830409994162994, Validation Loss: 0.23687308673908875\n",
      "Epoch 1611/3000, Training Loss: 0.10823480865715766, Validation Loss: 0.2368289108669861\n",
      "Epoch 1612/3000, Training Loss: 0.10816473415093858, Validation Loss: 0.23678282682377821\n",
      "Epoch 1613/3000, Training Loss: 0.10809544520166776, Validation Loss: 0.23674284620038483\n",
      "Epoch 1614/3000, Training Loss: 0.10802552735938425, Validation Loss: 0.23670522629697405\n",
      "Epoch 1615/3000, Training Loss: 0.10795645367973103, Validation Loss: 0.23666237032409254\n",
      "Epoch 1616/3000, Training Loss: 0.10788667377594727, Validation Loss: 0.23661912124886994\n",
      "Epoch 1617/3000, Training Loss: 0.10781776923105432, Validation Loss: 0.23657637260481315\n",
      "Epoch 1618/3000, Training Loss: 0.10774813997014614, Validation Loss: 0.23653297778027343\n",
      "Epoch 1619/3000, Training Loss: 0.10767925035389271, Validation Loss: 0.2364896421740196\n",
      "Epoch 1620/3000, Training Loss: 0.10761000468571885, Validation Loss: 0.23645109113131016\n",
      "Epoch 1621/3000, Training Loss: 0.10754095941532439, Validation Loss: 0.23640580706649997\n",
      "Epoch 1622/3000, Training Loss: 0.10747184446705402, Validation Loss: 0.23636574994391307\n",
      "Epoch 1623/3000, Training Loss: 0.10740296527949718, Validation Loss: 0.23632045676566027\n",
      "Epoch 1624/3000, Training Loss: 0.1073342067317244, Validation Loss: 0.23627784118941503\n",
      "Epoch 1625/3000, Training Loss: 0.10726522705685212, Validation Loss: 0.2362351439723681\n",
      "Epoch 1626/3000, Training Loss: 0.10719643891083533, Validation Loss: 0.23619214546632775\n",
      "Epoch 1627/3000, Training Loss: 0.10712780944461181, Validation Loss: 0.23614981676723226\n",
      "Epoch 1628/3000, Training Loss: 0.1070589751401833, Validation Loss: 0.23610532411055915\n",
      "Epoch 1629/3000, Training Loss: 0.10699049433732605, Validation Loss: 0.23607054534640545\n",
      "Epoch 1630/3000, Training Loss: 0.1069218076818193, Validation Loss: 0.23602545517387744\n",
      "Epoch 1631/3000, Training Loss: 0.10685312949460904, Validation Loss: 0.23598899271732626\n",
      "Epoch 1632/3000, Training Loss: 0.10678482743451097, Validation Loss: 0.23594809616772466\n",
      "Epoch 1633/3000, Training Loss: 0.10671610331598659, Validation Loss: 0.23590784617332597\n",
      "Epoch 1634/3000, Training Loss: 0.10664799460938743, Validation Loss: 0.23585978537554675\n",
      "Epoch 1635/3000, Training Loss: 0.10657925283902984, Validation Loss: 0.2358138974686622\n",
      "Epoch 1636/3000, Training Loss: 0.10651132418788066, Validation Loss: 0.23577708193074487\n",
      "Epoch 1637/3000, Training Loss: 0.10644287077106591, Validation Loss: 0.2357335298630588\n",
      "Epoch 1638/3000, Training Loss: 0.10637468496860869, Validation Loss: 0.2356961891054482\n",
      "Epoch 1639/3000, Training Loss: 0.10630667400173827, Validation Loss: 0.23565701955371177\n",
      "Epoch 1640/3000, Training Loss: 0.10623844288971904, Validation Loss: 0.23561031746832975\n",
      "Epoch 1641/3000, Training Loss: 0.10617056199853728, Validation Loss: 0.23557587000709893\n",
      "Epoch 1642/3000, Training Loss: 0.10610257412295716, Validation Loss: 0.23553266739617035\n",
      "Epoch 1643/3000, Training Loss: 0.10603471981870756, Validation Loss: 0.23549434289562107\n",
      "Epoch 1644/3000, Training Loss: 0.10596685493198373, Validation Loss: 0.23544667010308298\n",
      "Epoch 1645/3000, Training Loss: 0.10589925491629987, Validation Loss: 0.23540171158550152\n",
      "Epoch 1646/3000, Training Loss: 0.10583164659298834, Validation Loss: 0.23536106568881934\n",
      "Epoch 1647/3000, Training Loss: 0.10576399726709354, Validation Loss: 0.2353218028491263\n",
      "Epoch 1648/3000, Training Loss: 0.10569640743332484, Validation Loss: 0.23528515797159663\n",
      "Epoch 1649/3000, Training Loss: 0.10562901920550757, Validation Loss: 0.23524078864957107\n",
      "Epoch 1650/3000, Training Loss: 0.10556160775628444, Validation Loss: 0.2352027602712083\n",
      "Epoch 1651/3000, Training Loss: 0.10549409236482457, Validation Loss: 0.23516279819344837\n",
      "Epoch 1652/3000, Training Loss: 0.10542717018644328, Validation Loss: 0.2351225224579649\n",
      "Epoch 1653/3000, Training Loss: 0.1053595036001621, Validation Loss: 0.23507975342472626\n",
      "Epoch 1654/3000, Training Loss: 0.10529277967831534, Validation Loss: 0.23504197579906555\n",
      "Epoch 1655/3000, Training Loss: 0.10522541285256481, Validation Loss: 0.2349958470124064\n",
      "Epoch 1656/3000, Training Loss: 0.10515869466011252, Validation Loss: 0.23496348454069182\n",
      "Epoch 1657/3000, Training Loss: 0.10509154055780767, Validation Loss: 0.23491732889511877\n",
      "Epoch 1658/3000, Training Loss: 0.10502453682300764, Validation Loss: 0.23487897880811476\n",
      "Epoch 1659/3000, Training Loss: 0.10495808933931366, Validation Loss: 0.23483922713198288\n",
      "Epoch 1660/3000, Training Loss: 0.10489082661784192, Validation Loss: 0.2347982706720257\n",
      "Epoch 1661/3000, Training Loss: 0.10482456822626215, Validation Loss: 0.23476083244027557\n",
      "Epoch 1662/3000, Training Loss: 0.10475763233179179, Validation Loss: 0.23471604867483187\n",
      "Epoch 1663/3000, Training Loss: 0.10469117200914244, Validation Loss: 0.2346816275122757\n",
      "Epoch 1664/3000, Training Loss: 0.10462472607200327, Validation Loss: 0.23464179370233396\n",
      "Epoch 1665/3000, Training Loss: 0.10455821778577, Validation Loss: 0.23459912940330768\n",
      "Epoch 1666/3000, Training Loss: 0.10449192767305969, Validation Loss: 0.2345596924830381\n",
      "Epoch 1667/3000, Training Loss: 0.10442529967598045, Validation Loss: 0.23451890222023408\n",
      "Epoch 1668/3000, Training Loss: 0.10435938391193801, Validation Loss: 0.23448370018350748\n",
      "Epoch 1669/3000, Training Loss: 0.10429299635351501, Validation Loss: 0.23443327630529354\n",
      "Epoch 1670/3000, Training Loss: 0.10422705655364094, Validation Loss: 0.23439848859608015\n",
      "Epoch 1671/3000, Training Loss: 0.1041608859669502, Validation Loss: 0.23435487566864746\n",
      "Epoch 1672/3000, Training Loss: 0.1040948273886242, Validation Loss: 0.23431652299055483\n",
      "Epoch 1673/3000, Training Loss: 0.10402899460640773, Validation Loss: 0.23428343549356573\n",
      "Epoch 1674/3000, Training Loss: 0.10396310534376495, Validation Loss: 0.23423959080863624\n",
      "Epoch 1675/3000, Training Loss: 0.10389725364814108, Validation Loss: 0.2341980028955584\n",
      "Epoch 1676/3000, Training Loss: 0.10383141743758181, Validation Loss: 0.23415822241797346\n",
      "Epoch 1677/3000, Training Loss: 0.10376606728225812, Validation Loss: 0.23412271782979707\n",
      "Epoch 1678/3000, Training Loss: 0.10370020751644964, Validation Loss: 0.23407892506293304\n",
      "Epoch 1679/3000, Training Loss: 0.10363495373056089, Validation Loss: 0.23404313132606575\n",
      "Epoch 1680/3000, Training Loss: 0.10356913701491516, Validation Loss: 0.23399613580354015\n",
      "Epoch 1681/3000, Training Loss: 0.10350414629387929, Validation Loss: 0.2339576362998318\n",
      "Epoch 1682/3000, Training Loss: 0.10343848446898393, Validation Loss: 0.23391820984146167\n",
      "Epoch 1683/3000, Training Loss: 0.10337347195348896, Validation Loss: 0.23388289712106403\n",
      "Epoch 1684/3000, Training Loss: 0.10330820769075424, Validation Loss: 0.2338434642088375\n",
      "Epoch 1685/3000, Training Loss: 0.10324303874448823, Validation Loss: 0.23380378176651684\n",
      "Epoch 1686/3000, Training Loss: 0.10317819735293424, Validation Loss: 0.23376965210932887\n",
      "Epoch 1687/3000, Training Loss: 0.10311304532690323, Validation Loss: 0.23372749017231517\n",
      "Epoch 1688/3000, Training Loss: 0.10304854221550486, Validation Loss: 0.2336902993582319\n",
      "Epoch 1689/3000, Training Loss: 0.10298344897117245, Validation Loss: 0.2336498462729463\n",
      "Epoch 1690/3000, Training Loss: 0.10291912726613689, Validation Loss: 0.23361551403449632\n",
      "Epoch 1691/3000, Training Loss: 0.10285447686114313, Validation Loss: 0.23357350564318363\n",
      "Epoch 1692/3000, Training Loss: 0.10279000155704171, Validation Loss: 0.23354016867334007\n",
      "Epoch 1693/3000, Training Loss: 0.10272563484990987, Validation Loss: 0.23349128096753183\n",
      "Epoch 1694/3000, Training Loss: 0.1026610994662721, Validation Loss: 0.2334594365373873\n",
      "Epoch 1695/3000, Training Loss: 0.10259714416588779, Validation Loss: 0.23341683920162054\n",
      "Epoch 1696/3000, Training Loss: 0.10253251442647723, Validation Loss: 0.23337848990475035\n",
      "Epoch 1697/3000, Training Loss: 0.10246862993445029, Validation Loss: 0.23334596760747633\n",
      "Epoch 1698/3000, Training Loss: 0.1024042486423755, Validation Loss: 0.23330461578059095\n",
      "Epoch 1699/3000, Training Loss: 0.10234039555645898, Validation Loss: 0.23327163703691361\n",
      "Epoch 1700/3000, Training Loss: 0.10227617702310164, Validation Loss: 0.23322941572172953\n",
      "Epoch 1701/3000, Training Loss: 0.10221199802589877, Validation Loss: 0.23319690839366652\n",
      "Epoch 1702/3000, Training Loss: 0.10214853473177161, Validation Loss: 0.23315552111859164\n",
      "Epoch 1703/3000, Training Loss: 0.10208427189523285, Validation Loss: 0.23311884380182946\n",
      "Epoch 1704/3000, Training Loss: 0.10202094828153117, Validation Loss: 0.23307203679606658\n",
      "Epoch 1705/3000, Training Loss: 0.10195688700048929, Validation Loss: 0.23303491113481392\n",
      "Epoch 1706/3000, Training Loss: 0.1018933713646123, Validation Loss: 0.23298969586626722\n",
      "Epoch 1707/3000, Training Loss: 0.1018297999614818, Validation Loss: 0.23295344486730274\n",
      "Epoch 1708/3000, Training Loss: 0.10176594852530639, Validation Loss: 0.23291351695601126\n",
      "Epoch 1709/3000, Training Loss: 0.10170274746458226, Validation Loss: 0.23287712132211044\n",
      "Epoch 1710/3000, Training Loss: 0.10163906902392822, Validation Loss: 0.23284124774355444\n",
      "Epoch 1711/3000, Training Loss: 0.10157577467762872, Validation Loss: 0.23279841086750017\n",
      "Epoch 1712/3000, Training Loss: 0.10151214893708442, Validation Loss: 0.23276364313520204\n",
      "Epoch 1713/3000, Training Loss: 0.10144917020927977, Validation Loss: 0.23272191815564744\n",
      "Epoch 1714/3000, Training Loss: 0.10138538724220714, Validation Loss: 0.23268324490856399\n",
      "Epoch 1715/3000, Training Loss: 0.10132229781847274, Validation Loss: 0.23264178551058537\n",
      "Epoch 1716/3000, Training Loss: 0.10125929767300854, Validation Loss: 0.23260689254575975\n",
      "Epoch 1717/3000, Training Loss: 0.10119575334178155, Validation Loss: 0.23256568853363765\n",
      "Epoch 1718/3000, Training Loss: 0.10113321599956347, Validation Loss: 0.23253018853152752\n",
      "Epoch 1719/3000, Training Loss: 0.10106950766359711, Validation Loss: 0.23248759660837104\n",
      "Epoch 1720/3000, Training Loss: 0.10100707891144428, Validation Loss: 0.232452833310507\n",
      "Epoch 1721/3000, Training Loss: 0.10094389827062268, Validation Loss: 0.23241241769323595\n",
      "Epoch 1722/3000, Training Loss: 0.10088118457983451, Validation Loss: 0.23236946836541306\n",
      "Epoch 1723/3000, Training Loss: 0.10081833096560085, Validation Loss: 0.23233419070133532\n",
      "Epoch 1724/3000, Training Loss: 0.10075572671545474, Validation Loss: 0.23229335560450443\n",
      "Epoch 1725/3000, Training Loss: 0.10069299161003735, Validation Loss: 0.23225789996009016\n",
      "Epoch 1726/3000, Training Loss: 0.10063027704020154, Validation Loss: 0.23221479239576298\n",
      "Epoch 1727/3000, Training Loss: 0.10056774209395067, Validation Loss: 0.23217972872710924\n",
      "Epoch 1728/3000, Training Loss: 0.10050512820692842, Validation Loss: 0.23214320465141505\n",
      "Epoch 1729/3000, Training Loss: 0.10044295143642878, Validation Loss: 0.2321046048241626\n",
      "Epoch 1730/3000, Training Loss: 0.10038026665482454, Validation Loss: 0.23206516025564874\n",
      "Epoch 1731/3000, Training Loss: 0.10031789951186554, Validation Loss: 0.2320318193705214\n",
      "Epoch 1732/3000, Training Loss: 0.10025595909850077, Validation Loss: 0.2319904960301597\n",
      "Epoch 1733/3000, Training Loss: 0.10019349076274675, Validation Loss: 0.2319564050551956\n",
      "Epoch 1734/3000, Training Loss: 0.10013129343581573, Validation Loss: 0.23191872191667895\n",
      "Epoch 1735/3000, Training Loss: 0.10006908178922354, Validation Loss: 0.23188013971964536\n",
      "Epoch 1736/3000, Training Loss: 0.10000701674763082, Validation Loss: 0.23184088300952305\n",
      "Epoch 1737/3000, Training Loss: 0.0999449076732565, Validation Loss: 0.23180493024974116\n",
      "Epoch 1738/3000, Training Loss: 0.09988290581251644, Validation Loss: 0.2317606665697614\n",
      "Epoch 1739/3000, Training Loss: 0.09982089139122055, Validation Loss: 0.2317280961656861\n",
      "Epoch 1740/3000, Training Loss: 0.09975895753150121, Validation Loss: 0.23168584800597106\n",
      "Epoch 1741/3000, Training Loss: 0.09969727568354685, Validation Loss: 0.23165377525680508\n",
      "Epoch 1742/3000, Training Loss: 0.09963510227632168, Validation Loss: 0.23161407004217055\n",
      "Epoch 1743/3000, Training Loss: 0.09957395309644562, Validation Loss: 0.23157951164577686\n",
      "Epoch 1744/3000, Training Loss: 0.09951182819225109, Validation Loss: 0.23153904628569177\n",
      "Epoch 1745/3000, Training Loss: 0.0994504256164289, Validation Loss: 0.23149857331380053\n",
      "Epoch 1746/3000, Training Loss: 0.09938872773115978, Validation Loss: 0.23146592822252027\n",
      "Epoch 1747/3000, Training Loss: 0.09932732381852767, Validation Loss: 0.23142948287465753\n",
      "Epoch 1748/3000, Training Loss: 0.09926585687382132, Validation Loss: 0.23139360672084253\n",
      "Epoch 1749/3000, Training Loss: 0.09920440680674197, Validation Loss: 0.23135635365759125\n",
      "Epoch 1750/3000, Training Loss: 0.09914313297652293, Validation Loss: 0.23132459206973574\n",
      "Epoch 1751/3000, Training Loss: 0.09908188174410157, Validation Loss: 0.23128562239946268\n",
      "Epoch 1752/3000, Training Loss: 0.09902077961518954, Validation Loss: 0.231252040088668\n",
      "Epoch 1753/3000, Training Loss: 0.09895926408370924, Validation Loss: 0.23121505536722903\n",
      "Epoch 1754/3000, Training Loss: 0.09889861864775608, Validation Loss: 0.23116708885481219\n",
      "Epoch 1755/3000, Training Loss: 0.09883744042903549, Validation Loss: 0.23113535922042155\n",
      "Epoch 1756/3000, Training Loss: 0.09877633055340568, Validation Loss: 0.23109473336994882\n",
      "Epoch 1757/3000, Training Loss: 0.0987153958860206, Validation Loss: 0.23106316096276705\n",
      "Epoch 1758/3000, Training Loss: 0.09865453368126698, Validation Loss: 0.2310284259488965\n",
      "Epoch 1759/3000, Training Loss: 0.09859370517347017, Validation Loss: 0.2309932987412067\n",
      "Epoch 1760/3000, Training Loss: 0.09853285633566339, Validation Loss: 0.2309586026327069\n",
      "Epoch 1761/3000, Training Loss: 0.09847201939948924, Validation Loss: 0.23091957460900708\n",
      "Epoch 1762/3000, Training Loss: 0.09841161983537386, Validation Loss: 0.2308878781162875\n",
      "Epoch 1763/3000, Training Loss: 0.09835079138974377, Validation Loss: 0.23085030314725535\n",
      "Epoch 1764/3000, Training Loss: 0.09829017794745894, Validation Loss: 0.23081737407545666\n",
      "Epoch 1765/3000, Training Loss: 0.09822969566260736, Validation Loss: 0.23077809680361527\n",
      "Epoch 1766/3000, Training Loss: 0.09816921360230019, Validation Loss: 0.23074392894268342\n",
      "Epoch 1767/3000, Training Loss: 0.09810865664315567, Validation Loss: 0.23070915246472254\n",
      "Epoch 1768/3000, Training Loss: 0.0980485623137251, Validation Loss: 0.2306726721964969\n",
      "Epoch 1769/3000, Training Loss: 0.09798787036369414, Validation Loss: 0.23064272933015417\n",
      "Epoch 1770/3000, Training Loss: 0.09792805781755992, Validation Loss: 0.2305981690245959\n",
      "Epoch 1771/3000, Training Loss: 0.09786760940631928, Validation Loss: 0.23056173166355273\n",
      "Epoch 1772/3000, Training Loss: 0.0978074464392093, Validation Loss: 0.23052839372883693\n",
      "Epoch 1773/3000, Training Loss: 0.09774760169008037, Validation Loss: 0.2304920774501936\n",
      "Epoch 1774/3000, Training Loss: 0.09768745784598243, Validation Loss: 0.2304621223584775\n",
      "Epoch 1775/3000, Training Loss: 0.09762767701641548, Validation Loss: 0.2304200550691747\n",
      "Epoch 1776/3000, Training Loss: 0.0975673304257795, Validation Loss: 0.2303930264860182\n",
      "Epoch 1777/3000, Training Loss: 0.09750794838637374, Validation Loss: 0.2303550815218329\n",
      "Epoch 1778/3000, Training Loss: 0.09744785873387599, Validation Loss: 0.23031486983146132\n",
      "Epoch 1779/3000, Training Loss: 0.09738836103770836, Validation Loss: 0.23028624368572498\n",
      "Epoch 1780/3000, Training Loss: 0.09732855905675979, Validation Loss: 0.2302478469018672\n",
      "Epoch 1781/3000, Training Loss: 0.09726906947443073, Validation Loss: 0.23021788408409513\n",
      "Epoch 1782/3000, Training Loss: 0.09720965000722451, Validation Loss: 0.23018090385043846\n",
      "Epoch 1783/3000, Training Loss: 0.09715002491324169, Validation Loss: 0.23014601864993703\n",
      "Epoch 1784/3000, Training Loss: 0.09709110572055536, Validation Loss: 0.23011275224875288\n",
      "Epoch 1785/3000, Training Loss: 0.09703131715463467, Validation Loss: 0.23007859987625204\n",
      "Epoch 1786/3000, Training Loss: 0.09697237819479107, Validation Loss: 0.2300363823604186\n",
      "Epoch 1787/3000, Training Loss: 0.09691301506363623, Validation Loss: 0.23000134029198177\n",
      "Epoch 1788/3000, Training Loss: 0.09685394858586303, Validation Loss: 0.22996780005617454\n",
      "Epoch 1789/3000, Training Loss: 0.09679508233262625, Validation Loss: 0.22993374933717867\n",
      "Epoch 1790/3000, Training Loss: 0.09673568205824536, Validation Loss: 0.22989846665674943\n",
      "Epoch 1791/3000, Training Loss: 0.09667689792118683, Validation Loss: 0.22986366219133839\n",
      "Epoch 1792/3000, Training Loss: 0.09661797080154685, Validation Loss: 0.229830714122855\n",
      "Epoch 1793/3000, Training Loss: 0.09655923789041909, Validation Loss: 0.22979908136665042\n",
      "Epoch 1794/3000, Training Loss: 0.09650021076791719, Validation Loss: 0.229762032575347\n",
      "Epoch 1795/3000, Training Loss: 0.0964415134286906, Validation Loss: 0.2297267301243002\n",
      "Epoch 1796/3000, Training Loss: 0.0963828684332066, Validation Loss: 0.22969365153871024\n",
      "Epoch 1797/3000, Training Loss: 0.09632413717754033, Validation Loss: 0.22965308760225314\n",
      "Epoch 1798/3000, Training Loss: 0.09626594231029285, Validation Loss: 0.22962360469709878\n",
      "Epoch 1799/3000, Training Loss: 0.09620730663920492, Validation Loss: 0.22958733850720678\n",
      "Epoch 1800/3000, Training Loss: 0.09614946447340163, Validation Loss: 0.22954967123274195\n",
      "Epoch 1801/3000, Training Loss: 0.0960908456599235, Validation Loss: 0.2295177368751495\n",
      "Epoch 1802/3000, Training Loss: 0.09603290449086786, Validation Loss: 0.22948050071762952\n",
      "Epoch 1803/3000, Training Loss: 0.09597485957143236, Validation Loss: 0.22944075444181328\n",
      "Epoch 1804/3000, Training Loss: 0.09591663392497313, Validation Loss: 0.2294058824322634\n",
      "Epoch 1805/3000, Training Loss: 0.09585886738655641, Validation Loss: 0.22936762474783956\n",
      "Epoch 1806/3000, Training Loss: 0.09580043362233051, Validation Loss: 0.2293391986283892\n",
      "Epoch 1807/3000, Training Loss: 0.09574313821404956, Validation Loss: 0.22929499680197196\n",
      "Epoch 1808/3000, Training Loss: 0.09568492071379443, Validation Loss: 0.22926046142397907\n",
      "Epoch 1809/3000, Training Loss: 0.09562724414056502, Validation Loss: 0.22922993334330863\n",
      "Epoch 1810/3000, Training Loss: 0.0955690786012235, Validation Loss: 0.22919010294548825\n",
      "Epoch 1811/3000, Training Loss: 0.09551157910220572, Validation Loss: 0.22915589608366782\n",
      "Epoch 1812/3000, Training Loss: 0.09545386023398654, Validation Loss: 0.22912635818575394\n",
      "Epoch 1813/3000, Training Loss: 0.09539603966736306, Validation Loss: 0.22908744518212096\n",
      "Epoch 1814/3000, Training Loss: 0.09533856214550147, Validation Loss: 0.22905431450474606\n",
      "Epoch 1815/3000, Training Loss: 0.09528072328460066, Validation Loss: 0.22902385120748892\n",
      "Epoch 1816/3000, Training Loss: 0.09522368864778628, Validation Loss: 0.22898461401417083\n",
      "Epoch 1817/3000, Training Loss: 0.09516566916564043, Validation Loss: 0.22894829845923714\n",
      "Epoch 1818/3000, Training Loss: 0.09510846164804436, Validation Loss: 0.22891675362535793\n",
      "Epoch 1819/3000, Training Loss: 0.09505135785412586, Validation Loss: 0.228877369146364\n",
      "Epoch 1820/3000, Training Loss: 0.0949936693060537, Validation Loss: 0.22884117587013614\n",
      "Epoch 1821/3000, Training Loss: 0.09493666819994437, Validation Loss: 0.2288020755871335\n",
      "Epoch 1822/3000, Training Loss: 0.09487908473390991, Validation Loss: 0.2287680007514169\n",
      "Epoch 1823/3000, Training Loss: 0.09482215272439368, Validation Loss: 0.22872964673427815\n",
      "Epoch 1824/3000, Training Loss: 0.09476495722770367, Validation Loss: 0.22869923061570627\n",
      "Epoch 1825/3000, Training Loss: 0.0947078406091714, Validation Loss: 0.2286650414814845\n",
      "Epoch 1826/3000, Training Loss: 0.09465069249687436, Validation Loss: 0.22862815515824741\n",
      "Epoch 1827/3000, Training Loss: 0.09459391639670471, Validation Loss: 0.22859800947142586\n",
      "Epoch 1828/3000, Training Loss: 0.09453700262609888, Validation Loss: 0.22856208462918504\n",
      "Epoch 1829/3000, Training Loss: 0.09447978174148221, Validation Loss: 0.22852575696739225\n",
      "Epoch 1830/3000, Training Loss: 0.0944232059236085, Validation Loss: 0.22849047100600609\n",
      "Epoch 1831/3000, Training Loss: 0.09436627480551621, Validation Loss: 0.2284547460176309\n",
      "Epoch 1832/3000, Training Loss: 0.09430968837945265, Validation Loss: 0.22842169724924613\n",
      "Epoch 1833/3000, Training Loss: 0.09425292353486003, Validation Loss: 0.22839000636079085\n",
      "Epoch 1834/3000, Training Loss: 0.0941960804162222, Validation Loss: 0.2283527342557035\n",
      "Epoch 1835/3000, Training Loss: 0.09413973513747155, Validation Loss: 0.22832256507536106\n",
      "Epoch 1836/3000, Training Loss: 0.09408306506324818, Validation Loss: 0.22828581914716717\n",
      "Epoch 1837/3000, Training Loss: 0.0940268145308214, Validation Loss: 0.22824407031190813\n",
      "Epoch 1838/3000, Training Loss: 0.093970247206754, Validation Loss: 0.22821359308312958\n",
      "Epoch 1839/3000, Training Loss: 0.09391388051595102, Validation Loss: 0.2281773846727015\n",
      "Epoch 1840/3000, Training Loss: 0.09385764806763064, Validation Loss: 0.2281448505725012\n",
      "Epoch 1841/3000, Training Loss: 0.09380138787574875, Validation Loss: 0.22811323937967087\n",
      "Epoch 1842/3000, Training Loss: 0.09374514961356757, Validation Loss: 0.22807472747212099\n",
      "Epoch 1843/3000, Training Loss: 0.09368887567194822, Validation Loss: 0.22804016205042457\n",
      "Epoch 1844/3000, Training Loss: 0.09363312606997189, Validation Loss: 0.22801006779264532\n",
      "Epoch 1845/3000, Training Loss: 0.09357653764164489, Validation Loss: 0.22797374444577093\n",
      "Epoch 1846/3000, Training Loss: 0.09352092627994615, Validation Loss: 0.22793916491139418\n",
      "Epoch 1847/3000, Training Loss: 0.09346471482329745, Validation Loss: 0.22791238507434022\n",
      "Epoch 1848/3000, Training Loss: 0.0934089703930391, Validation Loss: 0.2278748075878321\n",
      "Epoch 1849/3000, Training Loss: 0.09335306552713994, Validation Loss: 0.2278392686604528\n",
      "Epoch 1850/3000, Training Loss: 0.0932970897835161, Validation Loss: 0.22781044757388852\n",
      "Epoch 1851/3000, Training Loss: 0.09324142417556126, Validation Loss: 0.22777406345661308\n",
      "Epoch 1852/3000, Training Loss: 0.09318543687789102, Validation Loss: 0.2277377274972479\n",
      "Epoch 1853/3000, Training Loss: 0.09313005209298644, Validation Loss: 0.22770727015371184\n",
      "Epoch 1854/3000, Training Loss: 0.0930738969520865, Validation Loss: 0.22767241513239236\n",
      "Epoch 1855/3000, Training Loss: 0.09301856867367973, Validation Loss: 0.22763233886417164\n",
      "Epoch 1856/3000, Training Loss: 0.09296271282740183, Validation Loss: 0.2276031760546367\n",
      "Epoch 1857/3000, Training Loss: 0.09290714626697759, Validation Loss: 0.2275672888025264\n",
      "Epoch 1858/3000, Training Loss: 0.09285166192585956, Validation Loss: 0.2275306801904352\n",
      "Epoch 1859/3000, Training Loss: 0.09279604794614336, Validation Loss: 0.22750514232797478\n",
      "Epoch 1860/3000, Training Loss: 0.09274141093624515, Validation Loss: 0.22746672400314635\n",
      "Epoch 1861/3000, Training Loss: 0.09268558017672421, Validation Loss: 0.22743727035162806\n",
      "Epoch 1862/3000, Training Loss: 0.09263055103563349, Validation Loss: 0.22740373358587807\n",
      "Epoch 1863/3000, Training Loss: 0.09257564899672224, Validation Loss: 0.2273686584597403\n",
      "Epoch 1864/3000, Training Loss: 0.0925203826662496, Validation Loss: 0.22733606252946337\n",
      "Epoch 1865/3000, Training Loss: 0.09246549303709937, Validation Loss: 0.22730260401297323\n",
      "Epoch 1866/3000, Training Loss: 0.0924102405187141, Validation Loss: 0.2272713537362916\n",
      "Epoch 1867/3000, Training Loss: 0.09235563776444303, Validation Loss: 0.2272362502756875\n",
      "Epoch 1868/3000, Training Loss: 0.09230030380696547, Validation Loss: 0.22720724197304196\n",
      "Epoch 1869/3000, Training Loss: 0.09224561678988402, Validation Loss: 0.2271767663903434\n",
      "Epoch 1870/3000, Training Loss: 0.09219027662586012, Validation Loss: 0.22714203606707237\n",
      "Epoch 1871/3000, Training Loss: 0.09213581469285408, Validation Loss: 0.22711328966629726\n",
      "Epoch 1872/3000, Training Loss: 0.09208087337190728, Validation Loss: 0.2270748944697395\n",
      "Epoch 1873/3000, Training Loss: 0.09202586356325304, Validation Loss: 0.22704222239482638\n",
      "Epoch 1874/3000, Training Loss: 0.0919715718267737, Validation Loss: 0.22700921485651848\n",
      "Epoch 1875/3000, Training Loss: 0.09191659951970445, Validation Loss: 0.22697850106955678\n",
      "Epoch 1876/3000, Training Loss: 0.09186223848999035, Validation Loss: 0.22694411271687173\n",
      "Epoch 1877/3000, Training Loss: 0.09180750698510004, Validation Loss: 0.22691612955392065\n",
      "Epoch 1878/3000, Training Loss: 0.09175297209029105, Validation Loss: 0.2268858771400832\n",
      "Epoch 1879/3000, Training Loss: 0.09169864065955258, Validation Loss: 0.226852004809942\n",
      "Epoch 1880/3000, Training Loss: 0.09164405411185612, Validation Loss: 0.22682327019490736\n",
      "Epoch 1881/3000, Training Loss: 0.09158986793042845, Validation Loss: 0.2267899860808343\n",
      "Epoch 1882/3000, Training Loss: 0.09153522458258906, Validation Loss: 0.22676073760697033\n",
      "Epoch 1883/3000, Training Loss: 0.09148156476719095, Validation Loss: 0.22673145222736174\n",
      "Epoch 1884/3000, Training Loss: 0.09142690230952794, Validation Loss: 0.22669997333045233\n",
      "Epoch 1885/3000, Training Loss: 0.09137294447770147, Validation Loss: 0.22666154791275805\n",
      "Epoch 1886/3000, Training Loss: 0.0913189827973872, Validation Loss: 0.22663736304927734\n",
      "Epoch 1887/3000, Training Loss: 0.09126467601092961, Validation Loss: 0.22660170276770247\n",
      "Epoch 1888/3000, Training Loss: 0.09121104871429433, Validation Loss: 0.22657320982971332\n",
      "Epoch 1889/3000, Training Loss: 0.09115683363350223, Validation Loss: 0.22654423186627434\n",
      "Epoch 1890/3000, Training Loss: 0.09110309505629989, Validation Loss: 0.22650885434921783\n",
      "Epoch 1891/3000, Training Loss: 0.09104933377413457, Validation Loss: 0.2264735759118905\n",
      "Epoch 1892/3000, Training Loss: 0.09099546524873792, Validation Loss: 0.226446746588884\n",
      "Epoch 1893/3000, Training Loss: 0.09094170774192019, Validation Loss: 0.2264115854876263\n",
      "Epoch 1894/3000, Training Loss: 0.09088797295648286, Validation Loss: 0.2263813513194702\n",
      "Epoch 1895/3000, Training Loss: 0.09083452526650314, Validation Loss: 0.22635383359141056\n",
      "Epoch 1896/3000, Training Loss: 0.09078065459347646, Validation Loss: 0.22631772417566098\n",
      "Epoch 1897/3000, Training Loss: 0.09072727870618463, Validation Loss: 0.22628583302806804\n",
      "Epoch 1898/3000, Training Loss: 0.09067372546638003, Validation Loss: 0.2262626330493012\n",
      "Epoch 1899/3000, Training Loss: 0.09062029026583691, Validation Loss: 0.2262259522772841\n",
      "Epoch 1900/3000, Training Loss: 0.09056690701220765, Validation Loss: 0.2261967759599737\n",
      "Epoch 1901/3000, Training Loss: 0.09051325683086436, Validation Loss: 0.2261689520334082\n",
      "Epoch 1902/3000, Training Loss: 0.09046042543059675, Validation Loss: 0.226137791801128\n",
      "Epoch 1903/3000, Training Loss: 0.09040683408034404, Validation Loss: 0.22610463402679498\n",
      "Epoch 1904/3000, Training Loss: 0.09035379365972189, Validation Loss: 0.2260795519236286\n",
      "Epoch 1905/3000, Training Loss: 0.09030053372454472, Validation Loss: 0.22604467448203092\n",
      "Epoch 1906/3000, Training Loss: 0.09024734310495001, Validation Loss: 0.2260091528526918\n",
      "Epoch 1907/3000, Training Loss: 0.09019441407893848, Validation Loss: 0.22598403383374893\n",
      "Epoch 1908/3000, Training Loss: 0.09014127210181287, Validation Loss: 0.22595265869569317\n",
      "Epoch 1909/3000, Training Loss: 0.09008850417950497, Validation Loss: 0.22592428080518256\n",
      "Epoch 1910/3000, Training Loss: 0.09003533812411818, Validation Loss: 0.22589652978372896\n",
      "Epoch 1911/3000, Training Loss: 0.08998290525434609, Validation Loss: 0.22586148336054485\n",
      "Epoch 1912/3000, Training Loss: 0.0899296278394562, Validation Loss: 0.22583119326708664\n",
      "Epoch 1913/3000, Training Loss: 0.08987687352681903, Validation Loss: 0.22580255231704954\n",
      "Epoch 1914/3000, Training Loss: 0.08982451136663026, Validation Loss: 0.22577244020976692\n",
      "Epoch 1915/3000, Training Loss: 0.08977129949785141, Validation Loss: 0.2257400167140053\n",
      "Epoch 1916/3000, Training Loss: 0.08971915020082831, Validation Loss: 0.22570615188912516\n",
      "Epoch 1917/3000, Training Loss: 0.08966631071373578, Validation Loss: 0.22567355888983798\n",
      "Epoch 1918/3000, Training Loss: 0.08961387713445843, Validation Loss: 0.22563859688787022\n",
      "Epoch 1919/3000, Training Loss: 0.08956141958048561, Validation Loss: 0.22561707758516653\n",
      "Epoch 1920/3000, Training Loss: 0.08950897599230634, Validation Loss: 0.2255824066096889\n",
      "Epoch 1921/3000, Training Loss: 0.08945653624290034, Validation Loss: 0.22555322816402157\n",
      "Epoch 1922/3000, Training Loss: 0.08940426199218067, Validation Loss: 0.2255189649711601\n",
      "Epoch 1923/3000, Training Loss: 0.0893522065578776, Validation Loss: 0.22549254149796613\n",
      "Epoch 1924/3000, Training Loss: 0.08929958617077516, Validation Loss: 0.2254589637030563\n",
      "Epoch 1925/3000, Training Loss: 0.08924773206462332, Validation Loss: 0.22542653489885706\n",
      "Epoch 1926/3000, Training Loss: 0.08919554514660741, Validation Loss: 0.2254017767453178\n",
      "Epoch 1927/3000, Training Loss: 0.08914325636988242, Validation Loss: 0.22536597016872487\n",
      "Epoch 1928/3000, Training Loss: 0.08909152686963626, Validation Loss: 0.2253338800268867\n",
      "Epoch 1929/3000, Training Loss: 0.08903939440674431, Validation Loss: 0.22530796234865483\n",
      "Epoch 1930/3000, Training Loss: 0.08898767358777773, Validation Loss: 0.22527562653597444\n",
      "Epoch 1931/3000, Training Loss: 0.08893565760038008, Validation Loss: 0.22524759939589759\n",
      "Epoch 1932/3000, Training Loss: 0.08888382529083577, Validation Loss: 0.22521872684914257\n",
      "Epoch 1933/3000, Training Loss: 0.08883209908815158, Validation Loss: 0.22518823591952394\n",
      "Epoch 1934/3000, Training Loss: 0.08878030263043923, Validation Loss: 0.2251566518366214\n",
      "Epoch 1935/3000, Training Loss: 0.08872865730719255, Validation Loss: 0.22513050842386798\n",
      "Epoch 1936/3000, Training Loss: 0.08867676106439658, Validation Loss: 0.22509424537882008\n",
      "Epoch 1937/3000, Training Loss: 0.0886257380366844, Validation Loss: 0.22506776391968966\n",
      "Epoch 1938/3000, Training Loss: 0.08857366331955001, Validation Loss: 0.22503961045391288\n",
      "Epoch 1939/3000, Training Loss: 0.08852261900601587, Validation Loss: 0.22500701790878377\n",
      "Epoch 1940/3000, Training Loss: 0.08847079885806396, Validation Loss: 0.22497617559960348\n",
      "Epoch 1941/3000, Training Loss: 0.08841964174739103, Validation Loss: 0.2249481461382311\n",
      "Epoch 1942/3000, Training Loss: 0.08836849043217283, Validation Loss: 0.2249150749653826\n",
      "Epoch 1943/3000, Training Loss: 0.08831689275270374, Validation Loss: 0.22488381470560248\n",
      "Epoch 1944/3000, Training Loss: 0.08826580343935374, Validation Loss: 0.22485673314427781\n",
      "Epoch 1945/3000, Training Loss: 0.08821452334379865, Validation Loss: 0.22482856051146574\n",
      "Epoch 1946/3000, Training Loss: 0.08816363161485435, Validation Loss: 0.22479504951957105\n",
      "Epoch 1947/3000, Training Loss: 0.08811222111894376, Validation Loss: 0.22476313153136973\n",
      "Epoch 1948/3000, Training Loss: 0.08806130682518422, Validation Loss: 0.2247387747473365\n",
      "Epoch 1949/3000, Training Loss: 0.08801027881437612, Validation Loss: 0.22470655724402988\n",
      "Epoch 1950/3000, Training Loss: 0.08795914757342448, Validation Loss: 0.22467120089464823\n",
      "Epoch 1951/3000, Training Loss: 0.08790848394934525, Validation Loss: 0.22464753008375762\n",
      "Epoch 1952/3000, Training Loss: 0.08785717592711562, Validation Loss: 0.2246191511287278\n",
      "Epoch 1953/3000, Training Loss: 0.08780668775500108, Validation Loss: 0.22458533151773621\n",
      "Epoch 1954/3000, Training Loss: 0.08775580198701145, Validation Loss: 0.22456043264511014\n",
      "Epoch 1955/3000, Training Loss: 0.08770481889705611, Validation Loss: 0.22453126659545722\n",
      "Epoch 1956/3000, Training Loss: 0.08765453893791622, Validation Loss: 0.2245009959808763\n",
      "Epoch 1957/3000, Training Loss: 0.08760336607361092, Validation Loss: 0.22447327101893141\n",
      "Epoch 1958/3000, Training Loss: 0.08755312525816199, Validation Loss: 0.2244458090744894\n",
      "Epoch 1959/3000, Training Loss: 0.08750223470316658, Validation Loss: 0.22441517938125716\n",
      "Epoch 1960/3000, Training Loss: 0.08745181326734437, Validation Loss: 0.2243848891589068\n",
      "Epoch 1961/3000, Training Loss: 0.08740140433623236, Validation Loss: 0.22435592610797808\n",
      "Epoch 1962/3000, Training Loss: 0.08735076587632644, Validation Loss: 0.22432297154330472\n",
      "Epoch 1963/3000, Training Loss: 0.0873005594970957, Validation Loss: 0.2242978011561049\n",
      "Epoch 1964/3000, Training Loss: 0.0872499446952968, Validation Loss: 0.22426854998879817\n",
      "Epoch 1965/3000, Training Loss: 0.08719998772010143, Validation Loss: 0.22423843787177306\n",
      "Epoch 1966/3000, Training Loss: 0.08714913688104267, Validation Loss: 0.22421410124869356\n",
      "Epoch 1967/3000, Training Loss: 0.08709935766607443, Validation Loss: 0.22417635308997635\n",
      "Epoch 1968/3000, Training Loss: 0.08704913544238903, Validation Loss: 0.22415162275760384\n",
      "Epoch 1969/3000, Training Loss: 0.08699881055771065, Validation Loss: 0.2241181646769242\n",
      "Epoch 1970/3000, Training Loss: 0.08694885705230546, Validation Loss: 0.2240947146871039\n",
      "Epoch 1971/3000, Training Loss: 0.08689866143188404, Validation Loss: 0.22405918383628903\n",
      "Epoch 1972/3000, Training Loss: 0.0868487286027264, Validation Loss: 0.2240306838251633\n",
      "Epoch 1973/3000, Training Loss: 0.08679883008136165, Validation Loss: 0.22400703224469026\n",
      "Epoch 1974/3000, Training Loss: 0.08674877927129318, Validation Loss: 0.22397557205289712\n",
      "Epoch 1975/3000, Training Loss: 0.0866986974248075, Validation Loss: 0.2239453672457317\n",
      "Epoch 1976/3000, Training Loss: 0.08664912886437046, Validation Loss: 0.22392021687895194\n",
      "Epoch 1977/3000, Training Loss: 0.0865993319459252, Validation Loss: 0.2238921780656712\n",
      "Epoch 1978/3000, Training Loss: 0.08654920617364939, Validation Loss: 0.22386129717572845\n",
      "Epoch 1979/3000, Training Loss: 0.08649992747314572, Validation Loss: 0.2238374313423609\n",
      "Epoch 1980/3000, Training Loss: 0.08645009560848195, Validation Loss: 0.223807404835273\n",
      "Epoch 1981/3000, Training Loss: 0.08640037719019487, Validation Loss: 0.22377588001408472\n",
      "Epoch 1982/3000, Training Loss: 0.08635095652170613, Validation Loss: 0.22375328277104695\n",
      "Epoch 1983/3000, Training Loss: 0.08630105846213328, Validation Loss: 0.22372328181856047\n",
      "Epoch 1984/3000, Training Loss: 0.08625197794263469, Validation Loss: 0.22369285946591366\n",
      "Epoch 1985/3000, Training Loss: 0.08620223454323268, Validation Loss: 0.22367173141308586\n",
      "Epoch 1986/3000, Training Loss: 0.08615307124591011, Validation Loss: 0.22363997678975475\n",
      "Epoch 1987/3000, Training Loss: 0.08610336285786196, Validation Loss: 0.2236132583864969\n",
      "Epoch 1988/3000, Training Loss: 0.0860542279328381, Validation Loss: 0.22358529810801278\n",
      "Epoch 1989/3000, Training Loss: 0.08600528367981847, Validation Loss: 0.22355838766926822\n",
      "Epoch 1990/3000, Training Loss: 0.08595556499574616, Validation Loss: 0.22353042244826463\n",
      "Epoch 1991/3000, Training Loss: 0.0859069644772837, Validation Loss: 0.22349995088201138\n",
      "Epoch 1992/3000, Training Loss: 0.08585765452090521, Validation Loss: 0.22347323962767848\n",
      "Epoch 1993/3000, Training Loss: 0.08580853212753714, Validation Loss: 0.22343919447597196\n",
      "Epoch 1994/3000, Training Loss: 0.08575975972825085, Validation Loss: 0.22341339523040518\n",
      "Epoch 1995/3000, Training Loss: 0.08571044781892977, Validation Loss: 0.22338884686409896\n",
      "Epoch 1996/3000, Training Loss: 0.08566204576408737, Validation Loss: 0.22336278430018877\n",
      "Epoch 1997/3000, Training Loss: 0.08561283681419357, Validation Loss: 0.22333442370226692\n",
      "Epoch 1998/3000, Training Loss: 0.08556404635600334, Validation Loss: 0.2233079879282445\n",
      "Epoch 1999/3000, Training Loss: 0.0855153840314846, Validation Loss: 0.22328212456427557\n",
      "Epoch 2000/3000, Training Loss: 0.08546649681806141, Validation Loss: 0.22325235570197394\n",
      "Epoch 2001/3000, Training Loss: 0.08541802244140975, Validation Loss: 0.22323003168867142\n",
      "Epoch 2002/3000, Training Loss: 0.08536891127001472, Validation Loss: 0.22320417824695962\n",
      "Epoch 2003/3000, Training Loss: 0.08532093045108703, Validation Loss: 0.22317296073218545\n",
      "Epoch 2004/3000, Training Loss: 0.08527208235642852, Validation Loss: 0.22315125194169194\n",
      "Epoch 2005/3000, Training Loss: 0.08522357262587246, Validation Loss: 0.22312078337708985\n",
      "Epoch 2006/3000, Training Loss: 0.08517510223753673, Validation Loss: 0.22309467183341422\n",
      "Epoch 2007/3000, Training Loss: 0.08512662393504913, Validation Loss: 0.22306654027362802\n",
      "Epoch 2008/3000, Training Loss: 0.08507841364052114, Validation Loss: 0.22304183418717535\n",
      "Epoch 2009/3000, Training Loss: 0.08503012000239234, Validation Loss: 0.22301275497676917\n",
      "Epoch 2010/3000, Training Loss: 0.084981668206349, Validation Loss: 0.22299079347506826\n",
      "Epoch 2011/3000, Training Loss: 0.08493374675418106, Validation Loss: 0.2229618944407163\n",
      "Epoch 2012/3000, Training Loss: 0.08488559629051351, Validation Loss: 0.22293195885394146\n",
      "Epoch 2013/3000, Training Loss: 0.08483746845962047, Validation Loss: 0.22291035224717823\n",
      "Epoch 2014/3000, Training Loss: 0.0847892696497576, Validation Loss: 0.22288139933765772\n",
      "Epoch 2015/3000, Training Loss: 0.0847417839140833, Validation Loss: 0.22285488657650246\n",
      "Epoch 2016/3000, Training Loss: 0.0846932385589959, Validation Loss: 0.22282662115862056\n",
      "Epoch 2017/3000, Training Loss: 0.0846457742946018, Validation Loss: 0.2227956274798724\n",
      "Epoch 2018/3000, Training Loss: 0.08459774987312649, Validation Loss: 0.22276845899742598\n",
      "Epoch 2019/3000, Training Loss: 0.08454982382796206, Validation Loss: 0.22273892728760605\n",
      "Epoch 2020/3000, Training Loss: 0.08450225925174992, Validation Loss: 0.22271831689954447\n",
      "Epoch 2021/3000, Training Loss: 0.08445411885423555, Validation Loss: 0.2226907311267965\n",
      "Epoch 2022/3000, Training Loss: 0.08440683350934537, Validation Loss: 0.2226595516398442\n",
      "Epoch 2023/3000, Training Loss: 0.08435899882353799, Validation Loss: 0.2226393258413459\n",
      "Epoch 2024/3000, Training Loss: 0.08431131953997652, Validation Loss: 0.22260913575877467\n",
      "Epoch 2025/3000, Training Loss: 0.08426363363220016, Validation Loss: 0.22258275940598124\n",
      "Epoch 2026/3000, Training Loss: 0.08421600429441292, Validation Loss: 0.22255991185458807\n",
      "Epoch 2027/3000, Training Loss: 0.08416890625593193, Validation Loss: 0.2225299302918054\n",
      "Epoch 2028/3000, Training Loss: 0.08412094310470405, Validation Loss: 0.22250425963559456\n",
      "Epoch 2029/3000, Training Loss: 0.08407362545613214, Validation Loss: 0.22247921310872865\n",
      "Epoch 2030/3000, Training Loss: 0.08402641650187123, Validation Loss: 0.22245469154381714\n",
      "Epoch 2031/3000, Training Loss: 0.08397872560741272, Validation Loss: 0.22242678493453139\n",
      "Epoch 2032/3000, Training Loss: 0.08393174783272431, Validation Loss: 0.2224037143260702\n",
      "Epoch 2033/3000, Training Loss: 0.0838840356545841, Validation Loss: 0.2223759128437283\n",
      "Epoch 2034/3000, Training Loss: 0.0838372148203836, Validation Loss: 0.22234919818396504\n",
      "Epoch 2035/3000, Training Loss: 0.08378981872001312, Validation Loss: 0.22232654586556094\n",
      "Epoch 2036/3000, Training Loss: 0.08374276198475204, Validation Loss: 0.2223001136930984\n",
      "Epoch 2037/3000, Training Loss: 0.08369545896619922, Validation Loss: 0.22227032863693114\n",
      "Epoch 2038/3000, Training Loss: 0.08364843531647122, Validation Loss: 0.2222485922738936\n",
      "Epoch 2039/3000, Training Loss: 0.0836015540304099, Validation Loss: 0.22222067957247665\n",
      "Epoch 2040/3000, Training Loss: 0.08355423687707184, Validation Loss: 0.22219622870108413\n",
      "Epoch 2041/3000, Training Loss: 0.08350763366198051, Validation Loss: 0.2221686470552946\n",
      "Epoch 2042/3000, Training Loss: 0.0834606424881428, Validation Loss: 0.2221460615525825\n",
      "Epoch 2043/3000, Training Loss: 0.0834135776682358, Validation Loss: 0.22210950339374627\n",
      "Epoch 2044/3000, Training Loss: 0.08336682881672773, Validation Loss: 0.22208542744210188\n",
      "Epoch 2045/3000, Training Loss: 0.08331995624962722, Validation Loss: 0.22206333408306964\n",
      "Epoch 2046/3000, Training Loss: 0.08327347236186938, Validation Loss: 0.2220363999171117\n",
      "Epoch 2047/3000, Training Loss: 0.08322647866051355, Validation Loss: 0.2220070415036573\n",
      "Epoch 2048/3000, Training Loss: 0.08317990256570726, Validation Loss: 0.22198290167506404\n",
      "Epoch 2049/3000, Training Loss: 0.08313311072175578, Validation Loss: 0.22195887934082725\n",
      "Epoch 2050/3000, Training Loss: 0.08308661024390637, Validation Loss: 0.22192962771337263\n",
      "Epoch 2051/3000, Training Loss: 0.08304017717412704, Validation Loss: 0.22191132042129608\n",
      "Epoch 2052/3000, Training Loss: 0.08299314016727878, Validation Loss: 0.2218814013005277\n",
      "Epoch 2053/3000, Training Loss: 0.08294740805906005, Validation Loss: 0.22185517507813943\n",
      "Epoch 2054/3000, Training Loss: 0.08290046969894474, Validation Loss: 0.2218351467193037\n",
      "Epoch 2055/3000, Training Loss: 0.08285399956751223, Validation Loss: 0.22180522629679866\n",
      "Epoch 2056/3000, Training Loss: 0.08280779981608494, Validation Loss: 0.22177999486477337\n",
      "Epoch 2057/3000, Training Loss: 0.0827611570659568, Validation Loss: 0.22175635388124085\n",
      "Epoch 2058/3000, Training Loss: 0.08271538888075834, Validation Loss: 0.2217293623919572\n",
      "Epoch 2059/3000, Training Loss: 0.08266864271942098, Validation Loss: 0.22170615303556723\n",
      "Epoch 2060/3000, Training Loss: 0.08262255744985414, Validation Loss: 0.22167651397498295\n",
      "Epoch 2061/3000, Training Loss: 0.08257632468301565, Validation Loss: 0.2216567891879375\n",
      "Epoch 2062/3000, Training Loss: 0.08253018814482695, Validation Loss: 0.2216269349449713\n",
      "Epoch 2063/3000, Training Loss: 0.08248427653600263, Validation Loss: 0.22160057881067338\n",
      "Epoch 2064/3000, Training Loss: 0.0824377731440625, Validation Loss: 0.22158204625883804\n",
      "Epoch 2065/3000, Training Loss: 0.08239209764901476, Validation Loss: 0.2215525257850777\n",
      "Epoch 2066/3000, Training Loss: 0.08234588614995074, Validation Loss: 0.2215283528173886\n",
      "Epoch 2067/3000, Training Loss: 0.08229969540161695, Validation Loss: 0.22149737062257716\n",
      "Epoch 2068/3000, Training Loss: 0.08225418300065283, Validation Loss: 0.2214727885072468\n",
      "Epoch 2069/3000, Training Loss: 0.08220788580253306, Validation Loss: 0.22144360046013833\n",
      "Epoch 2070/3000, Training Loss: 0.0821621408089199, Validation Loss: 0.22142376439783662\n",
      "Epoch 2071/3000, Training Loss: 0.08211619983772118, Validation Loss: 0.22139752622490239\n",
      "Epoch 2072/3000, Training Loss: 0.08207053082905771, Validation Loss: 0.22137245798138022\n",
      "Epoch 2073/3000, Training Loss: 0.08202442689452923, Validation Loss: 0.2213483875576091\n",
      "Epoch 2074/3000, Training Loss: 0.08197875693948793, Validation Loss: 0.22132239016485605\n",
      "Epoch 2075/3000, Training Loss: 0.08193297379329481, Validation Loss: 0.22129583539065747\n",
      "Epoch 2076/3000, Training Loss: 0.08188692430677944, Validation Loss: 0.22127395117775853\n",
      "Epoch 2077/3000, Training Loss: 0.08184181442611291, Validation Loss: 0.22125051499864745\n",
      "Epoch 2078/3000, Training Loss: 0.08179574677432923, Validation Loss: 0.22122245157978548\n",
      "Epoch 2079/3000, Training Loss: 0.08175008619881168, Validation Loss: 0.22119754680384213\n",
      "Epoch 2080/3000, Training Loss: 0.08170478556297096, Validation Loss: 0.22117708170556638\n",
      "Epoch 2081/3000, Training Loss: 0.08165905473903592, Validation Loss: 0.22114770343901766\n",
      "Epoch 2082/3000, Training Loss: 0.08161356811230308, Validation Loss: 0.22112400402477017\n",
      "Epoch 2083/3000, Training Loss: 0.08156805009362249, Validation Loss: 0.22110229808404544\n",
      "Epoch 2084/3000, Training Loss: 0.08152283933004925, Validation Loss: 0.22107336358292762\n",
      "Epoch 2085/3000, Training Loss: 0.08147722193353746, Validation Loss: 0.2210514618586673\n",
      "Epoch 2086/3000, Training Loss: 0.08143191019353435, Validation Loss: 0.2210264007038534\n",
      "Epoch 2087/3000, Training Loss: 0.08138673881879693, Validation Loss: 0.22100305407622922\n",
      "Epoch 2088/3000, Training Loss: 0.0813411341349364, Validation Loss: 0.2209743402926638\n",
      "Epoch 2089/3000, Training Loss: 0.08129627983995792, Validation Loss: 0.2209531045819343\n",
      "Epoch 2090/3000, Training Loss: 0.08125075759117124, Validation Loss: 0.22092165941765105\n",
      "Epoch 2091/3000, Training Loss: 0.08120586217026929, Validation Loss: 0.2208994671433112\n",
      "Epoch 2092/3000, Training Loss: 0.08116057097454003, Validation Loss: 0.2208765992761417\n",
      "Epoch 2093/3000, Training Loss: 0.08111551535596383, Validation Loss: 0.2208496769185085\n",
      "Epoch 2094/3000, Training Loss: 0.08107054054749838, Validation Loss: 0.22082417941595453\n",
      "Epoch 2095/3000, Training Loss: 0.08102523367869105, Validation Loss: 0.2208006437999506\n",
      "Epoch 2096/3000, Training Loss: 0.08098078877777548, Validation Loss: 0.22077559487246612\n",
      "Epoch 2097/3000, Training Loss: 0.08093539381167388, Validation Loss: 0.22075598583214348\n",
      "Epoch 2098/3000, Training Loss: 0.08089064985696291, Validation Loss: 0.22072566962613757\n",
      "Epoch 2099/3000, Training Loss: 0.08084603382389584, Validation Loss: 0.2207051671947545\n",
      "Epoch 2100/3000, Training Loss: 0.08080106484199778, Validation Loss: 0.2206797988503211\n",
      "Epoch 2101/3000, Training Loss: 0.08075643547758211, Validation Loss: 0.22065372692089022\n",
      "Epoch 2102/3000, Training Loss: 0.08071141329199906, Validation Loss: 0.22063670752537887\n",
      "Epoch 2103/3000, Training Loss: 0.08066706053212652, Validation Loss: 0.22060992188093653\n",
      "Epoch 2104/3000, Training Loss: 0.08062218258090227, Validation Loss: 0.2205821980371829\n",
      "Epoch 2105/3000, Training Loss: 0.08057762345887039, Validation Loss: 0.22056137512959895\n",
      "Epoch 2106/3000, Training Loss: 0.08053311997568514, Validation Loss: 0.22053992438142228\n",
      "Epoch 2107/3000, Training Loss: 0.08048837004387074, Validation Loss: 0.2205111592759085\n",
      "Epoch 2108/3000, Training Loss: 0.0804442706687915, Validation Loss: 0.22049252333747554\n",
      "Epoch 2109/3000, Training Loss: 0.08039937867309638, Validation Loss: 0.22046316702364904\n",
      "Epoch 2110/3000, Training Loss: 0.08035506039999035, Validation Loss: 0.22044053692508989\n",
      "Epoch 2111/3000, Training Loss: 0.08031080473592642, Validation Loss: 0.2204169456095798\n",
      "Epoch 2112/3000, Training Loss: 0.08026636196609098, Validation Loss: 0.22038440207774362\n",
      "Epoch 2113/3000, Training Loss: 0.08022216321917394, Validation Loss: 0.22036270102274236\n",
      "Epoch 2114/3000, Training Loss: 0.08017759422224345, Validation Loss: 0.22034166690947335\n",
      "Epoch 2115/3000, Training Loss: 0.08013381319283858, Validation Loss: 0.2203123311625694\n",
      "Epoch 2116/3000, Training Loss: 0.0800891465397107, Validation Loss: 0.22028825592105866\n",
      "Epoch 2117/3000, Training Loss: 0.08004535447844702, Validation Loss: 0.22026618478739673\n",
      "Epoch 2118/3000, Training Loss: 0.08000106435732009, Validation Loss: 0.22024110654223306\n",
      "Epoch 2119/3000, Training Loss: 0.07995685627554783, Validation Loss: 0.22022207959386697\n",
      "Epoch 2120/3000, Training Loss: 0.07991311459584008, Validation Loss: 0.22019235207118376\n",
      "Epoch 2121/3000, Training Loss: 0.07986869564715805, Validation Loss: 0.22017027216198612\n",
      "Epoch 2122/3000, Training Loss: 0.07982508022795995, Validation Loss: 0.22014874308568483\n",
      "Epoch 2123/3000, Training Loss: 0.07978100712444683, Validation Loss: 0.2201230135159433\n",
      "Epoch 2124/3000, Training Loss: 0.07973698337931173, Validation Loss: 0.2201009288220146\n",
      "Epoch 2125/3000, Training Loss: 0.07969332474096116, Validation Loss: 0.22007804691276037\n",
      "Epoch 2126/3000, Training Loss: 0.07964931558368704, Validation Loss: 0.22005046777096518\n",
      "Epoch 2127/3000, Training Loss: 0.07960590211551881, Validation Loss: 0.22002949782596784\n",
      "Epoch 2128/3000, Training Loss: 0.07956171762611, Validation Loss: 0.2200073838549402\n",
      "Epoch 2129/3000, Training Loss: 0.07951819487145188, Validation Loss: 0.21998703024038851\n",
      "Epoch 2130/3000, Training Loss: 0.0794743754320682, Validation Loss: 0.2199639496021611\n",
      "Epoch 2131/3000, Training Loss: 0.07943018766841944, Validation Loss: 0.21993774368633356\n",
      "Epoch 2132/3000, Training Loss: 0.07938677654567071, Validation Loss: 0.21991452813983034\n",
      "Epoch 2133/3000, Training Loss: 0.07934261880536062, Validation Loss: 0.2198855054555529\n",
      "Epoch 2134/3000, Training Loss: 0.07929924494498504, Validation Loss: 0.21986356457054743\n",
      "Epoch 2135/3000, Training Loss: 0.07925526663156253, Validation Loss: 0.21983961794232282\n",
      "Epoch 2136/3000, Training Loss: 0.07921167280261127, Validation Loss: 0.21981421030421236\n",
      "Epoch 2137/3000, Training Loss: 0.07916825015091096, Validation Loss: 0.21979911334022792\n",
      "Epoch 2138/3000, Training Loss: 0.07912444947734214, Validation Loss: 0.21977404019776184\n",
      "Epoch 2139/3000, Training Loss: 0.07908130720647755, Validation Loss: 0.21975232249733082\n",
      "Epoch 2140/3000, Training Loss: 0.07903749921035015, Validation Loss: 0.21973183140199726\n",
      "Epoch 2141/3000, Training Loss: 0.07899437256382123, Validation Loss: 0.21970488634469493\n",
      "Epoch 2142/3000, Training Loss: 0.07895090807698432, Validation Loss: 0.21968483657239377\n",
      "Epoch 2143/3000, Training Loss: 0.0789075095615523, Validation Loss: 0.2196589836821879\n",
      "Epoch 2144/3000, Training Loss: 0.07886432297215666, Validation Loss: 0.21963887734540416\n",
      "Epoch 2145/3000, Training Loss: 0.07882091666553691, Validation Loss: 0.21962209833440863\n",
      "Epoch 2146/3000, Training Loss: 0.07877804405387781, Validation Loss: 0.21960248076988714\n",
      "Epoch 2147/3000, Training Loss: 0.07873450944903722, Validation Loss: 0.2195746158118417\n",
      "Epoch 2148/3000, Training Loss: 0.07869152224800316, Validation Loss: 0.21955482131492296\n",
      "Epoch 2149/3000, Training Loss: 0.07864843180903221, Validation Loss: 0.2195319054313748\n",
      "Epoch 2150/3000, Training Loss: 0.07860497671085145, Validation Loss: 0.21950746099153576\n",
      "Epoch 2151/3000, Training Loss: 0.07856243837995042, Validation Loss: 0.2194861427310116\n",
      "Epoch 2152/3000, Training Loss: 0.07851900258739627, Validation Loss: 0.21946635385465366\n",
      "Epoch 2153/3000, Training Loss: 0.07847645434603476, Validation Loss: 0.21944482346728286\n",
      "Epoch 2154/3000, Training Loss: 0.07843335163840831, Validation Loss: 0.219416503829429\n",
      "Epoch 2155/3000, Training Loss: 0.0783904880185559, Validation Loss: 0.21939326730905917\n",
      "Epoch 2156/3000, Training Loss: 0.07834770986794731, Validation Loss: 0.21937447076887576\n",
      "Epoch 2157/3000, Training Loss: 0.07830469486053226, Validation Loss: 0.21934677838695618\n",
      "Epoch 2158/3000, Training Loss: 0.07826227758736216, Validation Loss: 0.2193264238590291\n",
      "Epoch 2159/3000, Training Loss: 0.07821924199842367, Validation Loss: 0.21930799056846823\n",
      "Epoch 2160/3000, Training Loss: 0.07817708213107917, Validation Loss: 0.21929051642329367\n",
      "Epoch 2161/3000, Training Loss: 0.07813416062156037, Validation Loss: 0.21926940872564543\n",
      "Epoch 2162/3000, Training Loss: 0.07809163288876272, Validation Loss: 0.2192439407491112\n",
      "Epoch 2163/3000, Training Loss: 0.07804922707396296, Validation Loss: 0.21922123354944162\n",
      "Epoch 2164/3000, Training Loss: 0.07800680636599916, Validation Loss: 0.2191978499010728\n",
      "Epoch 2165/3000, Training Loss: 0.07796444526514588, Validation Loss: 0.219180378169723\n",
      "Epoch 2166/3000, Training Loss: 0.07792177434051119, Validation Loss: 0.21915681606169465\n",
      "Epoch 2167/3000, Training Loss: 0.07787959922922208, Validation Loss: 0.21913737495345254\n",
      "Epoch 2168/3000, Training Loss: 0.07783726769049998, Validation Loss: 0.21911384668604064\n",
      "Epoch 2169/3000, Training Loss: 0.07779492324937054, Validation Loss: 0.2190889323257602\n",
      "Epoch 2170/3000, Training Loss: 0.07775270605722752, Validation Loss: 0.21907160679222745\n",
      "Epoch 2171/3000, Training Loss: 0.0777100955427146, Validation Loss: 0.21905041638546108\n",
      "Epoch 2172/3000, Training Loss: 0.07766851685433966, Validation Loss: 0.2190236989326038\n",
      "Epoch 2173/3000, Training Loss: 0.07762601551191502, Validation Loss: 0.21900137751786497\n",
      "Epoch 2174/3000, Training Loss: 0.0775841223513236, Validation Loss: 0.2189840836345603\n",
      "Epoch 2175/3000, Training Loss: 0.07754176601587814, Validation Loss: 0.2189644876282868\n",
      "Epoch 2176/3000, Training Loss: 0.07749981275621667, Validation Loss: 0.21893573781161307\n",
      "Epoch 2177/3000, Training Loss: 0.07745812797052858, Validation Loss: 0.2189152574247048\n",
      "Epoch 2178/3000, Training Loss: 0.07741556718358689, Validation Loss: 0.21889275057280483\n",
      "Epoch 2179/3000, Training Loss: 0.07737421236280574, Validation Loss: 0.21887056726447973\n",
      "Epoch 2180/3000, Training Loss: 0.07733207777051372, Validation Loss: 0.21885275719402894\n",
      "Epoch 2181/3000, Training Loss: 0.07729023073698107, Validation Loss: 0.21882823433149645\n",
      "Epoch 2182/3000, Training Loss: 0.07724842031269875, Validation Loss: 0.21881077980575647\n",
      "Epoch 2183/3000, Training Loss: 0.07720650033930385, Validation Loss: 0.21878291253012122\n",
      "Epoch 2184/3000, Training Loss: 0.0771650094157504, Validation Loss: 0.21876644103955817\n",
      "Epoch 2185/3000, Training Loss: 0.07712282613927432, Validation Loss: 0.21873787762603078\n",
      "Epoch 2186/3000, Training Loss: 0.07708150148584114, Validation Loss: 0.21871777792123237\n",
      "Epoch 2187/3000, Training Loss: 0.07703982574176012, Validation Loss: 0.21869757747709315\n",
      "Epoch 2188/3000, Training Loss: 0.07699799517733817, Validation Loss: 0.21867584251228062\n",
      "Epoch 2189/3000, Training Loss: 0.07695649347101866, Validation Loss: 0.21865960153171904\n",
      "Epoch 2190/3000, Training Loss: 0.07691477079845277, Validation Loss: 0.21863428267931614\n",
      "Epoch 2191/3000, Training Loss: 0.07687377291859639, Validation Loss: 0.21860982939673612\n",
      "Epoch 2192/3000, Training Loss: 0.07683184996651587, Validation Loss: 0.2185905520688562\n",
      "Epoch 2193/3000, Training Loss: 0.07679049846610536, Validation Loss: 0.21856755588478946\n",
      "Epoch 2194/3000, Training Loss: 0.07674891116493951, Validation Loss: 0.21854605076263967\n",
      "Epoch 2195/3000, Training Loss: 0.07670755841630249, Validation Loss: 0.21851753588881617\n",
      "Epoch 2196/3000, Training Loss: 0.07666634107884475, Validation Loss: 0.21850198531872148\n",
      "Epoch 2197/3000, Training Loss: 0.0766247184468945, Validation Loss: 0.21848266443473968\n",
      "Epoch 2198/3000, Training Loss: 0.0765838654011217, Validation Loss: 0.21846284059932577\n",
      "Epoch 2199/3000, Training Loss: 0.07654224987928178, Validation Loss: 0.21844100024741492\n",
      "Epoch 2200/3000, Training Loss: 0.07650124145906793, Validation Loss: 0.21841292895065784\n",
      "Epoch 2201/3000, Training Loss: 0.07645997228468503, Validation Loss: 0.2183941186380622\n",
      "Epoch 2202/3000, Training Loss: 0.0764187322085575, Validation Loss: 0.21837657031463453\n",
      "Epoch 2203/3000, Training Loss: 0.07637776215947571, Validation Loss: 0.21835640281943325\n",
      "Epoch 2204/3000, Training Loss: 0.07633632511172966, Validation Loss: 0.21833539467613244\n",
      "Epoch 2205/3000, Training Loss: 0.07629579175873684, Validation Loss: 0.21830761500071097\n",
      "Epoch 2206/3000, Training Loss: 0.07625431579669782, Validation Loss: 0.21828995142718938\n",
      "Epoch 2207/3000, Training Loss: 0.07621337555744337, Validation Loss: 0.21826902163780396\n",
      "Epoch 2208/3000, Training Loss: 0.07617221193446004, Validation Loss: 0.21824784456254587\n",
      "Epoch 2209/3000, Training Loss: 0.07613106790425407, Validation Loss: 0.21822713617888417\n",
      "Epoch 2210/3000, Training Loss: 0.07609052632146274, Validation Loss: 0.21820524816801673\n",
      "Epoch 2211/3000, Training Loss: 0.07604924743677977, Validation Loss: 0.21818582016966526\n",
      "Epoch 2212/3000, Training Loss: 0.07600847318308473, Validation Loss: 0.21816465168287588\n",
      "Epoch 2213/3000, Training Loss: 0.07596741974427178, Validation Loss: 0.2181478248098096\n",
      "Epoch 2214/3000, Training Loss: 0.07592676160508317, Validation Loss: 0.2181170711556655\n",
      "Epoch 2215/3000, Training Loss: 0.0758859232161259, Validation Loss: 0.21810502367484277\n",
      "Epoch 2216/3000, Training Loss: 0.07584497964952688, Validation Loss: 0.21808388337010212\n",
      "Epoch 2217/3000, Training Loss: 0.07580456083980161, Validation Loss: 0.2180642209788772\n",
      "Epoch 2218/3000, Training Loss: 0.07576352967540587, Validation Loss: 0.21804633350054708\n",
      "Epoch 2219/3000, Training Loss: 0.07572309630023322, Validation Loss: 0.21801968817190445\n",
      "Epoch 2220/3000, Training Loss: 0.07568231749965557, Validation Loss: 0.21799755168604398\n",
      "Epoch 2221/3000, Training Loss: 0.0756418758714641, Validation Loss: 0.21797899500302864\n",
      "Epoch 2222/3000, Training Loss: 0.0756011510278309, Validation Loss: 0.21795779560833128\n",
      "Epoch 2223/3000, Training Loss: 0.075560386030697, Validation Loss: 0.21793624537099263\n",
      "Epoch 2224/3000, Training Loss: 0.07552055002403298, Validation Loss: 0.2179211423608934\n",
      "Epoch 2225/3000, Training Loss: 0.07547953582312608, Validation Loss: 0.21790090601288334\n",
      "Epoch 2226/3000, Training Loss: 0.07543933938076168, Validation Loss: 0.21787708846663736\n",
      "Epoch 2227/3000, Training Loss: 0.07539874225153702, Validation Loss: 0.21785848778179231\n",
      "Epoch 2228/3000, Training Loss: 0.07535841753062313, Validation Loss: 0.21783227251096315\n",
      "Epoch 2229/3000, Training Loss: 0.0753182138583578, Validation Loss: 0.21781807101459982\n",
      "Epoch 2230/3000, Training Loss: 0.07527758024670417, Validation Loss: 0.21779765063269452\n",
      "Epoch 2231/3000, Training Loss: 0.07523768735778877, Validation Loss: 0.21777632217970658\n",
      "Epoch 2232/3000, Training Loss: 0.07519713702468529, Validation Loss: 0.217759874557323\n",
      "Epoch 2233/3000, Training Loss: 0.07515723793130966, Validation Loss: 0.2177324378587074\n",
      "Epoch 2234/3000, Training Loss: 0.07511701749345467, Validation Loss: 0.21771379943310523\n",
      "Epoch 2235/3000, Training Loss: 0.07507652711886138, Validation Loss: 0.21769445254046368\n",
      "Epoch 2236/3000, Training Loss: 0.075036786973377, Validation Loss: 0.2176777426068317\n",
      "Epoch 2237/3000, Training Loss: 0.07499648779254968, Validation Loss: 0.2176512977150467\n",
      "Epoch 2238/3000, Training Loss: 0.07495673721804413, Validation Loss: 0.21763254041837854\n",
      "Epoch 2239/3000, Training Loss: 0.07491653758050712, Validation Loss: 0.21761262532292958\n",
      "Epoch 2240/3000, Training Loss: 0.07487676343583281, Validation Loss: 0.217594417294241\n",
      "Epoch 2241/3000, Training Loss: 0.07483679305261579, Validation Loss: 0.21757264270425086\n",
      "Epoch 2242/3000, Training Loss: 0.07479676050464555, Validation Loss: 0.217551661505189\n",
      "Epoch 2243/3000, Training Loss: 0.0747572257855599, Validation Loss: 0.21753097616086112\n",
      "Epoch 2244/3000, Training Loss: 0.07471721554449312, Validation Loss: 0.21751467037702202\n",
      "Epoch 2245/3000, Training Loss: 0.074677618059277, Validation Loss: 0.21749289561384247\n",
      "Epoch 2246/3000, Training Loss: 0.07463744197998497, Validation Loss: 0.21747406308627112\n",
      "Epoch 2247/3000, Training Loss: 0.07459825623440701, Validation Loss: 0.2174456685471806\n",
      "Epoch 2248/3000, Training Loss: 0.07455830192171732, Validation Loss: 0.21742925635013116\n",
      "Epoch 2249/3000, Training Loss: 0.0745185927129037, Validation Loss: 0.21741139908449006\n",
      "Epoch 2250/3000, Training Loss: 0.07447916281035658, Validation Loss: 0.2173950204757452\n",
      "Epoch 2251/3000, Training Loss: 0.07443932646153648, Validation Loss: 0.21736945615459852\n",
      "Epoch 2252/3000, Training Loss: 0.07440009661783152, Validation Loss: 0.21734757921254355\n",
      "Epoch 2253/3000, Training Loss: 0.07436045686690132, Validation Loss: 0.217328330346925\n",
      "Epoch 2254/3000, Training Loss: 0.07432096330173414, Validation Loss: 0.21730677265658702\n",
      "Epoch 2255/3000, Training Loss: 0.07428151524794938, Validation Loss: 0.21729648263215995\n",
      "Epoch 2256/3000, Training Loss: 0.0742419104659019, Validation Loss: 0.21726606851032704\n",
      "Epoch 2257/3000, Training Loss: 0.07420292880245231, Validation Loss: 0.21724833889644035\n",
      "Epoch 2258/3000, Training Loss: 0.07416311796634854, Validation Loss: 0.21723163192651831\n",
      "Epoch 2259/3000, Training Loss: 0.0741241737637029, Validation Loss: 0.21721120090736556\n",
      "Epoch 2260/3000, Training Loss: 0.07408463194425922, Validation Loss: 0.21719122889310155\n",
      "Epoch 2261/3000, Training Loss: 0.0740454219009646, Validation Loss: 0.21716561658809444\n",
      "Epoch 2262/3000, Training Loss: 0.07400620556441821, Validation Loss: 0.21715013863927296\n",
      "Epoch 2263/3000, Training Loss: 0.07396700056582772, Validation Loss: 0.21712962645872638\n",
      "Epoch 2264/3000, Training Loss: 0.07392785930802089, Validation Loss: 0.21711053043875433\n",
      "Epoch 2265/3000, Training Loss: 0.0738881877810136, Validation Loss: 0.217096053234074\n",
      "Epoch 2266/3000, Training Loss: 0.07384971881006239, Validation Loss: 0.21706807507919537\n",
      "Epoch 2267/3000, Training Loss: 0.0738103273138064, Validation Loss: 0.21704947380749273\n",
      "Epoch 2268/3000, Training Loss: 0.07377126713270482, Validation Loss: 0.21702906678573153\n",
      "Epoch 2269/3000, Training Loss: 0.07373202047969826, Validation Loss: 0.21701464306872925\n",
      "Epoch 2270/3000, Training Loss: 0.07369321902882858, Validation Loss: 0.2169874275814713\n",
      "Epoch 2271/3000, Training Loss: 0.07365436400988373, Validation Loss: 0.2169702919625587\n",
      "Epoch 2272/3000, Training Loss: 0.07361498071982366, Validation Loss: 0.2169526889869968\n",
      "Epoch 2273/3000, Training Loss: 0.07357647324932493, Validation Loss: 0.21693105296034507\n",
      "Epoch 2274/3000, Training Loss: 0.07353753009666411, Validation Loss: 0.21691456384779212\n",
      "Epoch 2275/3000, Training Loss: 0.07349857747216738, Validation Loss: 0.21689018811399766\n",
      "Epoch 2276/3000, Training Loss: 0.07346008585233431, Validation Loss: 0.2168755205506034\n",
      "Epoch 2277/3000, Training Loss: 0.07342112358368738, Validation Loss: 0.21685617333445742\n",
      "Epoch 2278/3000, Training Loss: 0.0733826645460833, Validation Loss: 0.2168386967363118\n",
      "Epoch 2279/3000, Training Loss: 0.07334359847479242, Validation Loss: 0.216819025105382\n",
      "Epoch 2280/3000, Training Loss: 0.07330540121502689, Validation Loss: 0.2167931779200115\n",
      "Epoch 2281/3000, Training Loss: 0.07326665348898559, Validation Loss: 0.21677792278540417\n",
      "Epoch 2282/3000, Training Loss: 0.0732280401604173, Validation Loss: 0.21675779181480204\n",
      "Epoch 2283/3000, Training Loss: 0.07318950483887308, Validation Loss: 0.21674274455911452\n",
      "Epoch 2284/3000, Training Loss: 0.07315086441588277, Validation Loss: 0.21672434997313256\n",
      "Epoch 2285/3000, Training Loss: 0.07311272745721475, Validation Loss: 0.2167014364321915\n",
      "Epoch 2286/3000, Training Loss: 0.07307390640923093, Validation Loss: 0.2166837481104238\n",
      "Epoch 2287/3000, Training Loss: 0.07303591205372092, Validation Loss: 0.21666380991595818\n",
      "Epoch 2288/3000, Training Loss: 0.07299706832868345, Validation Loss: 0.21664970111809748\n",
      "Epoch 2289/3000, Training Loss: 0.07295892135564108, Validation Loss: 0.21661877922391146\n",
      "Epoch 2290/3000, Training Loss: 0.07292078065610642, Validation Loss: 0.2166075364820449\n",
      "Epoch 2291/3000, Training Loss: 0.07288222202419378, Validation Loss: 0.21658904413475175\n",
      "Epoch 2292/3000, Training Loss: 0.07284414536485696, Validation Loss: 0.21657382287703994\n",
      "Epoch 2293/3000, Training Loss: 0.0728056905432509, Validation Loss: 0.21655521355278187\n",
      "Epoch 2294/3000, Training Loss: 0.07276774883405979, Validation Loss: 0.21652883687647434\n",
      "Epoch 2295/3000, Training Loss: 0.07272940557902478, Validation Loss: 0.2165109525796456\n",
      "Epoch 2296/3000, Training Loss: 0.07269133240710593, Validation Loss: 0.21649155389307922\n",
      "Epoch 2297/3000, Training Loss: 0.07265335861240159, Validation Loss: 0.21647968443769025\n",
      "Epoch 2298/3000, Training Loss: 0.07261488162553707, Validation Loss: 0.2164615466939118\n",
      "Epoch 2299/3000, Training Loss: 0.07257712621140096, Validation Loss: 0.2164330186209691\n",
      "Epoch 2300/3000, Training Loss: 0.07253901209101739, Validation Loss: 0.21641732113589973\n",
      "Epoch 2301/3000, Training Loss: 0.07250120954689562, Validation Loss: 0.21640184896498818\n",
      "Epoch 2302/3000, Training Loss: 0.07246284477986056, Validation Loss: 0.21638299627953614\n",
      "Epoch 2303/3000, Training Loss: 0.07242504791934985, Validation Loss: 0.2163638149917559\n",
      "Epoch 2304/3000, Training Loss: 0.07238716947781322, Validation Loss: 0.21634640434075988\n",
      "Epoch 2305/3000, Training Loss: 0.07234919157307826, Validation Loss: 0.21632383117289897\n",
      "Epoch 2306/3000, Training Loss: 0.07231130995864889, Validation Loss: 0.2163050874751193\n",
      "Epoch 2307/3000, Training Loss: 0.07227326134282853, Validation Loss: 0.21628803405164393\n",
      "Epoch 2308/3000, Training Loss: 0.07223576053630776, Validation Loss: 0.21627111611744493\n",
      "Epoch 2309/3000, Training Loss: 0.07219768534269513, Validation Loss: 0.21625006996059593\n",
      "Epoch 2310/3000, Training Loss: 0.07216029292522011, Validation Loss: 0.2162276941903962\n",
      "Epoch 2311/3000, Training Loss: 0.07212234849855442, Validation Loss: 0.21621830821428042\n",
      "Epoch 2312/3000, Training Loss: 0.07208460441278831, Validation Loss: 0.2161977842210309\n",
      "Epoch 2313/3000, Training Loss: 0.07204705976884533, Validation Loss: 0.21617409616977937\n",
      "Epoch 2314/3000, Training Loss: 0.07200929034431253, Validation Loss: 0.21615367981094824\n",
      "Epoch 2315/3000, Training Loss: 0.07197191004222463, Validation Loss: 0.21613787176971455\n",
      "Epoch 2316/3000, Training Loss: 0.07193404888274668, Validation Loss: 0.21612001517849158\n",
      "Epoch 2317/3000, Training Loss: 0.07189671430482926, Validation Loss: 0.21610091803530196\n",
      "Epoch 2318/3000, Training Loss: 0.0718589885198405, Validation Loss: 0.21608456430492579\n",
      "Epoch 2319/3000, Training Loss: 0.07182158164503939, Validation Loss: 0.2160675080842581\n",
      "Epoch 2320/3000, Training Loss: 0.07178424781036297, Validation Loss: 0.21604970695574058\n",
      "Epoch 2321/3000, Training Loss: 0.07174641540951386, Validation Loss: 0.2160289459759354\n",
      "Epoch 2322/3000, Training Loss: 0.07170930980382566, Validation Loss: 0.21601281257238675\n",
      "Epoch 2323/3000, Training Loss: 0.07167158727542869, Validation Loss: 0.2159898849252765\n",
      "Epoch 2324/3000, Training Loss: 0.071634718700296, Validation Loss: 0.2159747645522808\n",
      "Epoch 2325/3000, Training Loss: 0.07159705613233255, Validation Loss: 0.2159577593320371\n",
      "Epoch 2326/3000, Training Loss: 0.07155966681941314, Validation Loss: 0.21593701523573922\n",
      "Epoch 2327/3000, Training Loss: 0.07152265542868556, Validation Loss: 0.2159237415186177\n",
      "Epoch 2328/3000, Training Loss: 0.07148510795232027, Validation Loss: 0.21589769566856118\n",
      "Epoch 2329/3000, Training Loss: 0.07144811219578186, Validation Loss: 0.21588249896642317\n",
      "Epoch 2330/3000, Training Loss: 0.07141065464962763, Validation Loss: 0.21586631423865812\n",
      "Epoch 2331/3000, Training Loss: 0.07137375159952716, Validation Loss: 0.21585100138395832\n",
      "Epoch 2332/3000, Training Loss: 0.0713363053043098, Validation Loss: 0.2158305756770555\n",
      "Epoch 2333/3000, Training Loss: 0.07129931318605173, Validation Loss: 0.2158063584245316\n",
      "Epoch 2334/3000, Training Loss: 0.07126233278863092, Validation Loss: 0.215791805471794\n",
      "Epoch 2335/3000, Training Loss: 0.07122506607899111, Validation Loss: 0.21577408499597625\n",
      "Epoch 2336/3000, Training Loss: 0.07118799479692202, Validation Loss: 0.21575906539302145\n",
      "Epoch 2337/3000, Training Loss: 0.07115082852164785, Validation Loss: 0.21574739306724044\n",
      "Epoch 2338/3000, Training Loss: 0.0711143393160357, Validation Loss: 0.21572493082933575\n",
      "Epoch 2339/3000, Training Loss: 0.07107692331793457, Validation Loss: 0.21570911696560857\n",
      "Epoch 2340/3000, Training Loss: 0.07103998449804824, Validation Loss: 0.2156897869924143\n",
      "Epoch 2341/3000, Training Loss: 0.0710031443684185, Validation Loss: 0.21567336325135603\n",
      "Epoch 2342/3000, Training Loss: 0.07096607962774358, Validation Loss: 0.21565688221625914\n",
      "Epoch 2343/3000, Training Loss: 0.0709294275940033, Validation Loss: 0.21563670374359775\n",
      "Epoch 2344/3000, Training Loss: 0.07089233222309303, Validation Loss: 0.2156228863469831\n",
      "Epoch 2345/3000, Training Loss: 0.07085591653203714, Validation Loss: 0.21560696472955188\n",
      "Epoch 2346/3000, Training Loss: 0.07081878207240942, Validation Loss: 0.21559489254387812\n",
      "Epoch 2347/3000, Training Loss: 0.07078212190099689, Validation Loss: 0.21556918407398853\n",
      "Epoch 2348/3000, Training Loss: 0.07074553685306993, Validation Loss: 0.21554922625491055\n",
      "Epoch 2349/3000, Training Loss: 0.0707086246210839, Validation Loss: 0.2155338214282306\n",
      "Epoch 2350/3000, Training Loss: 0.07067210934525285, Validation Loss: 0.21551916109989194\n",
      "Epoch 2351/3000, Training Loss: 0.07063523986498103, Validation Loss: 0.21550576854531484\n",
      "Epoch 2352/3000, Training Loss: 0.0705988556585816, Validation Loss: 0.21548138403697734\n",
      "Epoch 2353/3000, Training Loss: 0.07056211109318838, Validation Loss: 0.2154718682388944\n",
      "Epoch 2354/3000, Training Loss: 0.07052579547388464, Validation Loss: 0.21545222624899654\n",
      "Epoch 2355/3000, Training Loss: 0.07048914337131411, Validation Loss: 0.2154347157301671\n",
      "Epoch 2356/3000, Training Loss: 0.070452391054131, Validation Loss: 0.2154188090361291\n",
      "Epoch 2357/3000, Training Loss: 0.07041615779958049, Validation Loss: 0.21540107809870926\n",
      "Epoch 2358/3000, Training Loss: 0.07037970451853486, Validation Loss: 0.21538400240125696\n",
      "Epoch 2359/3000, Training Loss: 0.07034330167434164, Validation Loss: 0.21536498955447686\n",
      "Epoch 2360/3000, Training Loss: 0.07030657125974973, Validation Loss: 0.21535349235738863\n",
      "Epoch 2361/3000, Training Loss: 0.07027068824073303, Validation Loss: 0.21533482239305965\n",
      "Epoch 2362/3000, Training Loss: 0.07023410904360679, Validation Loss: 0.21531610291322487\n",
      "Epoch 2363/3000, Training Loss: 0.0701978222226882, Validation Loss: 0.2153026687789986\n",
      "Epoch 2364/3000, Training Loss: 0.07016142111940075, Validation Loss: 0.21528512013470183\n",
      "Epoch 2365/3000, Training Loss: 0.0701253194413751, Validation Loss: 0.21526879066135104\n",
      "Epoch 2366/3000, Training Loss: 0.07008902009631823, Validation Loss: 0.21525263486405974\n",
      "Epoch 2367/3000, Training Loss: 0.07005257577576586, Validation Loss: 0.21522918904970317\n",
      "Epoch 2368/3000, Training Loss: 0.07001687491565425, Validation Loss: 0.21521473336291483\n",
      "Epoch 2369/3000, Training Loss: 0.06998047159918648, Validation Loss: 0.21520406437054038\n",
      "Epoch 2370/3000, Training Loss: 0.06994438011715609, Validation Loss: 0.21518817159745066\n",
      "Epoch 2371/3000, Training Loss: 0.06990821041376846, Validation Loss: 0.21517165117169146\n",
      "Epoch 2372/3000, Training Loss: 0.06987232252611568, Validation Loss: 0.21514887528538287\n",
      "Epoch 2373/3000, Training Loss: 0.06983632812406818, Validation Loss: 0.2151338934665672\n",
      "Epoch 2374/3000, Training Loss: 0.06980001572023195, Validation Loss: 0.2151158823665417\n",
      "Epoch 2375/3000, Training Loss: 0.06976430367096952, Validation Loss: 0.21509881554070726\n",
      "Epoch 2376/3000, Training Loss: 0.06972809011975001, Validation Loss: 0.2150845211838272\n",
      "Epoch 2377/3000, Training Loss: 0.06969247311342457, Validation Loss: 0.2150635116340333\n",
      "Epoch 2378/3000, Training Loss: 0.06965640738647917, Validation Loss: 0.21504748130373585\n",
      "Epoch 2379/3000, Training Loss: 0.0696204593477642, Validation Loss: 0.21503501861067043\n",
      "Epoch 2380/3000, Training Loss: 0.06958469191427198, Validation Loss: 0.21502058220469936\n",
      "Epoch 2381/3000, Training Loss: 0.06954870519437173, Validation Loss: 0.21499460963797637\n",
      "Epoch 2382/3000, Training Loss: 0.06951319476048995, Validation Loss: 0.2149833709211171\n",
      "Epoch 2383/3000, Training Loss: 0.06947704526004396, Validation Loss: 0.21497098415368834\n",
      "Epoch 2384/3000, Training Loss: 0.06944161250579832, Validation Loss: 0.21495330015172676\n",
      "Epoch 2385/3000, Training Loss: 0.06940563184716712, Validation Loss: 0.21493395003773194\n",
      "Epoch 2386/3000, Training Loss: 0.06937005447543892, Validation Loss: 0.21491229750897473\n",
      "Epoch 2387/3000, Training Loss: 0.06933436569205434, Validation Loss: 0.21489973144606447\n",
      "Epoch 2388/3000, Training Loss: 0.06929870138845538, Validation Loss: 0.2148819280103331\n",
      "Epoch 2389/3000, Training Loss: 0.06926319409673554, Validation Loss: 0.2148703269323635\n",
      "Epoch 2390/3000, Training Loss: 0.06922727265883957, Validation Loss: 0.21485704068673295\n",
      "Epoch 2391/3000, Training Loss: 0.06919208987596052, Validation Loss: 0.2148339434293948\n",
      "Epoch 2392/3000, Training Loss: 0.06915637349238199, Validation Loss: 0.2148164108066691\n",
      "Epoch 2393/3000, Training Loss: 0.06912104496755367, Validation Loss: 0.21480054471004317\n",
      "Epoch 2394/3000, Training Loss: 0.06908526634120489, Validation Loss: 0.2147858811739861\n",
      "Epoch 2395/3000, Training Loss: 0.06904995850303444, Validation Loss: 0.21477515829675567\n",
      "Epoch 2396/3000, Training Loss: 0.06901458601200229, Validation Loss: 0.2147541269717002\n",
      "Epoch 2397/3000, Training Loss: 0.06897907406043863, Validation Loss: 0.2147372526958036\n",
      "Epoch 2398/3000, Training Loss: 0.06894376175975386, Validation Loss: 0.2147229063916153\n",
      "Epoch 2399/3000, Training Loss: 0.06890812129935693, Validation Loss: 0.21470799136141624\n",
      "Epoch 2400/3000, Training Loss: 0.06887313228298719, Validation Loss: 0.2146914181266935\n",
      "Epoch 2401/3000, Training Loss: 0.06883752600727915, Validation Loss: 0.21466852288660906\n",
      "Epoch 2402/3000, Training Loss: 0.06880234808515236, Validation Loss: 0.21465671782148046\n",
      "Epoch 2403/3000, Training Loss: 0.0687670796442428, Validation Loss: 0.2146410370072092\n",
      "Epoch 2404/3000, Training Loss: 0.0687315986110243, Validation Loss: 0.21462450133661964\n",
      "Epoch 2405/3000, Training Loss: 0.06869645473245092, Validation Loss: 0.21460801081353828\n",
      "Epoch 2406/3000, Training Loss: 0.0686611330316558, Validation Loss: 0.21459072347111657\n",
      "Epoch 2407/3000, Training Loss: 0.06862641653621021, Validation Loss: 0.21457282843877482\n",
      "Epoch 2408/3000, Training Loss: 0.0685908175195121, Validation Loss: 0.21456231611318619\n",
      "Epoch 2409/3000, Training Loss: 0.06855586820014396, Validation Loss: 0.21454492031951095\n",
      "Epoch 2410/3000, Training Loss: 0.06852071977671384, Validation Loss: 0.21453356318963776\n",
      "Epoch 2411/3000, Training Loss: 0.06848577725990658, Validation Loss: 0.21450907308945122\n",
      "Epoch 2412/3000, Training Loss: 0.06845073384109868, Validation Loss: 0.21449401573797344\n",
      "Epoch 2413/3000, Training Loss: 0.0684154221990525, Validation Loss: 0.21448058477288826\n",
      "Epoch 2414/3000, Training Loss: 0.06838086332636198, Validation Loss: 0.21447144924840605\n",
      "Epoch 2415/3000, Training Loss: 0.06834560616711048, Validation Loss: 0.2144494008247952\n",
      "Epoch 2416/3000, Training Loss: 0.06831105576031621, Validation Loss: 0.21443539739683762\n",
      "Epoch 2417/3000, Training Loss: 0.06827576831176911, Validation Loss: 0.21442093854458075\n",
      "Epoch 2418/3000, Training Loss: 0.06824121480687331, Validation Loss: 0.21440126899669046\n",
      "Epoch 2419/3000, Training Loss: 0.06820619715682802, Validation Loss: 0.2143872551069132\n",
      "Epoch 2420/3000, Training Loss: 0.06817126200787105, Validation Loss: 0.2143712927385357\n",
      "Epoch 2421/3000, Training Loss: 0.068136749640729, Validation Loss: 0.214357678730921\n",
      "Epoch 2422/3000, Training Loss: 0.06810182349540468, Validation Loss: 0.21433710136716425\n",
      "Epoch 2423/3000, Training Loss: 0.06806732002349325, Validation Loss: 0.21432493834354535\n",
      "Epoch 2424/3000, Training Loss: 0.06803213166932187, Validation Loss: 0.21431254614971118\n",
      "Epoch 2425/3000, Training Loss: 0.06799790153380499, Validation Loss: 0.21429100921929028\n",
      "Epoch 2426/3000, Training Loss: 0.06796318646969009, Validation Loss: 0.2142806854317726\n",
      "Epoch 2427/3000, Training Loss: 0.06792855597572177, Validation Loss: 0.21426360452070817\n",
      "Epoch 2428/3000, Training Loss: 0.06789384839360392, Validation Loss: 0.21424820889088786\n",
      "Epoch 2429/3000, Training Loss: 0.06785924652718565, Validation Loss: 0.21423380580865958\n",
      "Epoch 2430/3000, Training Loss: 0.06782500855773568, Validation Loss: 0.2142122101498239\n",
      "Epoch 2431/3000, Training Loss: 0.06779008746237714, Validation Loss: 0.21419862927976527\n",
      "Epoch 2432/3000, Training Loss: 0.06775600385525238, Validation Loss: 0.21418555492314464\n",
      "Epoch 2433/3000, Training Loss: 0.06772140802560037, Validation Loss: 0.214176177261345\n",
      "Epoch 2434/3000, Training Loss: 0.0676869713997227, Validation Loss: 0.21415592798505872\n",
      "Epoch 2435/3000, Training Loss: 0.0676526340383931, Validation Loss: 0.21413517502655394\n",
      "Epoch 2436/3000, Training Loss: 0.06761824826266707, Validation Loss: 0.2141229299357467\n",
      "Epoch 2437/3000, Training Loss: 0.06758406119277198, Validation Loss: 0.21410818311833993\n",
      "Epoch 2438/3000, Training Loss: 0.06754943761819052, Validation Loss: 0.21409326062879758\n",
      "Epoch 2439/3000, Training Loss: 0.0675154719045373, Validation Loss: 0.21408008781077778\n",
      "Epoch 2440/3000, Training Loss: 0.06748096007982934, Validation Loss: 0.2140601397963131\n",
      "Epoch 2441/3000, Training Loss: 0.06744701832977945, Validation Loss: 0.21404136865430565\n",
      "Epoch 2442/3000, Training Loss: 0.06741246679730874, Validation Loss: 0.21403011231916944\n",
      "Epoch 2443/3000, Training Loss: 0.06737856682331045, Validation Loss: 0.21401256051166404\n",
      "Epoch 2444/3000, Training Loss: 0.06734441815676462, Validation Loss: 0.2140043225419412\n",
      "Epoch 2445/3000, Training Loss: 0.06731022961123209, Validation Loss: 0.21397809740218926\n",
      "Epoch 2446/3000, Training Loss: 0.06727640103747917, Validation Loss: 0.2139647643406\n",
      "Epoch 2447/3000, Training Loss: 0.0672420009157516, Validation Loss: 0.2139487941407111\n",
      "Epoch 2448/3000, Training Loss: 0.06720832706429233, Validation Loss: 0.2139340502597515\n",
      "Epoch 2449/3000, Training Loss: 0.06717390927051688, Validation Loss: 0.21391884650129384\n",
      "Epoch 2450/3000, Training Loss: 0.0671403037415182, Validation Loss: 0.21389918016587875\n",
      "Epoch 2451/3000, Training Loss: 0.0671061778198809, Validation Loss: 0.21388501646074734\n",
      "Epoch 2452/3000, Training Loss: 0.06707242484538163, Validation Loss: 0.2138724353380601\n",
      "Epoch 2453/3000, Training Loss: 0.06703850727476689, Validation Loss: 0.21385441834719746\n",
      "Epoch 2454/3000, Training Loss: 0.06700438750022185, Validation Loss: 0.21383258068591487\n",
      "Epoch 2455/3000, Training Loss: 0.06697094541217657, Validation Loss: 0.21381898451467235\n",
      "Epoch 2456/3000, Training Loss: 0.0669368543813439, Validation Loss: 0.21380838616744907\n",
      "Epoch 2457/3000, Training Loss: 0.06690336139015875, Validation Loss: 0.21378953688262756\n",
      "Epoch 2458/3000, Training Loss: 0.06686924260131955, Validation Loss: 0.21377470540962934\n",
      "Epoch 2459/3000, Training Loss: 0.06683575716896403, Validation Loss: 0.21375579395675534\n",
      "Epoch 2460/3000, Training Loss: 0.06680199890936338, Validation Loss: 0.21374005365760423\n",
      "Epoch 2461/3000, Training Loss: 0.06676828886845478, Validation Loss: 0.21372137010595016\n",
      "Epoch 2462/3000, Training Loss: 0.06673468380797894, Validation Loss: 0.21371613693990926\n",
      "Epoch 2463/3000, Training Loss: 0.06670101975541917, Validation Loss: 0.21369909831640257\n",
      "Epoch 2464/3000, Training Loss: 0.06666740879803844, Validation Loss: 0.21367647510571294\n",
      "Epoch 2465/3000, Training Loss: 0.06663373257067909, Validation Loss: 0.21365821836327056\n",
      "Epoch 2466/3000, Training Loss: 0.06660050474312715, Validation Loss: 0.21364605437980236\n",
      "Epoch 2467/3000, Training Loss: 0.06656657895917463, Validation Loss: 0.21363068125803827\n",
      "Epoch 2468/3000, Training Loss: 0.06653324487276464, Validation Loss: 0.213620066716723\n",
      "Epoch 2469/3000, Training Loss: 0.06649982091089253, Validation Loss: 0.2135973072310048\n",
      "Epoch 2470/3000, Training Loss: 0.06646632471544753, Validation Loss: 0.2135839987908291\n",
      "Epoch 2471/3000, Training Loss: 0.06643293623553705, Validation Loss: 0.21357075772558512\n",
      "Epoch 2472/3000, Training Loss: 0.06639940617604659, Validation Loss: 0.21355455786366742\n",
      "Epoch 2473/3000, Training Loss: 0.06636609842786503, Validation Loss: 0.21353583317834413\n",
      "Epoch 2474/3000, Training Loss: 0.06633252341342939, Validation Loss: 0.21352022040653557\n",
      "Epoch 2475/3000, Training Loss: 0.06629968410323296, Validation Loss: 0.21350593156108252\n",
      "Epoch 2476/3000, Training Loss: 0.06626573083843754, Validation Loss: 0.21349346433334201\n",
      "Epoch 2477/3000, Training Loss: 0.06623234676802592, Validation Loss: 0.21347588232715956\n",
      "Epoch 2478/3000, Training Loss: 0.06619886664425885, Validation Loss: 0.2134639981419158\n",
      "Epoch 2479/3000, Training Loss: 0.06616536276987438, Validation Loss: 0.21344322853389125\n",
      "Epoch 2480/3000, Training Loss: 0.06613202085553238, Validation Loss: 0.213435230869709\n",
      "Epoch 2481/3000, Training Loss: 0.0660984163675545, Validation Loss: 0.2134190297745932\n",
      "Epoch 2482/3000, Training Loss: 0.06606525619946046, Validation Loss: 0.21340614739372352\n",
      "Epoch 2483/3000, Training Loss: 0.06603152462847316, Validation Loss: 0.21338908381041097\n",
      "Epoch 2484/3000, Training Loss: 0.06599846729678738, Validation Loss: 0.2133709680275803\n",
      "Epoch 2485/3000, Training Loss: 0.06596499297314423, Validation Loss: 0.2133562493688836\n",
      "Epoch 2486/3000, Training Loss: 0.06593190044159136, Validation Loss: 0.2133452870884325\n",
      "Epoch 2487/3000, Training Loss: 0.06589839965663526, Validation Loss: 0.21333368198527633\n",
      "Epoch 2488/3000, Training Loss: 0.06586525579103276, Validation Loss: 0.2133170472272538\n",
      "Epoch 2489/3000, Training Loss: 0.06583213069827978, Validation Loss: 0.2133017270337799\n",
      "Epoch 2490/3000, Training Loss: 0.06579878535816133, Validation Loss: 0.21328258865736555\n",
      "Epoch 2491/3000, Training Loss: 0.06576579598010476, Validation Loss: 0.21327014686935916\n",
      "Epoch 2492/3000, Training Loss: 0.06573234164287042, Validation Loss: 0.21326108885297448\n",
      "Epoch 2493/3000, Training Loss: 0.06569948280426836, Validation Loss: 0.2132487108853786\n",
      "Epoch 2494/3000, Training Loss: 0.06566631492696447, Validation Loss: 0.21322610384204288\n",
      "Epoch 2495/3000, Training Loss: 0.06563340098769309, Validation Loss: 0.21321180127997263\n",
      "Epoch 2496/3000, Training Loss: 0.06560020243859835, Validation Loss: 0.21319912296273766\n",
      "Epoch 2497/3000, Training Loss: 0.06556715028071308, Validation Loss: 0.21318509891449153\n",
      "Epoch 2498/3000, Training Loss: 0.06553428739330366, Validation Loss: 0.21317526234763529\n",
      "Epoch 2499/3000, Training Loss: 0.06550128237129116, Validation Loss: 0.213155373741616\n",
      "Epoch 2500/3000, Training Loss: 0.06546846771348762, Validation Loss: 0.21314337150851345\n",
      "Epoch 2501/3000, Training Loss: 0.06543543641396013, Validation Loss: 0.21312771927572213\n",
      "Epoch 2502/3000, Training Loss: 0.06540269236673878, Validation Loss: 0.21311083467415143\n",
      "Epoch 2503/3000, Training Loss: 0.06536939463383504, Validation Loss: 0.21309518421123658\n",
      "Epoch 2504/3000, Training Loss: 0.06533719291429227, Validation Loss: 0.21308353885852158\n",
      "Epoch 2505/3000, Training Loss: 0.06530408298399543, Validation Loss: 0.21306980700647443\n",
      "Epoch 2506/3000, Training Loss: 0.0652713467334104, Validation Loss: 0.21305244468548148\n",
      "Epoch 2507/3000, Training Loss: 0.06523849969112937, Validation Loss: 0.21304432162204195\n",
      "Epoch 2508/3000, Training Loss: 0.06520579956913564, Validation Loss: 0.2130209967390646\n",
      "Epoch 2509/3000, Training Loss: 0.06517334908083856, Validation Loss: 0.21300768972380854\n",
      "Epoch 2510/3000, Training Loss: 0.0651402824165721, Validation Loss: 0.212996173005055\n",
      "Epoch 2511/3000, Training Loss: 0.06510800976180553, Validation Loss: 0.21298435659052348\n",
      "Epoch 2512/3000, Training Loss: 0.06507503657242679, Validation Loss: 0.21296953765881285\n",
      "Epoch 2513/3000, Training Loss: 0.06504289130839623, Validation Loss: 0.21294804003228637\n",
      "Epoch 2514/3000, Training Loss: 0.06500995573655861, Validation Loss: 0.21293431822622588\n",
      "Epoch 2515/3000, Training Loss: 0.06497763199516095, Validation Loss: 0.21292078982962848\n",
      "Epoch 2516/3000, Training Loss: 0.06494492112222229, Validation Loss: 0.2129125067240588\n",
      "Epoch 2517/3000, Training Loss: 0.06491242533837725, Validation Loss: 0.21289922356760083\n",
      "Epoch 2518/3000, Training Loss: 0.06488012470091013, Validation Loss: 0.21287875296793643\n",
      "Epoch 2519/3000, Training Loss: 0.06484757861448205, Validation Loss: 0.21286366509533694\n",
      "Epoch 2520/3000, Training Loss: 0.06481530890323235, Validation Loss: 0.21284945665524543\n",
      "Epoch 2521/3000, Training Loss: 0.06478255205336546, Validation Loss: 0.21283555828282394\n",
      "Epoch 2522/3000, Training Loss: 0.06475050689753366, Validation Loss: 0.2128183109264916\n",
      "Epoch 2523/3000, Training Loss: 0.06471789644750484, Validation Loss: 0.21280557370184536\n",
      "Epoch 2524/3000, Training Loss: 0.06468588836580458, Validation Loss: 0.2127909322420359\n",
      "Epoch 2525/3000, Training Loss: 0.06465328106932434, Validation Loss: 0.2127789576774609\n",
      "Epoch 2526/3000, Training Loss: 0.06462107639478999, Validation Loss: 0.21276457234456503\n",
      "Epoch 2527/3000, Training Loss: 0.06458887198629598, Validation Loss: 0.2127447348672933\n",
      "Epoch 2528/3000, Training Loss: 0.06455672334507204, Validation Loss: 0.21273508972167135\n",
      "Epoch 2529/3000, Training Loss: 0.06452448681608468, Validation Loss: 0.21272226029190464\n",
      "Epoch 2530/3000, Training Loss: 0.0644921541537819, Validation Loss: 0.21270553755538957\n",
      "Epoch 2531/3000, Training Loss: 0.0644601679666049, Validation Loss: 0.21269321057305163\n",
      "Epoch 2532/3000, Training Loss: 0.06442758590248766, Validation Loss: 0.2126709543149449\n",
      "Epoch 2533/3000, Training Loss: 0.06439565344208147, Validation Loss: 0.21265742840446714\n",
      "Epoch 2534/3000, Training Loss: 0.0643632119932794, Validation Loss: 0.21264734739953445\n",
      "Epoch 2535/3000, Training Loss: 0.06433129408886179, Validation Loss: 0.2126325948275274\n",
      "Epoch 2536/3000, Training Loss: 0.06429891142043544, Validation Loss: 0.21261869196518987\n",
      "Epoch 2537/3000, Training Loss: 0.06426701644577493, Validation Loss: 0.21260014799334492\n",
      "Epoch 2538/3000, Training Loss: 0.0642348300974471, Validation Loss: 0.21258685665348884\n",
      "Epoch 2539/3000, Training Loss: 0.06420278163191158, Validation Loss: 0.2125693674240482\n",
      "Epoch 2540/3000, Training Loss: 0.06417067215090544, Validation Loss: 0.21255918335674182\n",
      "Epoch 2541/3000, Training Loss: 0.06413853091751032, Validation Loss: 0.2125393336670851\n",
      "Epoch 2542/3000, Training Loss: 0.06410694100489021, Validation Loss: 0.21252356928659688\n",
      "Epoch 2543/3000, Training Loss: 0.06407456384798374, Validation Loss: 0.21250882587478429\n",
      "Epoch 2544/3000, Training Loss: 0.06404285923811703, Validation Loss: 0.2124950341033714\n",
      "Epoch 2545/3000, Training Loss: 0.06401075168264725, Validation Loss: 0.2124837446976626\n",
      "Epoch 2546/3000, Training Loss: 0.06397909958488891, Validation Loss: 0.21246714045858306\n",
      "Epoch 2547/3000, Training Loss: 0.06394711679851955, Validation Loss: 0.21245153953719179\n",
      "Epoch 2548/3000, Training Loss: 0.0639152737541015, Validation Loss: 0.21243800317860367\n",
      "Epoch 2549/3000, Training Loss: 0.06388339326418355, Validation Loss: 0.2124252652969649\n",
      "Epoch 2550/3000, Training Loss: 0.06385159967128655, Validation Loss: 0.2124084933672103\n",
      "Epoch 2551/3000, Training Loss: 0.06381984314597844, Validation Loss: 0.2123897216409205\n",
      "Epoch 2552/3000, Training Loss: 0.06378794320799093, Validation Loss: 0.21238098484239187\n",
      "Epoch 2553/3000, Training Loss: 0.06375645902402773, Validation Loss: 0.21236446072665227\n",
      "Epoch 2554/3000, Training Loss: 0.06372447241795742, Validation Loss: 0.21234894587044084\n",
      "Epoch 2555/3000, Training Loss: 0.06369295013619265, Validation Loss: 0.21233237751608652\n",
      "Epoch 2556/3000, Training Loss: 0.06366120053811346, Validation Loss: 0.21231936135804363\n",
      "Epoch 2557/3000, Training Loss: 0.06362982382940659, Validation Loss: 0.21230560419257896\n",
      "Epoch 2558/3000, Training Loss: 0.0635980074939642, Validation Loss: 0.21229582522592247\n",
      "Epoch 2559/3000, Training Loss: 0.06356636573676984, Validation Loss: 0.21228055968377127\n",
      "Epoch 2560/3000, Training Loss: 0.06353497929640005, Validation Loss: 0.21226143997978308\n",
      "Epoch 2561/3000, Training Loss: 0.06350335170899336, Validation Loss: 0.21224314502348102\n",
      "Epoch 2562/3000, Training Loss: 0.06347195042783851, Validation Loss: 0.21222951442205049\n",
      "Epoch 2563/3000, Training Loss: 0.06344015692219852, Validation Loss: 0.21221906475038038\n",
      "Epoch 2564/3000, Training Loss: 0.06340911858957045, Validation Loss: 0.21220392673177055\n",
      "Epoch 2565/3000, Training Loss: 0.0633774017177429, Validation Loss: 0.21218848115441646\n",
      "Epoch 2566/3000, Training Loss: 0.06334631302210643, Validation Loss: 0.2121763415865234\n",
      "Epoch 2567/3000, Training Loss: 0.06331455332695675, Validation Loss: 0.21216278927544033\n",
      "Epoch 2568/3000, Training Loss: 0.06328356514408333, Validation Loss: 0.2121473526460235\n",
      "Epoch 2569/3000, Training Loss: 0.06325180711278244, Validation Loss: 0.2121273771051914\n",
      "Epoch 2570/3000, Training Loss: 0.06322089556409347, Validation Loss: 0.21211805743068146\n",
      "Epoch 2571/3000, Training Loss: 0.0631894208887234, Validation Loss: 0.21210487902167835\n",
      "Epoch 2572/3000, Training Loss: 0.06315817284825274, Validation Loss: 0.2120894365268648\n",
      "Epoch 2573/3000, Training Loss: 0.06312678058673064, Validation Loss: 0.21207472083416068\n",
      "Epoch 2574/3000, Training Loss: 0.06309566899617333, Validation Loss: 0.21205560961782247\n",
      "Epoch 2575/3000, Training Loss: 0.06306454318065546, Validation Loss: 0.2120448733872666\n",
      "Epoch 2576/3000, Training Loss: 0.063033144499701, Validation Loss: 0.2120316310144479\n",
      "Epoch 2577/3000, Training Loss: 0.06300225138898659, Validation Loss: 0.21201773061993892\n",
      "Epoch 2578/3000, Training Loss: 0.06297086775721418, Validation Loss: 0.21200486638987767\n",
      "Epoch 2579/3000, Training Loss: 0.06294010185213011, Validation Loss: 0.21198588325712087\n",
      "Epoch 2580/3000, Training Loss: 0.06290853956953125, Validation Loss: 0.21197183741125064\n",
      "Epoch 2581/3000, Training Loss: 0.06287804723724745, Validation Loss: 0.21195562989422356\n",
      "Epoch 2582/3000, Training Loss: 0.0628465860453053, Validation Loss: 0.21194329604030868\n",
      "Epoch 2583/3000, Training Loss: 0.0628158458420303, Validation Loss: 0.21192865719646672\n",
      "Epoch 2584/3000, Training Loss: 0.06278461463769726, Validation Loss: 0.21191293484464685\n",
      "Epoch 2585/3000, Training Loss: 0.06275389232505685, Validation Loss: 0.21190286124586355\n",
      "Epoch 2586/3000, Training Loss: 0.06272271878590757, Validation Loss: 0.21188962734689712\n",
      "Epoch 2587/3000, Training Loss: 0.06269181495647859, Validation Loss: 0.2118735858462078\n",
      "Epoch 2588/3000, Training Loss: 0.0626610292236236, Validation Loss: 0.2118555597131219\n",
      "Epoch 2589/3000, Training Loss: 0.06263018889299508, Validation Loss: 0.21184576824548873\n",
      "Epoch 2590/3000, Training Loss: 0.06259934883960967, Validation Loss: 0.21183020553823179\n",
      "Epoch 2591/3000, Training Loss: 0.06256831625404671, Validation Loss: 0.21181697177073155\n",
      "Epoch 2592/3000, Training Loss: 0.06253770785992144, Validation Loss: 0.21179780767927692\n",
      "Epoch 2593/3000, Training Loss: 0.06250690641978175, Validation Loss: 0.211782970634862\n",
      "Epoch 2594/3000, Training Loss: 0.06247629425390275, Validation Loss: 0.21177136040643466\n",
      "Epoch 2595/3000, Training Loss: 0.06244524874825938, Validation Loss: 0.21176106496757877\n",
      "Epoch 2596/3000, Training Loss: 0.062414761165909445, Validation Loss: 0.21175042494255783\n",
      "Epoch 2597/3000, Training Loss: 0.06238401568085511, Validation Loss: 0.21173100734805145\n",
      "Epoch 2598/3000, Training Loss: 0.06235352088527356, Validation Loss: 0.21171619150080562\n",
      "Epoch 2599/3000, Training Loss: 0.06232263480174855, Validation Loss: 0.21170202925848763\n",
      "Epoch 2600/3000, Training Loss: 0.06229212527461539, Validation Loss: 0.21169027563526474\n",
      "Epoch 2601/3000, Training Loss: 0.06226148060198875, Validation Loss: 0.21167508133139398\n",
      "Epoch 2602/3000, Training Loss: 0.06223107499829931, Validation Loss: 0.21166018986605503\n",
      "Epoch 2603/3000, Training Loss: 0.06220033084108658, Validation Loss: 0.21164794361850692\n",
      "Epoch 2604/3000, Training Loss: 0.06216981222445088, Validation Loss: 0.2116346633563379\n",
      "Epoch 2605/3000, Training Loss: 0.06213952244069838, Validation Loss: 0.21162370034169367\n",
      "Epoch 2606/3000, Training Loss: 0.06210869586959123, Validation Loss: 0.21160264859081201\n",
      "Epoch 2607/3000, Training Loss: 0.06207854034022307, Validation Loss: 0.2115903105317214\n",
      "Epoch 2608/3000, Training Loss: 0.0620479293787075, Validation Loss: 0.2115803397455903\n",
      "Epoch 2609/3000, Training Loss: 0.06201785615739317, Validation Loss: 0.21156709461135637\n",
      "Epoch 2610/3000, Training Loss: 0.061986888415058686, Validation Loss: 0.21155447236346286\n",
      "Epoch 2611/3000, Training Loss: 0.06195687622101061, Validation Loss: 0.2115359657116886\n",
      "Epoch 2612/3000, Training Loss: 0.061926499596759284, Validation Loss: 0.21152278260741517\n",
      "Epoch 2613/3000, Training Loss: 0.061896183984944314, Validation Loss: 0.21150854629938473\n",
      "Epoch 2614/3000, Training Loss: 0.061865633171771635, Validation Loss: 0.21150319027783593\n",
      "Epoch 2615/3000, Training Loss: 0.06183555413676707, Validation Loss: 0.21148362428284181\n",
      "Epoch 2616/3000, Training Loss: 0.061805464449761746, Validation Loss: 0.21147081858020744\n",
      "Epoch 2617/3000, Training Loss: 0.061774977159132674, Validation Loss: 0.2114540994627408\n",
      "Epoch 2618/3000, Training Loss: 0.06174475248607591, Validation Loss: 0.21144440954387003\n",
      "Epoch 2619/3000, Training Loss: 0.06171452910783852, Validation Loss: 0.21143043751940005\n",
      "Epoch 2620/3000, Training Loss: 0.061684559694290306, Validation Loss: 0.2114100247473197\n",
      "Epoch 2621/3000, Training Loss: 0.06165418445755482, Validation Loss: 0.21140064040860987\n",
      "Epoch 2622/3000, Training Loss: 0.06162426569511052, Validation Loss: 0.2113911573307595\n",
      "Epoch 2623/3000, Training Loss: 0.06159400225645474, Validation Loss: 0.21137510480094723\n",
      "Epoch 2624/3000, Training Loss: 0.0615641372188507, Validation Loss: 0.21135916901742677\n",
      "Epoch 2625/3000, Training Loss: 0.06153384666076546, Validation Loss: 0.21134596554797463\n",
      "Epoch 2626/3000, Training Loss: 0.061504302647217145, Validation Loss: 0.21133389860150423\n",
      "Epoch 2627/3000, Training Loss: 0.06147386358418851, Validation Loss: 0.2113255816318523\n",
      "Epoch 2628/3000, Training Loss: 0.061444175614311335, Validation Loss: 0.21130921877725797\n",
      "Epoch 2629/3000, Training Loss: 0.061414047346718716, Validation Loss: 0.21129382258299256\n",
      "Epoch 2630/3000, Training Loss: 0.06138441758349673, Validation Loss: 0.21128027574482125\n",
      "Epoch 2631/3000, Training Loss: 0.06135423149631738, Validation Loss: 0.21126726260940865\n",
      "Epoch 2632/3000, Training Loss: 0.06132450413202939, Validation Loss: 0.21125464455967544\n",
      "Epoch 2633/3000, Training Loss: 0.061294634715987006, Validation Loss: 0.21123692371126065\n",
      "Epoch 2634/3000, Training Loss: 0.06126488139076324, Validation Loss: 0.21123010607470286\n",
      "Epoch 2635/3000, Training Loss: 0.061235070021890915, Validation Loss: 0.21121719162197375\n",
      "Epoch 2636/3000, Training Loss: 0.06120523512288486, Validation Loss: 0.21120297917508668\n",
      "Epoch 2637/3000, Training Loss: 0.061175610426774704, Validation Loss: 0.21119233650929417\n",
      "Epoch 2638/3000, Training Loss: 0.06114567819706032, Validation Loss: 0.21117358287414595\n",
      "Epoch 2639/3000, Training Loss: 0.06111619579564127, Validation Loss: 0.21116041669114213\n",
      "Epoch 2640/3000, Training Loss: 0.061086255376233085, Validation Loss: 0.21115174040807436\n",
      "Epoch 2641/3000, Training Loss: 0.06105683410166762, Validation Loss: 0.2111389300957412\n",
      "Epoch 2642/3000, Training Loss: 0.06102690951578778, Validation Loss: 0.21112054280905182\n",
      "Epoch 2643/3000, Training Loss: 0.0609976145056015, Validation Loss: 0.21111189349439616\n",
      "Epoch 2644/3000, Training Loss: 0.06096771852300175, Validation Loss: 0.2111011790231488\n",
      "Epoch 2645/3000, Training Loss: 0.060938539547133484, Validation Loss: 0.21108729117915642\n",
      "Epoch 2646/3000, Training Loss: 0.06090849954739144, Validation Loss: 0.21108160057585548\n",
      "Epoch 2647/3000, Training Loss: 0.0608793610291139, Validation Loss: 0.21105996165289231\n",
      "Epoch 2648/3000, Training Loss: 0.06084961606050899, Validation Loss: 0.21105045303474215\n",
      "Epoch 2649/3000, Training Loss: 0.06082025686273355, Validation Loss: 0.2110352283282665\n",
      "Epoch 2650/3000, Training Loss: 0.06079062135950915, Validation Loss: 0.21102711156834125\n",
      "Epoch 2651/3000, Training Loss: 0.06076113495999029, Validation Loss: 0.21100646316503197\n",
      "Epoch 2652/3000, Training Loss: 0.060731940516092854, Validation Loss: 0.21099518205022424\n",
      "Epoch 2653/3000, Training Loss: 0.060702530608846304, Validation Loss: 0.2109903560507983\n",
      "Epoch 2654/3000, Training Loss: 0.06067300440697575, Validation Loss: 0.21097926677766832\n",
      "Epoch 2655/3000, Training Loss: 0.06064355308121402, Validation Loss: 0.21096370703993128\n",
      "Epoch 2656/3000, Training Loss: 0.060614542180992874, Validation Loss: 0.21094712688979825\n",
      "Epoch 2657/3000, Training Loss: 0.06058504597082986, Validation Loss: 0.21093506029689807\n",
      "Epoch 2658/3000, Training Loss: 0.06055569394584225, Validation Loss: 0.21092415018692887\n",
      "Epoch 2659/3000, Training Loss: 0.06052634272516205, Validation Loss: 0.2109110697869189\n",
      "Epoch 2660/3000, Training Loss: 0.06049740112117468, Validation Loss: 0.21089844445730394\n",
      "Epoch 2661/3000, Training Loss: 0.06046790458285684, Validation Loss: 0.21088557721686868\n",
      "Epoch 2662/3000, Training Loss: 0.0604388635668635, Validation Loss: 0.21087440477916022\n",
      "Epoch 2663/3000, Training Loss: 0.06040944984095018, Validation Loss: 0.2108637754642114\n",
      "Epoch 2664/3000, Training Loss: 0.06038066223738351, Validation Loss: 0.21085288778156827\n",
      "Epoch 2665/3000, Training Loss: 0.060351081127304, Validation Loss: 0.21083716408775915\n",
      "Epoch 2666/3000, Training Loss: 0.06032232120334075, Validation Loss: 0.21082751615277356\n",
      "Epoch 2667/3000, Training Loss: 0.0602930790765021, Validation Loss: 0.2108179722763893\n",
      "Epoch 2668/3000, Training Loss: 0.06026410240973345, Validation Loss: 0.2108046694189441\n",
      "Epoch 2669/3000, Training Loss: 0.06023473901112831, Validation Loss: 0.21078633053626333\n",
      "Epoch 2670/3000, Training Loss: 0.06020594467943625, Validation Loss: 0.2107764597057521\n",
      "Epoch 2671/3000, Training Loss: 0.06017691746255755, Validation Loss: 0.21076524754534995\n",
      "Epoch 2672/3000, Training Loss: 0.06014795392257856, Validation Loss: 0.21075658204834488\n",
      "Epoch 2673/3000, Training Loss: 0.06011894769004499, Validation Loss: 0.21074432073988644\n",
      "Epoch 2674/3000, Training Loss: 0.060090014522849344, Validation Loss: 0.21073037306153714\n",
      "Epoch 2675/3000, Training Loss: 0.06006123004399023, Validation Loss: 0.21071590564025605\n",
      "Epoch 2676/3000, Training Loss: 0.06003214900056379, Validation Loss: 0.21070306130074354\n",
      "Epoch 2677/3000, Training Loss: 0.060003265859673915, Validation Loss: 0.21069682265873466\n",
      "Epoch 2678/3000, Training Loss: 0.05997424810127473, Validation Loss: 0.21067779677339324\n",
      "Epoch 2679/3000, Training Loss: 0.05994558940609932, Validation Loss: 0.21067074466988234\n",
      "Epoch 2680/3000, Training Loss: 0.059916398406163446, Validation Loss: 0.21065995387093933\n",
      "Epoch 2681/3000, Training Loss: 0.05988765764497799, Validation Loss: 0.21065012866187946\n",
      "Epoch 2682/3000, Training Loss: 0.05985865312794493, Validation Loss: 0.21064183469759112\n",
      "Epoch 2683/3000, Training Loss: 0.05983018667598642, Validation Loss: 0.21062479564645048\n",
      "Epoch 2684/3000, Training Loss: 0.05980088445473093, Validation Loss: 0.21061165116848599\n",
      "Epoch 2685/3000, Training Loss: 0.05977254160736243, Validation Loss: 0.21060528430882602\n",
      "Epoch 2686/3000, Training Loss: 0.05974330891823096, Validation Loss: 0.21059839503394265\n",
      "Epoch 2687/3000, Training Loss: 0.05971514806501646, Validation Loss: 0.2105798191797358\n",
      "Epoch 2688/3000, Training Loss: 0.05968594706894801, Validation Loss: 0.21057026936484433\n",
      "Epoch 2689/3000, Training Loss: 0.05965776299553073, Validation Loss: 0.21055749220247144\n",
      "Epoch 2690/3000, Training Loss: 0.0596286261277519, Validation Loss: 0.21054511254546543\n",
      "Epoch 2691/3000, Training Loss: 0.0596002006899432, Validation Loss: 0.2105330584702563\n",
      "Epoch 2692/3000, Training Loss: 0.05957167099604167, Validation Loss: 0.21052017622150182\n",
      "Epoch 2693/3000, Training Loss: 0.0595432020800804, Validation Loss: 0.21050913731275553\n",
      "Epoch 2694/3000, Training Loss: 0.059514327692576134, Validation Loss: 0.21049777203837525\n",
      "Epoch 2695/3000, Training Loss: 0.0594859166843197, Validation Loss: 0.2104884612572698\n",
      "Epoch 2696/3000, Training Loss: 0.05945751851776413, Validation Loss: 0.21046966407787898\n",
      "Epoch 2697/3000, Training Loss: 0.05942891162945352, Validation Loss: 0.21045657376698657\n",
      "Epoch 2698/3000, Training Loss: 0.05940031044255959, Validation Loss: 0.2104493335663187\n",
      "Epoch 2699/3000, Training Loss: 0.05937206420135181, Validation Loss: 0.21043379206191581\n",
      "Epoch 2700/3000, Training Loss: 0.05934345207599131, Validation Loss: 0.21042281569803478\n",
      "Epoch 2701/3000, Training Loss: 0.05931506272004162, Validation Loss: 0.21041144121571909\n",
      "Epoch 2702/3000, Training Loss: 0.0592867862324773, Validation Loss: 0.21040199590665332\n",
      "Epoch 2703/3000, Training Loss: 0.059258390211072626, Validation Loss: 0.21038831631123733\n",
      "Epoch 2704/3000, Training Loss: 0.05922988284822952, Validation Loss: 0.21037884652452346\n",
      "Epoch 2705/3000, Training Loss: 0.05920162833023496, Validation Loss: 0.21036058445772265\n",
      "Epoch 2706/3000, Training Loss: 0.05917343415503277, Validation Loss: 0.2103499711472335\n",
      "Epoch 2707/3000, Training Loss: 0.059144991267602885, Validation Loss: 0.21034182273037424\n",
      "Epoch 2708/3000, Training Loss: 0.05911687263085012, Validation Loss: 0.21033033858974992\n",
      "Epoch 2709/3000, Training Loss: 0.059088498224888784, Validation Loss: 0.21031326948080992\n",
      "Epoch 2710/3000, Training Loss: 0.05906043029679483, Validation Loss: 0.21030243321534497\n",
      "Epoch 2711/3000, Training Loss: 0.05903203720399181, Validation Loss: 0.21029190151602722\n",
      "Epoch 2712/3000, Training Loss: 0.059004141314792084, Validation Loss: 0.2102831406094735\n",
      "Epoch 2713/3000, Training Loss: 0.05897569137193955, Validation Loss: 0.2102715108017088\n",
      "Epoch 2714/3000, Training Loss: 0.05894773552221454, Validation Loss: 0.21025594677999707\n",
      "Epoch 2715/3000, Training Loss: 0.05891954428459552, Validation Loss: 0.21024953902220161\n",
      "Epoch 2716/3000, Training Loss: 0.05889166827235327, Validation Loss: 0.21023596579838966\n",
      "Epoch 2717/3000, Training Loss: 0.058863143441170336, Validation Loss: 0.21022402153485026\n",
      "Epoch 2718/3000, Training Loss: 0.05883541569004664, Validation Loss: 0.210208228381579\n",
      "Epoch 2719/3000, Training Loss: 0.05880728155276384, Validation Loss: 0.21019816955182383\n",
      "Epoch 2720/3000, Training Loss: 0.058779418178632574, Validation Loss: 0.2101850561653359\n",
      "Epoch 2721/3000, Training Loss: 0.058751142932448215, Validation Loss: 0.21018165132681874\n",
      "Epoch 2722/3000, Training Loss: 0.05872341646467405, Validation Loss: 0.2101665143104123\n",
      "Epoch 2723/3000, Training Loss: 0.05869541444981872, Validation Loss: 0.21015131613674728\n",
      "Epoch 2724/3000, Training Loss: 0.05866747386986644, Validation Loss: 0.21014190424545293\n",
      "Epoch 2725/3000, Training Loss: 0.0586394881881716, Validation Loss: 0.2101308874178243\n",
      "Epoch 2726/3000, Training Loss: 0.05861159757274035, Validation Loss: 0.21012032380239856\n",
      "Epoch 2727/3000, Training Loss: 0.05858380453163431, Validation Loss: 0.2101021387555105\n",
      "Epoch 2728/3000, Training Loss: 0.058555997350986023, Validation Loss: 0.21009698077991262\n",
      "Epoch 2729/3000, Training Loss: 0.0585281767866674, Validation Loss: 0.21008535570406311\n",
      "Epoch 2730/3000, Training Loss: 0.058500277086164704, Validation Loss: 0.2100761837848847\n",
      "Epoch 2731/3000, Training Loss: 0.05847260484262246, Validation Loss: 0.2100603510103965\n",
      "Epoch 2732/3000, Training Loss: 0.058444804868255706, Validation Loss: 0.21004841823949533\n",
      "Epoch 2733/3000, Training Loss: 0.0584171270981594, Validation Loss: 0.21003866031560575\n",
      "Epoch 2734/3000, Training Loss: 0.05838923192738793, Validation Loss: 0.21002789810476497\n",
      "Epoch 2735/3000, Training Loss: 0.058361692118414424, Validation Loss: 0.21002026449620861\n",
      "Epoch 2736/3000, Training Loss: 0.058334009776244915, Validation Loss: 0.21000383614498316\n",
      "Epoch 2737/3000, Training Loss: 0.058306348701967575, Validation Loss: 0.20999668634017835\n",
      "Epoch 2738/3000, Training Loss: 0.05827857630952762, Validation Loss: 0.20998164426039823\n",
      "Epoch 2739/3000, Training Loss: 0.05825104738356222, Validation Loss: 0.20997218005632728\n",
      "Epoch 2740/3000, Training Loss: 0.05822341136031264, Validation Loss: 0.20995694789633013\n",
      "Epoch 2741/3000, Training Loss: 0.058196065267869954, Validation Loss: 0.2099482589619024\n",
      "Epoch 2742/3000, Training Loss: 0.05816823575329275, Validation Loss: 0.20993550166538424\n",
      "Epoch 2743/3000, Training Loss: 0.05814090135075384, Validation Loss: 0.20993096184614646\n",
      "Epoch 2744/3000, Training Loss: 0.05811333778714639, Validation Loss: 0.20991324404199793\n",
      "Epoch 2745/3000, Training Loss: 0.05808607906039927, Validation Loss: 0.20990346688984537\n",
      "Epoch 2746/3000, Training Loss: 0.05805831287546979, Validation Loss: 0.20989202476534996\n",
      "Epoch 2747/3000, Training Loss: 0.05803112176687894, Validation Loss: 0.20988046540393815\n",
      "Epoch 2748/3000, Training Loss: 0.058003459529913017, Validation Loss: 0.20986906582033224\n",
      "Epoch 2749/3000, Training Loss: 0.05797636076027046, Validation Loss: 0.20985705394127235\n",
      "Epoch 2750/3000, Training Loss: 0.05794864014725463, Validation Loss: 0.20984552200165893\n",
      "Epoch 2751/3000, Training Loss: 0.057921757824234726, Validation Loss: 0.20983898396608372\n",
      "Epoch 2752/3000, Training Loss: 0.05789398685422041, Validation Loss: 0.20982818936313577\n",
      "Epoch 2753/3000, Training Loss: 0.05786700228498654, Validation Loss: 0.20981217172314676\n",
      "Epoch 2754/3000, Training Loss: 0.05783941064395913, Validation Loss: 0.20980447094006915\n",
      "Epoch 2755/3000, Training Loss: 0.05781255173604557, Validation Loss: 0.20979153711623724\n",
      "Epoch 2756/3000, Training Loss: 0.05778474044862077, Validation Loss: 0.20978184082193496\n",
      "Epoch 2757/3000, Training Loss: 0.057757954316944915, Validation Loss: 0.2097664021532032\n",
      "Epoch 2758/3000, Training Loss: 0.05773052146164287, Validation Loss: 0.20976082020189155\n",
      "Epoch 2759/3000, Training Loss: 0.05770349449404234, Validation Loss: 0.20974912036923957\n",
      "Epoch 2760/3000, Training Loss: 0.05767608529771578, Validation Loss: 0.2097414745487728\n",
      "Epoch 2761/3000, Training Loss: 0.05764915593115131, Validation Loss: 0.20972838782453107\n",
      "Epoch 2762/3000, Training Loss: 0.05762181250123626, Validation Loss: 0.2097122584841074\n",
      "Epoch 2763/3000, Training Loss: 0.05759485053509779, Validation Loss: 0.20970373396673594\n",
      "Epoch 2764/3000, Training Loss: 0.0575677822347573, Validation Loss: 0.20969401504254617\n",
      "Epoch 2765/3000, Training Loss: 0.05754073542072907, Validation Loss: 0.20968555171178332\n",
      "Epoch 2766/3000, Training Loss: 0.057513441845965935, Validation Loss: 0.20967134011804775\n",
      "Epoch 2767/3000, Training Loss: 0.05748676859529545, Validation Loss: 0.20966117123390643\n",
      "Epoch 2768/3000, Training Loss: 0.05745955542598649, Validation Loss: 0.20964777678810786\n",
      "Epoch 2769/3000, Training Loss: 0.05743259688599558, Validation Loss: 0.20964133181746575\n",
      "Epoch 2770/3000, Training Loss: 0.0574054888440256, Validation Loss: 0.20962512141797285\n",
      "Epoch 2771/3000, Training Loss: 0.05737883634800849, Validation Loss: 0.20961488057594693\n",
      "Epoch 2772/3000, Training Loss: 0.057351618542182986, Validation Loss: 0.20960439728048927\n",
      "Epoch 2773/3000, Training Loss: 0.05732482367082367, Validation Loss: 0.20959315695659445\n",
      "Epoch 2774/3000, Training Loss: 0.05729788088426974, Validation Loss: 0.20958786082650166\n",
      "Epoch 2775/3000, Training Loss: 0.0572710214894104, Validation Loss: 0.20957086363861588\n",
      "Epoch 2776/3000, Training Loss: 0.057244083987806546, Validation Loss: 0.20956222201511038\n",
      "Epoch 2777/3000, Training Loss: 0.057217484304947296, Validation Loss: 0.20955088410955575\n",
      "Epoch 2778/3000, Training Loss: 0.05719042528873041, Validation Loss: 0.2095436144770778\n",
      "Epoch 2779/3000, Training Loss: 0.05716364149306801, Validation Loss: 0.20952775003453034\n",
      "Epoch 2780/3000, Training Loss: 0.05713708591992139, Validation Loss: 0.20951752905196921\n",
      "Epoch 2781/3000, Training Loss: 0.05711028922432298, Validation Loss: 0.2095083663883524\n",
      "Epoch 2782/3000, Training Loss: 0.05708329881057385, Validation Loss: 0.2094987333245635\n",
      "Epoch 2783/3000, Training Loss: 0.057056843498653004, Validation Loss: 0.2094852474053334\n",
      "Epoch 2784/3000, Training Loss: 0.057030118340770077, Validation Loss: 0.20947481449301547\n",
      "Epoch 2785/3000, Training Loss: 0.05700348477429874, Validation Loss: 0.20946091346341902\n",
      "Epoch 2786/3000, Training Loss: 0.05697671915101556, Validation Loss: 0.20945429964189563\n",
      "Epoch 2787/3000, Training Loss: 0.05695025590586632, Validation Loss: 0.20944294476491668\n",
      "Epoch 2788/3000, Training Loss: 0.05692345019155117, Validation Loss: 0.20942910500837614\n",
      "Epoch 2789/3000, Training Loss: 0.05689703510937777, Validation Loss: 0.20941908792858385\n",
      "Epoch 2790/3000, Training Loss: 0.05687050440578463, Validation Loss: 0.20941307978738136\n",
      "Epoch 2791/3000, Training Loss: 0.05684382966898641, Validation Loss: 0.2093997363358036\n",
      "Epoch 2792/3000, Training Loss: 0.05681724471323302, Validation Loss: 0.20938577284397483\n",
      "Epoch 2793/3000, Training Loss: 0.056791019543677455, Validation Loss: 0.2093753246526866\n",
      "Epoch 2794/3000, Training Loss: 0.056764501910896485, Validation Loss: 0.20936313195447484\n",
      "Epoch 2795/3000, Training Loss: 0.0567378238551646, Validation Loss: 0.2093526558081597\n",
      "Epoch 2796/3000, Training Loss: 0.05671169087991037, Validation Loss: 0.20934188583699687\n",
      "Epoch 2797/3000, Training Loss: 0.05668522199834491, Validation Loss: 0.2093349113213758\n",
      "Epoch 2798/3000, Training Loss: 0.056658886355451035, Validation Loss: 0.20932550594467686\n",
      "Epoch 2799/3000, Training Loss: 0.056632518193376474, Validation Loss: 0.20931723192802693\n",
      "Epoch 2800/3000, Training Loss: 0.05660618822847829, Validation Loss: 0.20930057976697447\n",
      "Epoch 2801/3000, Training Loss: 0.056579856106885526, Validation Loss: 0.20928943160903676\n",
      "Epoch 2802/3000, Training Loss: 0.05655359865577339, Validation Loss: 0.20928112206176078\n",
      "Epoch 2803/3000, Training Loss: 0.05652743427908538, Validation Loss: 0.20926916513540464\n",
      "Epoch 2804/3000, Training Loss: 0.05650094044839977, Validation Loss: 0.20926097062331261\n",
      "Epoch 2805/3000, Training Loss: 0.05647474085970283, Validation Loss: 0.20924255846315445\n",
      "Epoch 2806/3000, Training Loss: 0.056448738953239555, Validation Loss: 0.20923939624768795\n",
      "Epoch 2807/3000, Training Loss: 0.056422359759914, Validation Loss: 0.20922953752632134\n",
      "Epoch 2808/3000, Training Loss: 0.05639619090782963, Validation Loss: 0.2092219491034674\n",
      "Epoch 2809/3000, Training Loss: 0.05637012968273003, Validation Loss: 0.2092057962200295\n",
      "Epoch 2810/3000, Training Loss: 0.05634405843714321, Validation Loss: 0.2091967444565614\n",
      "Epoch 2811/3000, Training Loss: 0.0563177226051808, Validation Loss: 0.20918420527610243\n",
      "Epoch 2812/3000, Training Loss: 0.05629174019485216, Validation Loss: 0.20917728341402017\n",
      "Epoch 2813/3000, Training Loss: 0.056265644162823966, Validation Loss: 0.20916133861510042\n",
      "Epoch 2814/3000, Training Loss: 0.0562394929682027, Validation Loss: 0.2091533256766027\n",
      "Epoch 2815/3000, Training Loss: 0.056213498083693615, Validation Loss: 0.2091465898053427\n",
      "Epoch 2816/3000, Training Loss: 0.056187489651937754, Validation Loss: 0.20913893843117748\n",
      "Epoch 2817/3000, Training Loss: 0.05616141539324167, Validation Loss: 0.2091206077493603\n",
      "Epoch 2818/3000, Training Loss: 0.056135456086532934, Validation Loss: 0.20911319151473315\n",
      "Epoch 2819/3000, Training Loss: 0.056109495734328184, Validation Loss: 0.20910423597327452\n",
      "Epoch 2820/3000, Training Loss: 0.05608349816560806, Validation Loss: 0.20909371995822323\n",
      "Epoch 2821/3000, Training Loss: 0.056057404121017826, Validation Loss: 0.20907850399850708\n",
      "Epoch 2822/3000, Training Loss: 0.056031749999176, Validation Loss: 0.20907116490604744\n",
      "Epoch 2823/3000, Training Loss: 0.05600572534485359, Validation Loss: 0.20906210245964482\n",
      "Epoch 2824/3000, Training Loss: 0.05597992319014606, Validation Loss: 0.20905452903061514\n",
      "Epoch 2825/3000, Training Loss: 0.05595387805489812, Validation Loss: 0.20904795591807337\n",
      "Epoch 2826/3000, Training Loss: 0.05592827477206772, Validation Loss: 0.2090310640421963\n",
      "Epoch 2827/3000, Training Loss: 0.055902357041552365, Validation Loss: 0.20901939417939977\n",
      "Epoch 2828/3000, Training Loss: 0.055876544868602704, Validation Loss: 0.20901371160038215\n",
      "Epoch 2829/3000, Training Loss: 0.05585081302894833, Validation Loss: 0.20900253769226163\n",
      "Epoch 2830/3000, Training Loss: 0.055825001852182254, Validation Loss: 0.20898729492039422\n",
      "Epoch 2831/3000, Training Loss: 0.055799272504361286, Validation Loss: 0.20897895423395232\n",
      "Epoch 2832/3000, Training Loss: 0.055773642611017096, Validation Loss: 0.20897406870035506\n",
      "Epoch 2833/3000, Training Loss: 0.05574788208536367, Validation Loss: 0.20896238298677874\n",
      "Epoch 2834/3000, Training Loss: 0.0557220951824524, Validation Loss: 0.20895475589451692\n",
      "Epoch 2835/3000, Training Loss: 0.055696499525414944, Validation Loss: 0.2089415641333071\n",
      "Epoch 2836/3000, Training Loss: 0.05567095960979737, Validation Loss: 0.2089348113231494\n",
      "Epoch 2837/3000, Training Loss: 0.055645211824220886, Validation Loss: 0.20892178235946715\n",
      "Epoch 2838/3000, Training Loss: 0.05561960045089422, Validation Loss: 0.2089114740683757\n",
      "Epoch 2839/3000, Training Loss: 0.05559403335105121, Validation Loss: 0.2089015153400555\n",
      "Epoch 2840/3000, Training Loss: 0.05556856671412362, Validation Loss: 0.2088907272477771\n",
      "Epoch 2841/3000, Training Loss: 0.05554283478796284, Validation Loss: 0.208883898977671\n",
      "Epoch 2842/3000, Training Loss: 0.05551738526374432, Validation Loss: 0.20887044250981307\n",
      "Epoch 2843/3000, Training Loss: 0.05549193118788613, Validation Loss: 0.20886430392032845\n",
      "Epoch 2844/3000, Training Loss: 0.0554662884350027, Validation Loss: 0.20885686994245514\n",
      "Epoch 2845/3000, Training Loss: 0.055441018148312295, Validation Loss: 0.20884748054884233\n",
      "Epoch 2846/3000, Training Loss: 0.055415389186971864, Validation Loss: 0.20883685540337382\n",
      "Epoch 2847/3000, Training Loss: 0.055390054585783055, Validation Loss: 0.20882440179972692\n",
      "Epoch 2848/3000, Training Loss: 0.05536454261781763, Validation Loss: 0.20881695765394143\n",
      "Epoch 2849/3000, Training Loss: 0.05533931444273808, Validation Loss: 0.20880392233128098\n",
      "Epoch 2850/3000, Training Loss: 0.05531370671582934, Validation Loss: 0.2087989202857456\n",
      "Epoch 2851/3000, Training Loss: 0.055288517119851846, Validation Loss: 0.20878424987975083\n",
      "Epoch 2852/3000, Training Loss: 0.05526310747364906, Validation Loss: 0.20878060460096504\n",
      "Epoch 2853/3000, Training Loss: 0.05523780617806312, Validation Loss: 0.20876932539572476\n",
      "Epoch 2854/3000, Training Loss: 0.055212373932337425, Validation Loss: 0.20876399527396666\n",
      "Epoch 2855/3000, Training Loss: 0.05518732551068939, Validation Loss: 0.20874685857552505\n",
      "Epoch 2856/3000, Training Loss: 0.05516193983618739, Validation Loss: 0.20873950114357276\n",
      "Epoch 2857/3000, Training Loss: 0.05513666718795318, Validation Loss: 0.20872958773255607\n",
      "Epoch 2858/3000, Training Loss: 0.05511148557252216, Validation Loss: 0.20872383906152972\n",
      "Epoch 2859/3000, Training Loss: 0.05508636213122109, Validation Loss: 0.2087080823780446\n",
      "Epoch 2860/3000, Training Loss: 0.05506095500127703, Validation Loss: 0.2086989184863407\n",
      "Epoch 2861/3000, Training Loss: 0.0550360655332996, Validation Loss: 0.2086935104887513\n",
      "Epoch 2862/3000, Training Loss: 0.055010678460572705, Validation Loss: 0.20868483592172693\n",
      "Epoch 2863/3000, Training Loss: 0.05498567589484781, Validation Loss: 0.20867353041704526\n",
      "Epoch 2864/3000, Training Loss: 0.05496039540497699, Validation Loss: 0.20866491779548066\n",
      "Epoch 2865/3000, Training Loss: 0.054935607121699495, Validation Loss: 0.2086568217069178\n",
      "Epoch 2866/3000, Training Loss: 0.05491009756362573, Validation Loss: 0.20864580353249168\n",
      "Epoch 2867/3000, Training Loss: 0.05488526973152023, Validation Loss: 0.2086342820928262\n",
      "Epoch 2868/3000, Training Loss: 0.05486021404732185, Validation Loss: 0.20862680451861057\n",
      "Epoch 2869/3000, Training Loss: 0.05483525179504533, Validation Loss: 0.20861537984265724\n",
      "Epoch 2870/3000, Training Loss: 0.05480999697739129, Validation Loss: 0.20860969560289044\n",
      "Epoch 2871/3000, Training Loss: 0.054785409899668215, Validation Loss: 0.2085977804013224\n",
      "Epoch 2872/3000, Training Loss: 0.05476023174463334, Validation Loss: 0.20859153495185154\n",
      "Epoch 2873/3000, Training Loss: 0.05473531830319034, Validation Loss: 0.20858228566748704\n",
      "Epoch 2874/3000, Training Loss: 0.05471029926577137, Validation Loss: 0.2085757451666872\n",
      "Epoch 2875/3000, Training Loss: 0.05468560077407619, Validation Loss: 0.2085644169667451\n",
      "Epoch 2876/3000, Training Loss: 0.05466039754250722, Validation Loss: 0.20855249091337058\n",
      "Epoch 2877/3000, Training Loss: 0.054635774187217906, Validation Loss: 0.20854440623209938\n",
      "Epoch 2878/3000, Training Loss: 0.05461071020092976, Validation Loss: 0.20853511443666453\n",
      "Epoch 2879/3000, Training Loss: 0.0545860082123753, Validation Loss: 0.20852891331726808\n",
      "Epoch 2880/3000, Training Loss: 0.054561029531428246, Validation Loss: 0.2085173028516674\n",
      "Epoch 2881/3000, Training Loss: 0.05453658338047785, Validation Loss: 0.20850662314039434\n",
      "Epoch 2882/3000, Training Loss: 0.05451135000951175, Validation Loss: 0.20849804937334343\n",
      "Epoch 2883/3000, Training Loss: 0.05448683786131696, Validation Loss: 0.20849141913259306\n",
      "Epoch 2884/3000, Training Loss: 0.054462040687685766, Validation Loss: 0.2084805359997842\n",
      "Epoch 2885/3000, Training Loss: 0.054437363507876445, Validation Loss: 0.20847210135050587\n",
      "Epoch 2886/3000, Training Loss: 0.05441240327959758, Validation Loss: 0.2084616931663327\n",
      "Epoch 2887/3000, Training Loss: 0.054387877477701775, Validation Loss: 0.20845730066852694\n",
      "Epoch 2888/3000, Training Loss: 0.05436321894096824, Validation Loss: 0.20844265469065898\n",
      "Epoch 2889/3000, Training Loss: 0.054338495040998185, Validation Loss: 0.2084344141439137\n",
      "Epoch 2890/3000, Training Loss: 0.05431380678417851, Validation Loss: 0.20842713750003855\n",
      "Epoch 2891/3000, Training Loss: 0.054289152173068664, Validation Loss: 0.2084199398061152\n",
      "Epoch 2892/3000, Training Loss: 0.05426455417699373, Validation Loss: 0.2084033315150204\n",
      "Epoch 2893/3000, Training Loss: 0.05423998658467352, Validation Loss: 0.2083969743468292\n",
      "Epoch 2894/3000, Training Loss: 0.05421536734399239, Validation Loss: 0.20839258890782553\n",
      "Epoch 2895/3000, Training Loss: 0.054190718806046714, Validation Loss: 0.20838353491391257\n",
      "Epoch 2896/3000, Training Loss: 0.054166261790616184, Validation Loss: 0.20837289103309323\n",
      "Epoch 2897/3000, Training Loss: 0.05414181506532931, Validation Loss: 0.20836541355735733\n",
      "Epoch 2898/3000, Training Loss: 0.054117220775476475, Validation Loss: 0.2083547009448014\n",
      "Epoch 2899/3000, Training Loss: 0.05409262145752918, Validation Loss: 0.20834810945154578\n",
      "Epoch 2900/3000, Training Loss: 0.05406820849122506, Validation Loss: 0.20833473162678395\n",
      "Epoch 2901/3000, Training Loss: 0.05404379196164046, Validation Loss: 0.2083268389621062\n",
      "Epoch 2902/3000, Training Loss: 0.054019528191742355, Validation Loss: 0.20831692775783886\n",
      "Epoch 2903/3000, Training Loss: 0.053994907255489936, Validation Loss: 0.20830897538361343\n",
      "Epoch 2904/3000, Training Loss: 0.053970835572576495, Validation Loss: 0.20829585510950868\n",
      "Epoch 2905/3000, Training Loss: 0.053946425643765304, Validation Loss: 0.208292586978853\n",
      "Epoch 2906/3000, Training Loss: 0.05392224550844462, Validation Loss: 0.20828084417248757\n",
      "Epoch 2907/3000, Training Loss: 0.053897881243961936, Validation Loss: 0.208275500332936\n",
      "Epoch 2908/3000, Training Loss: 0.05387375876894448, Validation Loss: 0.20825987817483183\n",
      "Epoch 2909/3000, Training Loss: 0.05384926777566137, Validation Loss: 0.20825225345604476\n",
      "Epoch 2910/3000, Training Loss: 0.053825315449075, Validation Loss: 0.20824325293689058\n",
      "Epoch 2911/3000, Training Loss: 0.05380094053967233, Validation Loss: 0.2082327900816159\n",
      "Epoch 2912/3000, Training Loss: 0.05377704109193122, Validation Loss: 0.20822096192428402\n",
      "Epoch 2913/3000, Training Loss: 0.05375246131963732, Validation Loss: 0.20821435188017945\n",
      "Epoch 2914/3000, Training Loss: 0.05372887523851106, Validation Loss: 0.20820530049480593\n",
      "Epoch 2915/3000, Training Loss: 0.05370427057303479, Validation Loss: 0.20819613102454038\n",
      "Epoch 2916/3000, Training Loss: 0.053680383115555674, Validation Loss: 0.20818824653613305\n",
      "Epoch 2917/3000, Training Loss: 0.053656304895643006, Validation Loss: 0.20817895256017774\n",
      "Epoch 2918/3000, Training Loss: 0.05363232249058476, Validation Loss: 0.20816914392572983\n",
      "Epoch 2919/3000, Training Loss: 0.05360797601670489, Validation Loss: 0.20816071217375876\n",
      "Epoch 2920/3000, Training Loss: 0.05358427143485641, Validation Loss: 0.20814639822373898\n",
      "Epoch 2921/3000, Training Loss: 0.053560117490130614, Validation Loss: 0.20813913279644236\n",
      "Epoch 2922/3000, Training Loss: 0.053536113788342485, Validation Loss: 0.2081329281900143\n",
      "Epoch 2923/3000, Training Loss: 0.05351216333385314, Validation Loss: 0.20812542835743617\n",
      "Epoch 2924/3000, Training Loss: 0.053488276528833846, Validation Loss: 0.2081111865152451\n",
      "Epoch 2925/3000, Training Loss: 0.05346424004378575, Validation Loss: 0.20810421358311615\n",
      "Epoch 2926/3000, Training Loss: 0.05344032802147484, Validation Loss: 0.20809517374059375\n",
      "Epoch 2927/3000, Training Loss: 0.05341653735301428, Validation Loss: 0.20808530932329505\n",
      "Epoch 2928/3000, Training Loss: 0.0533924207825323, Validation Loss: 0.20807772209460743\n",
      "Epoch 2929/3000, Training Loss: 0.05336877856981151, Validation Loss: 0.20807066231499635\n",
      "Epoch 2930/3000, Training Loss: 0.05334482539593744, Validation Loss: 0.2080638138868972\n",
      "Epoch 2931/3000, Training Loss: 0.05332104416927748, Validation Loss: 0.20805362217455287\n",
      "Epoch 2932/3000, Training Loss: 0.05329702781119965, Validation Loss: 0.20804224064288063\n",
      "Epoch 2933/3000, Training Loss: 0.0532735712218133, Validation Loss: 0.2080327952145304\n",
      "Epoch 2934/3000, Training Loss: 0.053249371724977373, Validation Loss: 0.20802756449854287\n",
      "Epoch 2935/3000, Training Loss: 0.05322579825325014, Validation Loss: 0.20801735415651706\n",
      "Epoch 2936/3000, Training Loss: 0.05320196106599753, Validation Loss: 0.2080047694453873\n",
      "Epoch 2937/3000, Training Loss: 0.05317846521306299, Validation Loss: 0.20799874035987456\n",
      "Epoch 2938/3000, Training Loss: 0.05315445188907107, Validation Loss: 0.20799043808527856\n",
      "Epoch 2939/3000, Training Loss: 0.05313108558022339, Validation Loss: 0.207983201361557\n",
      "Epoch 2940/3000, Training Loss: 0.05310704379037684, Validation Loss: 0.20797105911705163\n",
      "Epoch 2941/3000, Training Loss: 0.05308364466375382, Validation Loss: 0.20796438837757683\n",
      "Epoch 2942/3000, Training Loss: 0.05305992153009396, Validation Loss: 0.20795620962754338\n",
      "Epoch 2943/3000, Training Loss: 0.053036366014399296, Validation Loss: 0.20794897468341783\n",
      "Epoch 2944/3000, Training Loss: 0.05301242361278922, Validation Loss: 0.20793380375537282\n",
      "Epoch 2945/3000, Training Loss: 0.05298921505069192, Validation Loss: 0.2079306445375429\n",
      "Epoch 2946/3000, Training Loss: 0.05296551760181841, Validation Loss: 0.207920928896708\n",
      "Epoch 2947/3000, Training Loss: 0.05294183740865581, Validation Loss: 0.20791179152334047\n",
      "Epoch 2948/3000, Training Loss: 0.05291829987636128, Validation Loss: 0.20790202185520457\n",
      "Epoch 2949/3000, Training Loss: 0.05289499283196663, Validation Loss: 0.20789187625764624\n",
      "Epoch 2950/3000, Training Loss: 0.05287131878292263, Validation Loss: 0.20788357499408933\n",
      "Epoch 2951/3000, Training Loss: 0.05284782266745913, Validation Loss: 0.20787994730280912\n",
      "Epoch 2952/3000, Training Loss: 0.05282438052081327, Validation Loss: 0.2078706646266339\n",
      "Epoch 2953/3000, Training Loss: 0.05280078546904994, Validation Loss: 0.20786187170983006\n",
      "Epoch 2954/3000, Training Loss: 0.0527774881806256, Validation Loss: 0.20785492861572125\n",
      "Epoch 2955/3000, Training Loss: 0.052753895544355535, Validation Loss: 0.20784739364793767\n",
      "Epoch 2956/3000, Training Loss: 0.05273053748704287, Validation Loss: 0.20783301757868736\n",
      "Epoch 2957/3000, Training Loss: 0.05270693275814829, Validation Loss: 0.2078273431556785\n",
      "Epoch 2958/3000, Training Loss: 0.05268389195347889, Validation Loss: 0.20781705049313096\n",
      "Epoch 2959/3000, Training Loss: 0.05266019725866137, Validation Loss: 0.20781158067095884\n",
      "Epoch 2960/3000, Training Loss: 0.05263695111879142, Validation Loss: 0.20779736338618113\n",
      "Epoch 2961/3000, Training Loss: 0.05261359772921033, Validation Loss: 0.20779016415075885\n",
      "Epoch 2962/3000, Training Loss: 0.05259053794401647, Validation Loss: 0.20778326228836905\n",
      "Epoch 2963/3000, Training Loss: 0.05256688724414198, Validation Loss: 0.20777442443659486\n",
      "Epoch 2964/3000, Training Loss: 0.0525438757555892, Validation Loss: 0.20776785126711259\n",
      "Epoch 2965/3000, Training Loss: 0.05252058203913911, Validation Loss: 0.20775750830113704\n",
      "Epoch 2966/3000, Training Loss: 0.05249726419947045, Validation Loss: 0.20775159655161543\n",
      "Epoch 2967/3000, Training Loss: 0.05247402245802884, Validation Loss: 0.20774284527676815\n",
      "Epoch 2968/3000, Training Loss: 0.05245089919770754, Validation Loss: 0.207731524458422\n",
      "Epoch 2969/3000, Training Loss: 0.052427648267025014, Validation Loss: 0.2077257387194553\n",
      "Epoch 2970/3000, Training Loss: 0.052404405158156, Validation Loss: 0.20771798340495012\n",
      "Epoch 2971/3000, Training Loss: 0.05238138422061955, Validation Loss: 0.20770954785606063\n",
      "Epoch 2972/3000, Training Loss: 0.05235808632856757, Validation Loss: 0.20769803643837356\n",
      "Epoch 2973/3000, Training Loss: 0.05233506498901001, Validation Loss: 0.20768955247750492\n",
      "Epoch 2974/3000, Training Loss: 0.052311895234919574, Validation Loss: 0.20768246050737965\n",
      "Epoch 2975/3000, Training Loss: 0.05228888456376369, Validation Loss: 0.2076756834673456\n",
      "Epoch 2976/3000, Training Loss: 0.05226560451452017, Validation Loss: 0.20766710187864826\n",
      "Epoch 2977/3000, Training Loss: 0.05224290567379173, Validation Loss: 0.20766234685660392\n",
      "Epoch 2978/3000, Training Loss: 0.05221959692936176, Validation Loss: 0.20765242050855834\n",
      "Epoch 2979/3000, Training Loss: 0.0521966268015147, Validation Loss: 0.20764574943651348\n",
      "Epoch 2980/3000, Training Loss: 0.05217350799461184, Validation Loss: 0.20763121671046508\n",
      "Epoch 2981/3000, Training Loss: 0.05215082172904814, Validation Loss: 0.20762555590656026\n",
      "Epoch 2982/3000, Training Loss: 0.052127506712581415, Validation Loss: 0.20761566725694733\n",
      "Epoch 2983/3000, Training Loss: 0.05210464030618899, Validation Loss: 0.20761156414111062\n",
      "Epoch 2984/3000, Training Loss: 0.05208163677413258, Validation Loss: 0.20759649029757848\n",
      "Epoch 2985/3000, Training Loss: 0.052058892215696734, Validation Loss: 0.20759219252293654\n",
      "Epoch 2986/3000, Training Loss: 0.05203580826227087, Validation Loss: 0.20758565224553746\n",
      "Epoch 2987/3000, Training Loss: 0.052012975214701204, Validation Loss: 0.20757816809791077\n",
      "Epoch 2988/3000, Training Loss: 0.051989987486007104, Validation Loss: 0.20756551874323362\n",
      "Epoch 2989/3000, Training Loss: 0.051967150480108686, Validation Loss: 0.20755783810725403\n",
      "Epoch 2990/3000, Training Loss: 0.0519443492940711, Validation Loss: 0.2075507127444679\n",
      "Epoch 2991/3000, Training Loss: 0.05192136239585003, Validation Loss: 0.2075464373396971\n",
      "Epoch 2992/3000, Training Loss: 0.05189853428431565, Validation Loss: 0.20753699309966406\n",
      "Epoch 2993/3000, Training Loss: 0.05187569268690922, Validation Loss: 0.20752992999389203\n",
      "Epoch 2994/3000, Training Loss: 0.051853088583312994, Validation Loss: 0.20752098250369294\n",
      "Epoch 2995/3000, Training Loss: 0.051829994975888735, Validation Loss: 0.2075151894043086\n",
      "Epoch 2996/3000, Training Loss: 0.05180745229070293, Validation Loss: 0.20750304915258874\n",
      "Epoch 2997/3000, Training Loss: 0.0517845420635308, Validation Loss: 0.20749469079563418\n",
      "Epoch 2998/3000, Training Loss: 0.05176195072480653, Validation Loss: 0.20748755884210834\n",
      "Epoch 2999/3000, Training Loss: 0.051739077057305446, Validation Loss: 0.2074797476475394\n",
      "Epoch 3000/3000, Training Loss: 0.05171654140992109, Validation Loss: 0.20746591011531318\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Normalize X\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # One-hot encode y if necessary for your setup\n",
    "encoder = OneHotEncoder()\n",
    "y_one_hot = encoder.fit_transform(y.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_one_hot, test_size=0.2, random_state=36)\n",
    "\n",
    "# Define the neural network architecture\n",
    "nn_arch = [\n",
    "    {'input_dim': 64, 'output_dim': 32, 'activation': 'relu'},\n",
    "    {'input_dim': 32, 'output_dim': 10, 'activation': 'sigmoid'}  # Assuming a sigmoid output layer for multi-class classification\n",
    "]\n",
    "\n",
    "# Instantiate the neural network\n",
    "nn = NeuralNetwork(nn_arch, lr=0.05, seed=57, batch_size=32, epochs=3000, loss_function='binary_cross_entropy')\n",
    "\n",
    "# Train the neural network\n",
    "train_loss, val_loss = nn.fit(X_train.T, y_train.T, X_val.T, y_val.T)  # Note: .T is used to transpose the matrices to match the expected shape\n",
    "\n",
    "# Predictions (Optional: evaluate the model's performance)\n",
    "y_hat_val = nn.predict(X_val.T)\n",
    "y_pred_labels = np.argmax(y_hat_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1437, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1437, 64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucsfbp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
